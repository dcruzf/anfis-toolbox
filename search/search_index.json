{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"ANFIS Toolbox <p> The most user-friendly Python library for Adaptive Neuro-Fuzzy Inference Systems (ANFIS) </p> <p>ANFIS Toolbox is a comprehensive Python library for creating, training, and deploying Adaptive Neuro-Fuzzy Inference Systems (ANFIS). It provides an intuitive API that makes fuzzy neural networks accessible to both beginners and experts.</p> \ud83d\udd17 GitHub | \ud83d\udce6 PyPI"},{"location":"#key-features","title":"Key Features","text":"\u2728 Easy to Use         Get started with just 3 lines of code               \ud83e\udd16 Versatile Modeling         Supports both classification and regression tasks               \ud83c\udfd7\ufe0f Flexible Architecture         13 membership functions               \ud83d\ude80 Adaptive Initialization         Fuzzy c-means, grid, and random initialization strategies               \ud83d\udcc9 Flexible Optimization         Multiple optimization algorithms               \ud83d\udcd0 Comprehensive Metrics         Rich collection of evaluation metrics               \ud83d\udcda Rich Documentation         Comprehensive examples"},{"location":"#why-anfis-toolbox","title":"Why ANFIS Toolbox?","text":""},{"location":"#simplicity-first","title":"\ud83d\ude80 Simplicity First","text":"<p>Most fuzzy logic libraries require extensive boilerplate code. ANFIS Toolbox gets you running in seconds:</p> RegressionClassification <pre><code>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor()\nmodel.fit(X, y)\n</code></pre> <pre><code>from anfis_toolbox import ANFISClassifier\n\nmodel = ANFISClassifier()\nmodel.fit(X, y)\n</code></pre>"},{"location":"#quick-example","title":"\u26a1 Quick Example","text":"RegressionClassification <pre><code>import numpy as np\nfrom anfis_toolbox import ANFISRegressor\n\nX = np.random.uniform(-2, 2, (100, 2))  # 2 inputs\ny = X[:, 0]**2 + X[:, 1]**2  # Target: x1\u00b2 + x2\u00b2\n\nmodel = ANFISRegressor()\nmodel.fit(X, y)\n</code></pre> <pre><code>import numpy as np\nfrom anfis_toolbox import ANFISClassifier\n\nX = np.r_[np.random.normal(-1, .3, (50, 2)), np.random.normal(1, .3, (50, 2))]\ny = np.r_[np.zeros(50, int), np.ones(50, int)]\n\nmodel = ANFISClassifier()\nmodel.fit(X, y)\n</code></pre>"},{"location":"#metrics-evaluation","title":"\ud83d\udcd0 Metrics &amp; Evaluation","text":"<p>Want a structured report instead of a plain dictionary? Use <code>evaluate</code> to detect the task type automatically and access every score.</p> <pre><code>metrics = model.evaluate(X, y)\n</code></pre> <p>That's it! \ud83c\udf89 You just created, trained and evaluate a neuro-fuzzy system!</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the core package with minimal dependencies:</p> <pre><code>pip install anfis-toolbox\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"Application Description Function Approximation Learn complex mathematical functions Regression Predict continuous values Classification Predict discrete class labels Time Series Forecast future values"},{"location":"#architecture","title":"Architecture","text":"<p>ANFIS Toolbox implements the complete 4-layer ANFIS architecture:</p> <pre><code>flowchart LR\n\n    %% Layer 1\n    subgraph L1 [layer 1]\n      direction TB\n      A1[\"A1\"]\n      A2[\"A2\"]\n      B1[\"B1\"]\n      B2[\"B2\"]\n    end\n\n    %% Inputs\n    x_input[x] --&gt; A1\n    x_input --&gt; A2\n    y_input[y] --&gt; B1\n    y_input --&gt; B2\n\n    %% Layer 2\n    subgraph L2 [layer 2]\n      direction TB\n      P1((\u03a0))\n      P2((\u03a0))\n    end\n    A1 --&gt; P1\n    B1 --&gt; P1\n    A2 --&gt; P2\n    B2 --&gt; P2\n\n    %% Layer 3\n    subgraph L3 [layer 3]\n      direction TB\n      N1((N))\n      N2((N))\n    end\n    P1 -- w\u2081 --&gt; N1\n    P1 ----&gt; N2\n    P2 ----&gt; N1\n    P2 -- w\u2082 --&gt; N2\n\n    %% Layer 4\n    subgraph L4 [layer 4]\n      direction TB\n      L4_1[x y]\n      L4_2[x y]\n    end\n    N1 -- w\u0305\u2081 --&gt; L4_1\n    N2 -- w\u0305\u2082 --&gt; L4_2\n\n    %% Layer 5\n    subgraph L5 [layer 5]\n      direction TB\n      Sum((\u03a3))\n    end\n    L4_1 -- \"w\u2081 f\u2081\" --&gt; Sum\n    L4_2 -- \"w\u2082 f\u2082\" --&gt; Sum\n\n    %% Output\n    Sum -- f --&gt; f_out[f]</code></pre>"},{"location":"#supported-membership-functions","title":"Supported Membership Functions","text":"<ul> <li>Gaussian (<code>GaussianMF</code>) - Smooth bell curves</li> <li>Gaussian2 (<code>Gaussian2MF</code>) - Two-sided Gaussian with flat region</li> <li>Triangular (<code>TriangularMF</code>) - Simple triangular shapes</li> <li>Trapezoidal (<code>TrapezoidalMF</code>) - Plateau regions</li> <li>Bell-shaped (<code>BellMF</code>) - Generalized bell curves</li> <li>Sigmoidal (<code>SigmoidalMF</code>) - S-shaped transitions</li> <li>Diff-Sigmoidal (<code>DiffSigmoidalMF</code>) - Difference of two sigmoids</li> <li>Prod-Sigmoidal (<code>ProdSigmoidalMF</code>) - Product of two sigmoids</li> <li>S-shaped (<code>SShapedMF</code>) - Smooth S-curve transitions</li> <li>Linear S-shaped (<code>LinSShapedMF</code>) - Piecewise linear S-curve</li> <li>Z-shaped (<code>ZShapedMF</code>) - Smooth Z-curve transitions</li> <li>Linear Z-shaped (<code>LinZShapedMF</code>) - Piecewise linear Z-curve</li> <li>Pi-shaped (<code>PiMF</code>) - Bell with flat top</li> </ul>"},{"location":"#training-methods","title":"Training Methods","text":"<ul> <li>SGD (Stochastic Gradient Descent) \u2013 Classic gradient-based optimization with incremental updates</li> <li>Adam \u2013 Adaptive learning rates with momentum for faster convergence</li> <li>RMSProp \u2013 Scales learning rates by recent gradient magnitudes for stable training</li> <li>PSO (Particle Swarm Optimization) \u2013 Population-based global search strategy</li> <li>Hybrid SGD + OLS \u2013 Combines gradient descent with least-squares parameter refinement</li> <li>Hybrid Adam + OLS \u2013 Integrates adaptive optimization with analytical least-squares adjustment</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>\ud83d\udca1 Examples - Real-world use cases</li> <li>\ud83d\udd27 API Reference - Complete function documentation</li> <li>\ud83e\udd16 ANFIS Models - Regression and classification models</li> <li>\ud83d\udcd0 Membership Functions - All MF classes</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udc1b Report Issues - Bug reports and feature requests</li> <li>\ud83d\udcac Discussions - Questions and community chat</li> <li>\ud83d\udcd8 Developer Guide - Architecture notes and contribution workflow</li> <li>\u2b50 Star on GitHub - Show your support!</li> </ul> Ready to dive into fuzzy neural networks? Get started now"},{"location":"guide/","title":"Developer Guide","text":"<p>Welcome to the ANFIS Toolbox developer guide. This document is intended for contributors who want to understand the architecture, extend the library, or work on the documentation and examples. If you are looking for the user-facing API reference, consult the generated MkDocs site or the module docstrings.</p>"},{"location":"guide/#project-goals","title":"Project Goals","text":"<ul> <li>Provide a batteries-included Adaptive Neuro-Fuzzy Inference System (ANFIS)     implementation in pure Python.</li> <li>Offer high-level estimators (<code>ANFISRegressor</code>, <code>ANFISClassifier</code>) that feel     familiar to scikit-learn users while remaining dependency-free.</li> <li>Support a wide range of membership function families and training regimes.</li> <li>Ship with reproducible examples, thorough tests, and easy-to-read docs.</li> </ul>"},{"location":"guide/#repository-layout","title":"Repository Layout","text":"<pre><code>anfis_toolbox/\n\u251c\u2500\u2500 model.py              # Low-level ANFIS graph (layers, forward passes)\n\u251c\u2500\u2500 regressor.py          # High-level regression estimator facade\n\u251c\u2500\u2500 classifier.py         # High-level classification estimator facade\n\u251c\u2500\u2500 membership.py         # Membership function implementations and helpers\n\u251c\u2500\u2500 builders.py           # Utilities that assemble models from configuration\n\u251c\u2500\u2500 optim/                # Optimizer and trainer implementations\n\u251c\u2500\u2500 metrics.py            # Regression and classification metric utilities\n\u251c\u2500\u2500 losses.py             # Loss definitions shared by optimizers\n\u2514\u2500\u2500 estimator_utils.py    # Mixins, validation helpers, sklearn-like machinery\n\ndocs/                     # MkDocs pages and reference material\nexamples/                 # Notebook-based walkthroughs\ntests/                    # Pytest suite with full coverage\n</code></pre>"},{"location":"guide/#high-level-architecture","title":"High-Level Architecture","text":"<ol> <li>Estimators (<code>ANFISRegressor</code>, <code>ANFISClassifier</code>) expose a drop-in API      with <code>fit</code>, <code>predict</code>, <code>predict_proba</code> (classifier), <code>evaluate</code>, and      serialization helpers (<code>save</code>, <code>load</code>). They orchestrate membership      generation, rule configuration, optimizer instantiation, and evaluation.</li> <li>Builders translate estimator configuration into the low-level model by      creating membership functions, calculating rule combinations, and producing      an object the trainers can consume.</li> <li>Low-level model (<code>model.ANFIS</code>, <code>model.TSKANFISClassifier</code>) implements      the forward pass, rule firing strengths, normalization, and consequent      evaluation.</li> <li>Optimizers (<code>optim.*</code>) encapsulate training strategies such as Hybrid      (OLS + gradient descent), Adam, RMSProp, SGD, and PSO. Each trainer accepts a      low-level model along with data and optional validation splits.</li> <li>Utilities provide common infrastructure: <code>estimator_utils</code> for mixins      and input validation, <code>metrics</code> for reporting, <code>losses</code> for objective      functions, and <code>logging_config</code> for opt-in training logs.</li> </ol> <p>The diagram below illustrates the runtime flow:</p> <pre><code>User code -&gt; ANFISRegressor.fit ------------------------------.\n                            |                                  |\n                            v                                  |\n                Membership config     Optimizer selection      |\n                            |                    |             |\n                            v                    v             |\n                builders.ANFISBuilder  optim.&lt;Trainer&gt;         |\n                            |                    |             |\n                            '------&gt; model.ANFIS &lt;-------------'\n                                                               |\n                                                               v\n                                                         Training loop\n</code></pre>"},{"location":"guide/#implementation-and-architecture","title":"Implementation and Architecture","text":"<p>ANFIS Toolbox implements the flow above through a set of small, composable modules. The public estimators in <code>anfis_toolbox.regressor</code> and <code>anfis_toolbox.classifier</code> act as fa\u00e7ades that translate user-friendly configuration into the lower-level computational graph defined in <code>anfis_toolbox.model</code> and <code>anfis_toolbox.layers</code>. Supporting modules such as <code>builders</code>, <code>config</code>, <code>membership</code>, <code>losses</code>, and <code>optim</code> provide reusable building blocks that keep concerns separated.</p>"},{"location":"guide/#core-execution-pipeline-anfis_toolboxmodel-and-anfis_toolboxlayers","title":"Core execution pipeline (<code>anfis_toolbox.model</code> and <code>anfis_toolbox.layers</code>)","text":"<ul> <li>Fuzzification sits in <code>MembershipLayer</code>, which wraps concrete membership-function objects. The layer caches the raw     inputs and per-function activations so that the backward pass can recover gradients without re-evaluating membership     functions.</li> <li>Rule evaluation is performed by <code>RuleLayer</code>. It receives the membership lattice, expands (or filters) the Cartesian     product of membership indices, and computes rule firing strengths using the configured T-norm (product). The layer also     propagates derivatives back to the membership layer by reusing the cached activations.</li> <li>Normalization is handled by <code>NormalizationLayer</code>, which performs numerically stable weight normalisation and exposes a     Jacobian-vector product in its <code>backward</code> method so that trainers can work directly with batched gradients.</li> <li>Consequents are implemented by <code>ConsequentLayer</code> (regression) and <code>ClassificationConsequentLayer</code> (classification).     Both layers augment the input batch with biases, compute per-rule linear consequents, and aggregate them using the     normalised firing strengths. The classification flavour maps the weighted outputs to logits, and integrates with the     built-in softmax helper in <code>metrics.py</code>.</li> <li>Model orchestration happens in <code>TSKANFIS</code> and <code>TSKANFISClassifier</code>. Each model wires the layers, exposes forward and     backward passes, persists gradient buffers, and implements <code>get_parameters</code>, <code>set_parameters</code>, and <code>update_parameters</code> so     optimizers can operate without needing to understand layer internals.</li> </ul>"},{"location":"guide/#builder-and-configuration-layer-builderspy-configpy","title":"Builder and configuration layer (<code>builders.py</code>, <code>config.py</code>)","text":"<ul> <li><code>ANFISBuilder</code> encapsulates membership-function synthesis. It centralises creation strategies (grid, FCM, random) and maps     human-readable names (<code>\"gaussian\"</code>, <code>\"bell\"</code>) to concrete membership classes. The builder also normalises explicit rule     sets via <code>set_rules</code> and exposes a <code>build()</code> factory that returns a fully wired <code>TSKANFIS</code> instance.</li> <li><code>ANFISConfig</code> provides a serialisable description of inputs and training defaults. It can emit builders, write JSON     configurations, and round-trip through <code>ANFISModelManager</code> alongside pickled models. This enables reproducible deployments     or experiment tracking without shipping raw Python objects.</li> <li>Membership families (e.g. <code>GaussianMF</code>, <code>PiMF</code>, <code>DiffSigmoidalMF</code>) live under <code>membership.py</code>. Each object exposes a     <code>parameters</code>/<code>gradients</code> interface so the layers can mutate them generically during backpropagation.</li> </ul>"},{"location":"guide/#estimator-orchestration-regressorpy-classifierpy-estimator_utilspy","title":"Estimator orchestration (<code>regressor.py</code>, <code>classifier.py</code>, <code>estimator_utils.py</code>)","text":"<ul> <li><code>ANFISRegressor.fit</code> and <code>ANFISClassifier.fit</code> delegate input validation and reshaping to helpers in <code>estimator_utils</code>,     ensuring NumPy arrays, pandas DataFrames, and Python iterables are handled consistently.</li> <li>During fitting, estimators resolve per-input overrides through <code>_resolve_input_specs</code>, instantiate an <code>ANFISBuilder</code>,     build the low-level model, select a trainer based on string aliases or explicit objects, and finally call     <code>trainer.fit(model, X, y, **kwargs)</code>. The resulting history dictionary is cached in <code>training_history_</code> for downstream     inspection.</li> <li>Persistence relies on the estimator mixins: <code>save</code>/<code>load</code> methods serialise the estimator and underlying model with     <code>pickle</code>, and <code>format_estimator_repr</code> produces concise <code>__repr__</code> output that mirrors scikit-learn conventions.</li> <li>Logging hooks are opt-in through <code>logging_config.enable_training_logs</code>, so verbose fits emit trainer progress while     remaining silent by default.</li> </ul>"},{"location":"guide/#training-infrastructure-optim","title":"Training infrastructure (<code>optim/</code>)","text":"<ul> <li><code>BaseTrainer</code> defines the contract (<code>fit</code>, <code>evaluate</code>, metric tracking) and shared utilities like batching, progress     reporting, and validation scheduling.</li> <li>Gradient-based trainers (<code>HybridTrainer</code>, <code>HybridAdamTrainer</code>, <code>AdamTrainer</code>, <code>RMSPropTrainer</code>, <code>SGDTrainer</code>) call the     model's <code>forward</code>, <code>backward</code>, <code>update_parameters</code>, and <code>reset_gradients</code> methods directly. Hybrid variants alternate     between closed-form least-squares updates for consequents and gradient descent for membership parameters to accelerate     convergence on regression tasks.</li> <li><code>PSOTrainer</code> diverges from gradient descent by optimising consequent coefficients with particle swarm optimisation while     refreshing membership parameters less aggressively, making it suitable for noisy or non-differentiable objectives.</li> <li>All trainers populate a <code>TrainingHistory</code> mapping (defined in <code>optim.base</code>) that captures per-epoch losses and optional     validation metrics, facilitating visualisation or early-stopping criteria implemented outside the core package.</li> </ul>"},{"location":"guide/#variants-and-extension-points","title":"Variants and extension points","text":"<ul> <li>Regression vs. classification: <code>ANFISRegressor</code> wraps <code>TSKANFIS</code>, using mean-squared-error-style losses by default and     exposing regression metrics. <code>ANFISClassifier</code> wraps <code>TSKANFISClassifier</code>, instantiates a multi-class consequent layer,     and defaults to cross-entropy losses with probability calibration via <code>predict_proba</code>.</li> <li>Membership variants: Users can mix automatic generation (<code>n_mfs</code>, <code>mf_type</code>, <code>init</code>, <code>overlap</code>, <code>margin</code>) with explicit     <code>MembershipFunction</code> instances per feature. Builders preserve the order in which inputs are added so the resulting rule     indices remain deterministic.</li> <li>Rule customisation: Providing <code>rules=[(...)]</code> to estimators or builders prunes the Cartesian-product rule set. The     <code>RuleLayer</code> validates dimensionality and membership bounds so sparsified rule bases remain consistent.</li> <li>Custom trainers and losses: Passing a subclass of <code>BaseTrainer</code> or a callable <code>LossFunction</code> lets advanced users swap     in bespoke optimisation strategies. Trainers may register additional callbacks or hooks (for example, to integrate with     <code>docs/hooks/*</code> when rendering docs) without patching the core model.</li> <li>Configuration persistence: <code>ANFISModelManager</code> ties trained models to the serialised configuration extracted from the     membership catalogue, enabling reproducible evaluation environments and lightweight deployment artefacts.</li> </ul>"},{"location":"guide/#uml-overview","title":"UML overview","text":"<pre><code>classDiagram\n    direction LR\n    class ANFISRegressor {\n        +fit(X, y)\n        +predict(X)\n        +evaluate(X, y)\n        +save(path)\n        +load(path)\n    }\n    class ANFISClassifier {\n        +fit(X, y)\n        +predict(X)\n        +predict_proba(X)\n        +evaluate(X, y)\n    }\n    class ANFISBuilder {\n        +add_input(name, min, max, n_mfs, mf_type)\n        +add_input_from_data(name, data, ...)\n        +set_rules(rules)\n        +build()\n    }\n    class TSKANFIS {\n        +forward(X)\n        +backward(dL)\n        +fit(X, y, trainer)\n        +get_parameters()\n    }\n    class TSKANFISClassifier {\n        +forward(X)\n        +backward(dL)\n        +predict_logits(X)\n        +get_parameters()\n    }\n    class MembershipLayer {\n        +forward(X)\n        +backward(gradients)\n    }\n    class RuleLayer {\n        +forward(mu)\n        +backward(dL)\n    }\n    class NormalizationLayer {\n        +forward(weights)\n        +backward(dL)\n    }\n    class ConsequentLayer {\n        +forward(X, w)\n        +backward(dL)\n    }\n    class ClassificationConsequentLayer {\n        +forward(X, w)\n        +backward(dL)\n    }\n    class BaseTrainer {\n        &lt;&lt;abstract&gt;&gt;\n        +fit(model, X, y)\n    }\n    class HybridTrainer\n    class HybridAdamTrainer\n    class AdamTrainer\n    class RMSPropTrainer\n    class SGDTrainer\n    class PSOTrainer\n\n    ANFISRegressor --&gt; ANFISBuilder : configures\n    ANFISClassifier --&gt; ANFISBuilder : configures\n    ANFISBuilder --&gt; TSKANFIS : builds\n    ANFISBuilder --&gt; TSKANFISClassifier : builds\n    TSKANFIS --&gt; MembershipLayer : composes\n    TSKANFIS --&gt; RuleLayer\n    TSKANFIS --&gt; NormalizationLayer\n    TSKANFIS --&gt; ConsequentLayer\n    TSKANFISClassifier --&gt; MembershipLayer\n    TSKANFISClassifier --&gt; RuleLayer\n    TSKANFISClassifier --&gt; NormalizationLayer\n    TSKANFISClassifier --&gt; ClassificationConsequentLayer\n    ANFISRegressor --&gt; BaseTrainer : selects\n    ANFISClassifier --&gt; BaseTrainer : selects\n    BaseTrainer &lt;|-- HybridTrainer\n    BaseTrainer &lt;|-- HybridAdamTrainer\n    BaseTrainer &lt;|-- AdamTrainer\n    BaseTrainer &lt;|-- RMSPropTrainer\n    BaseTrainer &lt;|-- SGDTrainer\n    BaseTrainer &lt;|-- PSOTrainer</code></pre>"},{"location":"guide/#working-with-estimators","title":"Working With Estimators","text":"<ul> <li>Initialization: Provide global defaults (<code>n_mfs</code>, <code>mf_type</code>, <code>init</code>,     <code>overlap</code>, <code>margin</code>) plus optional <code>inputs_config</code> overrides for each input.     <code>inputs_config</code> values can be dictionaries with membership parameters or     explicit <code>MembershipFunction</code> instances.</li> <li>Rules: By default, all membership combinations form rules. Supply     <code>rules=[(i1, i2, ...)]</code> to restrict to a subset of combinations.</li> <li>Optimizers: Pass <code>optimizer=\"adam\"</code> (default classifier) or     <code>optimizer=\"hybrid\"</code> (default regressor). You can also supply instantiated     trainers or custom subclasses of <code>BaseTrainer</code>.</li> <li>Training: <code>fit</code> accepts NumPy arrays, array-like objects, or pandas     DataFrames. Use <code>validation_data=(X_val, y_val)</code> to monitor generalization.</li> <li>Evaluation: <code>evaluate</code> returns a metrics dictionary and optionally prints     a nicely formatted summary. Use <code>return_dict=False</code> to suppress the return     value.</li> <li>Persistence: <code>save</code> and <code>load</code> rely on <code>pickle</code>. Saved estimators capture     fitted membership functions, rules, and optimizer state, enabling reuse.</li> </ul>"},{"location":"guide/#membership-functions","title":"Membership Functions","text":"<p>Membership families live in <code>membership.py</code>. Key points:</p> <ul> <li>Each family subclasses <code>MembershipFunction</code> and implements <code>__call__</code>,     <code>derivative</code>, and metadata accessors.</li> <li>Many families accept parameters such as centers, widths, slopes, or plateaus.</li> <li>Builders automatically infer parameters using grid spacing, fuzzy C-means     clustering, or random sampling depending on <code>init</code>.</li> <li> <p>Provide explicit membership functions via <code>inputs_config</code> to lock down     shapes, e.g.:</p> <pre><code>from anfis_toolbox.membership import GaussianMF\n\ninputs_config = {\n        0: {\"membership_functions\": [GaussianMF(mean=-1, sigma=0.3), GaussianMF(mean=1, sigma=0.3)]},\n        1: 4,  # shorthand for n_mfs=4 using estimator defaults\n}\n</code></pre> </li> </ul>"},{"location":"guide/#training-strategies","title":"Training Strategies","text":"<p>Optimizers live under <code>anfis_toolbox/optim/</code> and share a common interface:</p> <ul> <li><code>BaseTrainer.fit(model, X, y, **kwargs)</code> drives epochs, batching, shuffling,     and validation.</li> <li>Hybrid trainers combine gradient descent with ordinary least squares rule     consequent updates, delivering fast convergence on regression tasks.</li> <li>Adam, RMSProp, and SGD offer familiar gradient-based alternatives.</li> <li>PSO provides a population-based search when gradient information is noisy.</li> <li>Trainers expose hooks for learning rate, epochs, batch size, shuffle, and     optional loss overrides.</li> </ul>"},{"location":"guide/#metrics-and-evaluation","title":"Metrics and Evaluation","text":"<ul> <li><code>ANFISRegressor.evaluate</code> reports MSE, RMSE, MAE, and R\u00b2 via     <code>metrics.ANFISMetrics.regression_metrics</code>.</li> <li><code>ANFISClassifier.evaluate</code> reports accuracy, balanced accuracy, macro/micro     precision/recall/F1, and the confusion matrix.</li> <li>Metrics are returned as dictionaries to simplify logging or experiment     tracking.</li> </ul>"},{"location":"guide/#examples-and-documentation","title":"Examples and Documentation","text":"<ul> <li>Explore the <code>docs/examples/</code> notebooks for step-by-step tutorials covering     regression, classification, time series, and membership customization.</li> <li>The MkDocs site (<code>mkdocs.yml</code>) assembles the <code>docs/</code> directory into a hosted     documentation portal. Run <code>make docs</code> to build locally and serve via     <code>mkdocs serve</code>.</li> </ul>"},{"location":"guide/#development-workflow","title":"Development Workflow","text":"<ol> <li>Install dependencies: <code>make install</code></li> <li>Run tests: <code>make test</code></li> <li>Lint: <code>make lint</code></li> <li>Format (optional): <code>make format</code></li> <li>Docs preview: <code>make docs</code></li> </ol> <p>Tests cover membership functions, optimizers, estimators, and integration with scikit-learn-like patterns. New features should include corresponding tests.</p>"},{"location":"guide/#contributing","title":"Contributing","text":"<ul> <li>Fork the repository and create a feature branch.</li> <li>Keep pull requests focused; tie them to an issue when possible.</li> <li>Update or add documentation (including this guide) when behavior changes.</li> <li>Ensure <code>make test</code> and <code>make lint</code> pass before submitting.</li> <li>Include demo snippets or notebooks if the feature benefits from examples.</li> </ul> <p>Thanks for contributing to ANFIS Toolbox! If you have questions, open a discussion or issue on GitHub.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section documents the public surface of ANFIS Toolbox. Use it alongside the user guides and examples when you need precise signatures, parameters, and return types.</p>"},{"location":"api/#estimators","title":"Estimators","text":"<ul> <li><code>ANFISRegressor</code> \u2013 Scikit-learn style interface for     Takagi\u2013Sugeno\u2013Kang regression.</li> <li><code>ANFISClassifier</code> \u2013 Classification counterpart with     probability predictions and evaluation helpers.</li> </ul>"},{"location":"api/#membership-functions","title":"Membership Functions","text":"<p>Thirteen membership function families covering Gaussian, bell, sigmoidal, and piecewise-linear shapes are documented in <code>membership-functions.md</code>. Each entry includes parameters, derivative support, and usage examples.</p>"},{"location":"api/#training","title":"Training","text":"<ul> <li>Optimizers \u2013 Gradient-based, hybrid, and swarm trainers are described in     <code>optim.md</code> with configuration notes and supported hyper-parameters.</li> <li>Losses \u2013 Regression and classification objectives (and their gradients)     are listed in <code>losses.md</code>.</li> </ul>"},{"location":"api/#metrics","title":"Metrics","text":"<p>Evaluation helpers for regression, classification, and clustering are grouped in <code>metrics.md</code>. Each function documents expected inputs and output formats so you can integrate metrics into experiments or monitoring.</p>"},{"location":"api/#core-internals","title":"Core Internals","text":"<ul> <li>Models \u2013 Low-level ANFIS graph classes and their rule representations live     in <code>models.md</code>.</li> <li>Layers \u2013 Individual computational layers and their tensor operations are     explained in <code>layers.md</code>.</li> </ul> <p>These pages are useful when you need to inspect or extend the internal pipeline that powers the high-level estimators.</p>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li>Configuration \u2013 Utilities for persisting and replaying setups appear in     <code>config.md</code>.</li> <li>Logging \u2013 Structured training logs and logging configuration are covered     in <code>logging.md</code>.</li> </ul>"},{"location":"api/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Builders \u2013 Advanced model construction hooks are described in     <code>builders.md</code>. Most users can rely on estimator defaults.</li> <li>Clustering \u2013 The fuzzy C-means implementation used for membership     initialization is detailed in <code>clustering.md</code>.</li> </ul>"},{"location":"api/#where-to-start","title":"Where to Start","text":"<ul> <li>New to ANFIS Toolbox? Begin with the models overview.</li> <li>Looking for ready-to-run notebooks? Browse the Examples section in the     navigation.</li> <li>Exploring code while reading docs? The \u201cView source\u201d actions in each page jump     straight to the implementation.</li> </ul>"},{"location":"api/builders/","title":"Builders","text":""},{"location":"api/builders/#anfis_toolbox.builders.ANFISBuilder","title":"anfis_toolbox.builders.ANFISBuilder","text":"<pre><code>ANFISBuilder()\n</code></pre> <p>Builder class for creating ANFIS models with intuitive API.</p>"},{"location":"api/builders/#anfis_toolbox.builders.ANFISBuilder.add_input","title":"add_input","text":"<pre><code>add_input(\n    name: str,\n    range_min: float,\n    range_max: float,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n) -&gt; ANFISBuilder\n</code></pre> <p>Add an input variable with automatic membership function generation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the input variable</p> required <code>range_min</code> <code>float</code> <p>Minimum value of the input range</p> required <code>range_max</code> <code>float</code> <p>Maximum value of the input range</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions (default: 3)</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Type of membership functions. Supported: 'gaussian', 'gaussian2', 'triangular', 'trapezoidal', 'bell', 'sigmoidal', 'sshape', 'zshape', 'pi'</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor between adjacent MFs (0.0 to 1.0)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ANFISBuilder</code> <p>Self for method chaining</p>"},{"location":"api/builders/#anfis_toolbox.builders.ANFISBuilder.add_input_from_data","title":"add_input_from_data","text":"<pre><code>add_input_from_data(\n    name: str,\n    data: ndarray,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    init: str | None = \"grid\",\n    random_state: int | None = None,\n) -&gt; ANFISBuilder\n</code></pre> <p>Add an input inferring range_min/range_max from data with a margin.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Input name</p> required <code>data</code> <code>ndarray</code> <p>1D array-like samples for this input</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Membership function type (see add_input)</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor between adjacent MFs</p> <code>0.5</code> <code>margin</code> <code>float</code> <p>Fraction of (max-min) to pad on each side</p> <code>0.1</code> <code>init</code> <code>str | None</code> <p>Initialization strategy: \"grid\" (default), \"fcm\", \"random\", or <code>None</code>. When <code>\"fcm\"</code>, clusters from the data determine MF centers and widths (supports 'gaussian' and 'bell').</p> <code>'grid'</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for deterministic FCM initialization.</p> <code>None</code>"},{"location":"api/builders/#anfis_toolbox.builders.ANFISBuilder.build","title":"build","text":"<pre><code>build() -&gt; ANFIS\n</code></pre> <p>Build the ANFIS model with configured parameters.</p>"},{"location":"api/builders/#anfis_toolbox.builders.ANFISBuilder.set_rules","title":"set_rules","text":"<pre><code>set_rules(\n    rules: Sequence[Sequence[int]] | None,\n) -&gt; ANFISBuilder\n</code></pre> <p>Define an explicit set of fuzzy rules to use when building the model.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>Sequence[Sequence[int]] | None</code> <p>Iterable of rules where each rule lists the membership index per input. <code>None</code> removes any previously configured custom rules and restores the default Cartesian-product behaviour.</p> required <p>Returns:</p> Type Description <code>ANFISBuilder</code> <p>Self for method chaining.</p>"},{"location":"api/classifier/","title":"Classifier API","text":"<p>The <code>ANFISClassifier</code> offers a scikit-learn inspired interface for multi-class classification tasks, wrapping membership-function management, model construction, and training into a single estimator.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier","title":"anfis_toolbox.classifier.ANFISClassifier","text":"<pre><code>ANFISClassifier(\n    *,\n    n_classes: int | None = None,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str\n    | BaseTrainer\n    | type[BaseTrainer]\n    | None = \"adam\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimatorLike</code>, <code>FittedMixin</code>, <code>ClassifierMixinLike</code></p> <p>Adaptive Neuro-Fuzzy classifier with a scikit-learn style API.</p> <p>The estimator manages membership-function synthesis, rule construction, and trainer selection so you can focus on calling :meth:<code>fit</code>, :meth:<code>predict</code>, :meth:<code>predict_proba</code>, and :meth:<code>evaluate</code> with familiar NumPy-like data structures.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier--examples","title":"Examples:","text":"<p>clf = ANFISClassifier() clf.fit(X, y) ANFISClassifier(...) clf.predict([[0.1, -0.2]]) array([...])</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier--parameters","title":"Parameters","text":"<p>n_classes : int, optional     Number of target classes. Must be &gt;= 2 when provided. If omitted, the     classifier infers the class count during the first call to <code>fit</code>. n_mfs : int, default=3     Default number of membership functions per input. mf_type : str, default=\"gaussian\"     Default membership function family applied when membership functions are     inferred from data. init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Strategy used when inferring membership functions from data. <code>None</code>     falls back to <code>\"grid\"</code>. overlap : float, default=0.5     Controls overlap when generating membership functions automatically. margin : float, default=0.10     Margin added around observed data ranges during grid initialization. inputs_config : Mapping, optional     Per-input overrides. Keys may be feature names (when <code>X</code> is a     :class:<code>pandas.DataFrame</code>) or integer indices. Values may be:</p> <pre><code>* ``dict`` with keys among ``{\"n_mfs\", \"mf_type\", \"init\", \"overlap\",\n  \"margin\", \"range\", \"membership_functions\", \"mfs\"}``.\n* A list or tuple of membership function objects for full control.\n* ``None`` for defaults.\n</code></pre> <p>random_state : int, optional     Random state forwarded to initialization routines and stochastic     optimizers. optimizer : str, BaseTrainer, type[BaseTrainer], or None, default=\"adam\"     Trainer identifier or instance used for fitting. Strings map to entries     in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to \"adam\". optimizer_params : Mapping, optional     Additional keyword arguments forwarded to the trainer constructor. learning_rate, epochs, batch_size, shuffle, verbose : optional scalars     Common trainer hyper-parameters provided for convenience. When the     selected trainer supports the parameter it is included automatically. loss : str or LossFunction, optional     Custom loss forwarded to trainers that expose a <code>loss</code> parameter. rules : Sequence[Sequence[int]] | None, optional     Explicit fuzzy rule indices to use instead of the full Cartesian product. Each     rule lists the membership-function index per input. <code>None</code> keeps the default     exhaustive rule set.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier--parameters","title":"Parameters","text":"<p>n_classes : int, optional     Number of output classes. Must be at least two when provided. If     omitted, the value is inferred from the training targets during     the first <code>fit</code> call. n_mfs : int, default=3     Default number of membership functions to allocate per input when     inferred from data. mf_type : str, default=\"gaussian\"     Membership function family used for automatically generated     membership functions. init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Initialization strategy applied when synthesizing membership     functions from the training data. <code>None</code> falls back to <code>\"grid\"</code>. overlap : float, default=0.5     Desired overlap between adjacent membership functions during     automatic generation. margin : float, default=0.10     Additional range padding applied around observed feature minima     and maxima for grid initialization. inputs_config : Mapping, optional     Per-feature overrides for the generated membership functions.     Keys may be feature names (when <code>X</code> is a :class:<code>pandas.DataFrame</code>),     integer indices, or <code>\"x{i}\"</code> aliases. Values may include dictionaries     with membership-generation arguments, explicit membership function     sequences, or <code>None</code> to retain defaults. random_state : int, optional     Seed forwarded to stochastic initializers and optimizers. optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"adam\"     Training algorithm identifier or instance. String aliases are looked     up in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to <code>\"adam\"</code>.     Hybrid variants that depend on least-squares refinements are limited     to regression and raise <code>ValueError</code> when supplied here. optimizer_params : Mapping, optional     Additional keyword arguments provided to the trainer constructor     when a string alias or trainer class is supplied. learning_rate, epochs, batch_size, shuffle, verbose : optional     Convenience hyper-parameters injected into the trainer whenever the     chosen implementation accepts them. <code>shuffle</code> supports <code>False</code>     to disable random shuffling. loss : str | LossFunction, optional     Custom loss specification forwarded to trainers that expose a     <code>loss</code> parameter. <code>None</code> resolves to cross-entropy. rules : Sequence[Sequence[int]] | None, optional     Optional explicit fuzzy rule definitions. Each rule lists the     membership-function index for each input. <code>None</code> uses the full     Cartesian product of configured membership functions.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def __init__(\n    self,\n    *,\n    n_classes: int | None = None,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.10,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str | BaseTrainer | type[BaseTrainer] | None = \"adam\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n) -&gt; None:\n    \"\"\"Configure an :class:`ANFISClassifier` with the supplied hyper-parameters.\n\n    Parameters\n    ----------\n    n_classes : int, optional\n        Number of output classes. Must be at least two when provided. If\n        omitted, the value is inferred from the training targets during\n        the first ``fit`` call.\n    n_mfs : int, default=3\n        Default number of membership functions to allocate per input when\n        inferred from data.\n    mf_type : str, default=\"gaussian\"\n        Membership function family used for automatically generated\n        membership functions.\n    init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"\n        Initialization strategy applied when synthesizing membership\n        functions from the training data. ``None`` falls back to ``\"grid\"``.\n    overlap : float, default=0.5\n        Desired overlap between adjacent membership functions during\n        automatic generation.\n    margin : float, default=0.10\n        Additional range padding applied around observed feature minima\n        and maxima for grid initialization.\n    inputs_config : Mapping, optional\n        Per-feature overrides for the generated membership functions.\n        Keys may be feature names (when ``X`` is a :class:`pandas.DataFrame`),\n        integer indices, or ``\"x{i}\"`` aliases. Values may include dictionaries\n        with membership-generation arguments, explicit membership function\n        sequences, or ``None`` to retain defaults.\n    random_state : int, optional\n        Seed forwarded to stochastic initializers and optimizers.\n    optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"adam\"\n        Training algorithm identifier or instance. String aliases are looked\n        up in :data:`TRAINER_REGISTRY`. ``None`` defaults to ``\"adam\"``.\n        Hybrid variants that depend on least-squares refinements are limited\n        to regression and raise ``ValueError`` when supplied here.\n    optimizer_params : Mapping, optional\n        Additional keyword arguments provided to the trainer constructor\n        when a string alias or trainer class is supplied.\n    learning_rate, epochs, batch_size, shuffle, verbose : optional\n        Convenience hyper-parameters injected into the trainer whenever the\n        chosen implementation accepts them. ``shuffle`` supports ``False``\n        to disable random shuffling.\n    loss : str | LossFunction, optional\n        Custom loss specification forwarded to trainers that expose a\n        ``loss`` parameter. ``None`` resolves to cross-entropy.\n    rules : Sequence[Sequence[int]] | None, optional\n        Optional explicit fuzzy rule definitions. Each rule lists the\n        membership-function index for each input. ``None`` uses the full\n        Cartesian product of configured membership functions.\n    \"\"\"\n    if n_classes is not None and int(n_classes) &lt; 2:\n        raise ValueError(\"n_classes must be &gt;= 2\")\n    self.n_classes: int | None = int(n_classes) if n_classes is not None else None\n    self.n_mfs = int(n_mfs)\n    self.mf_type = str(mf_type)\n    self.init = None if init is None else str(init)\n    self.overlap = float(overlap)\n    self.margin = float(margin)\n    self.inputs_config = dict(inputs_config) if inputs_config is not None else None\n    self.random_state = random_state\n    self.optimizer = optimizer\n    self.optimizer_params = dict(optimizer_params) if optimizer_params is not None else None\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    self.verbose = verbose\n    self.loss = loss\n    self.rules = None if rules is None else tuple(tuple(int(idx) for idx in rule) for rule in rules)\n\n    # Fitted attributes (initialised during fit)\n    self.model_: LowLevelANFISClassifier | None = None\n    self.optimizer_: BaseTrainer | None = None\n    self.feature_names_in_: list[str] | None = None\n    self.n_features_in_: int | None = None\n    self.training_history_: TrainingHistory | None = None\n    self.input_specs_: list[dict[str, Any]] | None = None\n    self.classes_: np.ndarray | None = None\n    self._class_to_index_: dict[Any, int] | None = None\n    self.rules_: list[tuple[int, ...]] | None = None\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return a formatted representation summarising configuration and fitted artefacts.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a formatted representation summarising configuration and fitted artefacts.\"\"\"\n    return format_estimator_repr(\n        type(self).__name__,\n        self._repr_config_pairs(),\n        self._repr_children_entries(),\n    )\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    X,\n    y,\n    *,\n    return_dict: bool = True,\n    print_results: bool = True,\n)\n</code></pre> <p>Evaluate predictive performance on a labelled dataset.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate--parameters","title":"Parameters","text":"<p>X : array-like     Evaluation inputs. y : array-like     Ground-truth labels. Accepts integer labels or one-hot encodings. return_dict : bool, default=True     When <code>True</code> return the computed metric dictionary; when <code>False</code>     return <code>None</code> after optional printing. print_results : bool, default=True     Emit a formatted summary to stdout. Set to <code>False</code> to suppress     printing.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate--returns","title":"Returns:","text":"<p>dict[str, float] | None     Dictionary containing accuracy, balanced accuracy, macro/micro     precision/recall/F1 scores, and the confusion matrix when     <code>return_dict</code> is <code>True</code>; otherwise <code>None</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate--raises","title":"Raises:","text":"<p>RuntimeError     If called before the estimator has been fitted. ValueError     When <code>X</code> and <code>y</code> disagree on sample count or labels are     incompatible with the configured class count.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def evaluate(self, X, y, *, return_dict: bool = True, print_results: bool = True):\n    \"\"\"Evaluate predictive performance on a labelled dataset.\n\n    Parameters\n    ----------\n    X : array-like\n        Evaluation inputs.\n    y : array-like\n        Ground-truth labels. Accepts integer labels or one-hot encodings.\n    return_dict : bool, default=True\n        When ``True`` return the computed metric dictionary; when ``False``\n        return ``None`` after optional printing.\n    print_results : bool, default=True\n        Emit a formatted summary to stdout. Set to ``False`` to suppress\n        printing.\n\n    Returns:\n    -------\n    dict[str, float] | None\n        Dictionary containing accuracy, balanced accuracy, macro/micro\n        precision/recall/F1 scores, and the confusion matrix when\n        ``return_dict`` is ``True``; otherwise ``None``.\n\n    Raises:\n    ------\n    RuntimeError\n        If called before the estimator has been fitted.\n    ValueError\n        When ``X`` and ``y`` disagree on sample count or labels are\n        incompatible with the configured class count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr, _ = _ensure_2d_array(X)\n    encoded_targets, _ = self._encode_targets(y, X_arr.shape[0], allow_partial_classes=True)\n    proba = self.predict_proba(X_arr)\n    metrics = ANFISMetrics.classification_metrics(encoded_targets, proba)\n    metrics.pop(\"log_loss\", None)\n    if print_results:\n\n        def _is_effectively_nan(value: Any) -&gt; bool:\n            if value is None:\n                return True\n            if isinstance(value, (float, np.floating)):\n                return bool(np.isnan(value))\n            if isinstance(value, (int, np.integer)):\n                return False\n            if isinstance(value, np.ndarray):\n                if value.size == 0:\n                    return False\n                if np.issubdtype(value.dtype, np.number):\n                    return bool(np.isnan(value.astype(float)).all())\n                return False\n            return False\n\n        print(\"ANFISClassifier evaluation:\")  # noqa: T201\n        for key, value in metrics.items():\n            if _is_effectively_nan(value):\n                continue\n            if isinstance(value, (float, np.floating)):\n                display_value = f\"{float(value):.6f}\"\n                print(f\"  {key}: {display_value}\")  # noqa: T201\n            elif isinstance(value, (int, np.integer)):\n                print(f\"  {key}: {int(value)}\")  # noqa: T201\n            elif isinstance(value, np.ndarray):\n                array_repr = np.array2string(value, precision=6, suppress_small=True)\n                if \"\\n\" in array_repr:\n                    indented = \"\\n    \".join(array_repr.splitlines())\n                    print(f\"  {key}:\\n    {indented}\")  # noqa: T201\n                else:\n                    print(f\"  {key}: {array_repr}\")  # noqa: T201\n            else:\n                print(f\"  {key}: {value}\")  # noqa: T201\n    return metrics if return_dict else None\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    *,\n    validation_data: tuple[ndarray, ndarray] | None = None,\n    validation_frequency: int = 1,\n    verbose: bool | None = None,\n    **fit_params: Any,\n) -&gt; ANFISClassifier\n</code></pre> <p>Fit the classifier on labelled data.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit--parameters","title":"Parameters","text":"<p>X : array-like     Training inputs with shape <code>(n_samples, n_features)</code>. y : array-like     Target labels. Accepts integer or string labels as well as one-hot     matrices with shape <code>(n_samples, n_classes)</code>. validation_data : tuple[np.ndarray, np.ndarray], optional     Optional validation split supplied to the underlying trainer.     Inputs and targets must already be numeric and share the same row     count. validation_frequency : int, default=1     Frequency (in epochs) at which validation metrics are computed when     <code>validation_data</code> is provided. verbose : bool, optional     Override the estimator's <code>verbose</code> flag for this fit call. When     provided, the value is stored on the estimator and forwarded to the     trainer configuration. **fit_params : Any     Additional keyword arguments forwarded directly to the trainer     <code>fit</code> method.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit--returns","title":"Returns:","text":"<p>ANFISClassifier     Reference to <code>self</code> to enable fluent-style chaining.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit--raises","title":"Raises:","text":"<p>ValueError     If the input arrays disagree on the number of samples or the label     encoding is incompatible with the configured <code>n_classes</code>. TypeError     If the trainer <code>fit</code> implementation does not return a     dictionary-style training history.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    *,\n    validation_data: tuple[np.ndarray, np.ndarray] | None = None,\n    validation_frequency: int = 1,\n    verbose: bool | None = None,\n    **fit_params: Any,\n) -&gt; ANFISClassifier:\n    \"\"\"Fit the classifier on labelled data.\n\n    Parameters\n    ----------\n    X : array-like\n        Training inputs with shape ``(n_samples, n_features)``.\n    y : array-like\n        Target labels. Accepts integer or string labels as well as one-hot\n        matrices with shape ``(n_samples, n_classes)``.\n    validation_data : tuple[np.ndarray, np.ndarray], optional\n        Optional validation split supplied to the underlying trainer.\n        Inputs and targets must already be numeric and share the same row\n        count.\n    validation_frequency : int, default=1\n        Frequency (in epochs) at which validation metrics are computed when\n        ``validation_data`` is provided.\n    verbose : bool, optional\n        Override the estimator's ``verbose`` flag for this fit call. When\n        provided, the value is stored on the estimator and forwarded to the\n        trainer configuration.\n    **fit_params : Any\n        Additional keyword arguments forwarded directly to the trainer\n        ``fit`` method.\n\n    Returns:\n    -------\n    ANFISClassifier\n        Reference to ``self`` to enable fluent-style chaining.\n\n    Raises:\n    ------\n    ValueError\n        If the input arrays disagree on the number of samples or the label\n        encoding is incompatible with the configured ``n_classes``.\n    TypeError\n        If the trainer ``fit`` implementation does not return a\n        dictionary-style training history.\n    \"\"\"\n    X_arr, feature_names = _ensure_2d_array(X)\n    n_samples = X_arr.shape[0]\n    y_encoded, classes = self._encode_targets(y, n_samples)\n\n    self.classes_ = classes\n    self._class_to_index_ = {self._normalize_class_key(cls): idx for idx, cls in enumerate(classes.tolist())}\n\n    self.feature_names_in_ = feature_names\n    self.n_features_in_ = X_arr.shape[1]\n    self.input_specs_ = self._resolve_input_specs(feature_names)\n\n    if verbose is not None:\n        self.verbose = bool(verbose)\n\n    _ensure_training_logging(self.verbose)\n    if self.n_classes is None:\n        raise RuntimeError(\"n_classes could not be inferred from the provided targets\")\n    self.model_ = self._build_model(X_arr, feature_names)\n    trainer = self._instantiate_trainer()\n    self.optimizer_ = trainer\n    trainer_kwargs: dict[str, Any] = dict(fit_params)\n    if validation_data is not None:\n        trainer_kwargs.setdefault(\"validation_data\", validation_data)\n    if validation_data is not None or validation_frequency != 1:\n        trainer_kwargs.setdefault(\"validation_frequency\", validation_frequency)\n\n    history = trainer.fit(self.model_, X_arr, y_encoded, **trainer_kwargs)\n    if not isinstance(history, dict):\n        raise TypeError(\"Trainer.fit must return a TrainingHistory dictionary\")\n    self.training_history_ = history\n    self.rules_ = self.model_.rules\n\n    self._mark_fitted()\n    return self\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.get_rules","title":"get_rules","text":"<pre><code>get_rules() -&gt; tuple[tuple[int, ...], ...]\n</code></pre> <p>Return the fuzzy rule index combinations used by the fitted model.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.get_rules--returns","title":"Returns:","text":"<p>tuple[tuple[int, ...], ...]     Immutable tuple describing each fuzzy rule as a per-input     membership index.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.get_rules--raises","title":"Raises:","text":"<p>RuntimeError     If invoked before <code>fit</code> completes.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def get_rules(self) -&gt; tuple[tuple[int, ...], ...]:\n    \"\"\"Return the fuzzy rule index combinations used by the fitted model.\n\n    Returns:\n    -------\n    tuple[tuple[int, ...], ...]\n        Immutable tuple describing each fuzzy rule as a per-input\n        membership index.\n\n    Raises:\n    ------\n    RuntimeError\n        If invoked before ``fit`` completes.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"rules_\"])\n    if not self.rules_:\n        return ()\n    return tuple(tuple(rule) for rule in self.rules_)\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: str | Path) -&gt; ANFISClassifier\n</code></pre> <p>Load a pickled <code>ANFISClassifier</code> from <code>filepath</code> and validate its type.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str | Path) -&gt; ANFISClassifier:\n    \"\"\"Load a pickled ``ANFISClassifier`` from ``filepath`` and validate its type.\"\"\"\n    path = Path(filepath)\n    with path.open(\"rb\") as stream:\n        estimator = pickle.load(stream)\n    if not isinstance(estimator, cls):\n        raise TypeError(f\"Expected pickled {cls.__name__} instance, got {type(estimator).__name__}.\")\n    return estimator\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels for the provided samples.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict--parameters","title":"Parameters","text":"<p>X : array-like     Samples to classify. One-dimensional arrays are treated as a single     sample; two-dimensional arrays must have shape <code>(n_samples, n_features)</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict--returns","title":"Returns:","text":"<p>np.ndarray     Predicted class labels with shape <code>(n_samples,)</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict--raises","title":"Raises:","text":"<p>RuntimeError     If invoked before the estimator is fitted. ValueError     When the supplied samples do not match the fitted feature count.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for the provided samples.\n\n    Parameters\n    ----------\n    X : array-like\n        Samples to classify. One-dimensional arrays are treated as a single\n        sample; two-dimensional arrays must have shape ``(n_samples, n_features)``.\n\n    Returns:\n    -------\n    np.ndarray\n        Predicted class labels with shape ``(n_samples,)``.\n\n    Raises:\n    ------\n    RuntimeError\n        If invoked before the estimator is fitted.\n    ValueError\n        When the supplied samples do not match the fitted feature count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\", \"classes_\"])\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n    else:\n        X_arr, _ = _ensure_2d_array(X)\n\n    if self.n_features_in_ is None:\n        raise RuntimeError(\"Model must be fitted before calling predict.\")\n    if X_arr.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Feature mismatch: expected {self.n_features_in_}, got {X_arr.shape[1]}.\")\n    model = self.model_\n    classes = self.classes_\n    if model is None or classes is None:\n        raise RuntimeError(\"Model must be fitted before calling predict.\")\n    encoded = np.asarray(model.predict(X_arr), dtype=int)\n    return np.asarray(classes)[encoded]\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities for the provided samples.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba--parameters","title":"Parameters","text":"<p>X : array-like     Samples for which to estimate class probabilities.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba--returns","title":"Returns:","text":"<p>np.ndarray     Matrix of shape <code>(n_samples, n_classes)</code> containing class     probability estimates.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba--raises","title":"Raises:","text":"<p>RuntimeError     If the estimator has not been fitted. ValueError     If sample dimensionality does not match the fitted feature count.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for the provided samples.\n\n    Parameters\n    ----------\n    X : array-like\n        Samples for which to estimate class probabilities.\n\n    Returns:\n    -------\n    np.ndarray\n        Matrix of shape ``(n_samples, n_classes)`` containing class\n        probability estimates.\n\n    Raises:\n    ------\n    RuntimeError\n        If the estimator has not been fitted.\n    ValueError\n        If sample dimensionality does not match the fitted feature count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n    else:\n        X_arr, _ = _ensure_2d_array(X)\n\n    if self.n_features_in_ is None:\n        raise RuntimeError(\"Model must be fitted before calling predict_proba.\")\n    if X_arr.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Feature mismatch: expected {self.n_features_in_}, got {X_arr.shape[1]}.\")\n    model = self.model_\n    if model is None:\n        raise RuntimeError(\"Model must be fitted before calling predict_proba.\")\n    return np.asarray(model.predict_proba(X_arr), dtype=float)\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.save","title":"save","text":"<pre><code>save(filepath: str | Path) -&gt; None\n</code></pre> <p>Serialize this estimator (including fitted artefacts) to <code>filepath</code>.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def save(self, filepath: str | Path) -&gt; None:\n    \"\"\"Serialize this estimator (including fitted artefacts) to ``filepath``.\"\"\"\n    path = Path(filepath)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"wb\") as stream:\n        pickle.dump(self, stream)\n</code></pre>"},{"location":"api/clustering/","title":"Clustering","text":""},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans","title":"anfis_toolbox.clustering.FuzzyCMeans","text":"<pre><code>FuzzyCMeans(\n    n_clusters: int,\n    m: float = 2.0,\n    max_iter: int = 300,\n    tol: float = 0.0001,\n    random_state: int | None = None,\n)\n</code></pre> <p>Fuzzy C-Means clustering.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters (&gt;= 2).</p> required <code>m</code> <code>float</code> <p>Fuzzifier (&gt; 1). Default 2.0.</p> <code>2.0</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations.</p> <code>300</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on centers.</p> <code>0.0001</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for reproducibility.</p> <code>None</code> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def __init__(\n    self,\n    n_clusters: int,\n    m: float = 2.0,\n    max_iter: int = 300,\n    tol: float = 1e-4,\n    random_state: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize FuzzyCMeans with hyperparameters.\"\"\"\n    if n_clusters &lt; 2:\n        raise ValueError(\"n_clusters must be &gt;= 2\")\n    if m &lt;= 1:\n        raise ValueError(\"m (fuzzifier) must be &gt; 1\")\n    self.n_clusters = int(n_clusters)\n    self.m = float(m)\n    self.max_iter = int(max_iter)\n    self.tol = float(tol)\n    self.random_state = random_state\n    self.cluster_centers_ = None\n    self.membership_ = None\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.classification_entropy","title":"classification_entropy","text":"<pre><code>classification_entropy() -&gt; float\n</code></pre> <p>Classification Entropy (CE). Lower is better (crisper).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def classification_entropy(self) -&gt; float:\n    \"\"\"Classification Entropy (CE). Lower is better (crisper).\"\"\"\n    if self.membership_ is None:\n        raise RuntimeError(\"Fit the model before calling classification_entropy().\")\n    return _ce(self.membership_)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.fit","title":"fit","text":"<pre><code>fit(X: ndarray) -&gt; FuzzyCMeans\n</code></pre> <p>Fit the FCM model.</p> <p>Sets cluster_centers_ (k,d) and membership_ (n,k).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; FuzzyCMeans:\n    \"\"\"Fit the FCM model.\n\n    Sets cluster_centers_ (k,d) and membership_ (n,k).\n    \"\"\"\n    X = self._check_X(X)\n    n, _ = X.shape\n    if n &lt; self.n_clusters:\n        raise ValueError(\"n_samples must be &gt;= n_clusters\")\n    U = self._init_membership(n)\n    m = self.m\n\n    def update_centers(Um: np.ndarray) -&gt; np.ndarray:\n        num = Um.T @ X  # (k,d)\n        den = np.maximum(Um.sum(axis=0)[:, None], 1e-12)\n        return num / den\n\n    Um = U**m\n    C = update_centers(Um)\n    for _ in range(self.max_iter):\n        d2 = np.maximum(self._pairwise_sq_dists(X, C), 1e-12)  # (n,k)\n        inv = d2 ** (-1.0 / (m - 1.0))\n        U_new = inv / np.sum(inv, axis=1, keepdims=True)\n        Um_new = U_new**m\n        C_new = update_centers(Um_new)\n        if np.max(np.linalg.norm(C_new - C, axis=1)) &lt; self.tol:\n            U, C = U_new, C_new\n            break\n        U, C = U_new, C_new\n    self.membership_ = U\n    self.cluster_centers_ = C\n    return self\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Fit and return hard labels via argmax of membership.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def fit_predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Fit and return hard labels via argmax of membership.\"\"\"\n    self.fit(X)\n    return self.predict(X)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.partition_coefficient","title":"partition_coefficient","text":"<pre><code>partition_coefficient() -&gt; float\n</code></pre> <p>Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def partition_coefficient(self) -&gt; float:\n    \"\"\"Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.\"\"\"\n    if self.membership_ is None:\n        raise RuntimeError(\"Fit the model before calling partition_coefficient().\")\n    return _pc(self.membership_)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return hard labels via argmax of predict_proba.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return hard labels via argmax of predict_proba.\"\"\"\n    U = self.predict_proba(X)\n    return np.argmax(U, axis=1)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return membership degrees for samples to clusters (rows sum to 1).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return membership degrees for samples to clusters (rows sum to 1).\"\"\"\n    if self.cluster_centers_ is None:\n        raise RuntimeError(\"Call fit() before predict_proba().\")\n    X = self._check_X(X)\n    C = self.cluster_centers_\n    m = self.m\n    d2 = np.maximum(self._pairwise_sq_dists(X, C), 1e-12)\n    inv = d2 ** (-1.0 / (m - 1.0))\n    return inv / np.sum(inv, axis=1, keepdims=True)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Alias for predict_proba.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Alias for predict_proba.\"\"\"\n    return self.predict_proba(X)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.xie_beni_index","title":"xie_beni_index","text":"<pre><code>xie_beni_index(X: ndarray) -&gt; float\n</code></pre> <p>Xie-Beni index (XB). Lower is better.</p> <p>XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def xie_beni_index(self, X: np.ndarray) -&gt; float:\n    \"\"\"Xie-Beni index (XB). Lower is better.\n\n    XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)\n    \"\"\"\n    if self.membership_ is None or self.cluster_centers_ is None:\n        raise RuntimeError(\"Fit the model before calling xie_beni_index().\")\n    X = self._check_X(X)\n    return _xb(X, self.membership_, self.cluster_centers_, m=self.m)\n</code></pre>"},{"location":"api/config/","title":"Configuration Utilities","text":"<p>API reference for configuration helpers that let you persist ANFIS setups, manage presets, and export trained models.</p>"},{"location":"api/config/#anfis_toolbox.config","title":"anfis_toolbox.config","text":"<p>Configuration utilities for ANFIS models.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig","title":"ANFISConfig","text":"<pre><code>ANFISConfig()\n</code></pre> <p>Configuration manager for ANFIS models.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation of configuration.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.add_input_config","title":"add_input_config","text":"<pre><code>add_input_config(\n    name: str,\n    range_min: float,\n    range_max: float,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n) -&gt; ANFISConfig\n</code></pre> <p>Add input configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Input variable name</p> required <code>range_min</code> <code>float</code> <p>Minimum input range</p> required <code>range_max</code> <code>float</code> <p>Maximum input range</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Type of membership functions</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.build_model","title":"build_model","text":"<pre><code>build_model() -&gt; ANFIS\n</code></pre> <p>Build ANFIS model from configuration.</p> <p>Returns:</p> Type Description <code>ANFIS</code> <p>Configured ANFIS model</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: str | Path) -&gt; ANFISConfig\n</code></pre> <p>Load configuration from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to configuration file</p> required <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>ANFISConfig object</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.save","title":"save","text":"<pre><code>save(filepath: str | Path)\n</code></pre> <p>Save configuration to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to save configuration file</p> required"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.set_training_config","title":"set_training_config","text":"<pre><code>set_training_config(\n    method: str = \"hybrid\",\n    epochs: int = 50,\n    learning_rate: float = 0.01,\n    verbose: bool = False,\n) -&gt; ANFISConfig\n</code></pre> <p>Set training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Training method ('hybrid' or 'backprop')</p> <code>'hybrid'</code> <code>epochs</code> <code>int</code> <p>Number of training epochs</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Learning rate</p> <code>0.01</code> <code>verbose</code> <code>bool</code> <p>Whether to show training progress</p> <code>False</code> <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert configuration to dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Configuration dictionary</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISModelManager","title":"ANFISModelManager","text":"<p>Model management utilities for saving/loading trained ANFIS models.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISModelManager.load_model","title":"load_model  <code>staticmethod</code>","text":"<pre><code>load_model(filepath: str | Path) -&gt; ANFIS\n</code></pre> <p>Load trained ANFIS model from file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to model file</p> required <p>Returns:</p> Type Description <code>ANFIS</code> <p>Loaded ANFIS model</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISModelManager.save_model","title":"save_model  <code>staticmethod</code>","text":"<pre><code>save_model(\n    model: ANFIS,\n    filepath: str | Path,\n    include_config: bool = True,\n)\n</code></pre> <p>Save trained ANFIS model to file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ANFIS</code> <p>Trained ANFIS model</p> required <code>filepath</code> <code>str | Path</code> <p>Path to save model file</p> required <code>include_config</code> <code>bool</code> <p>Whether to save model configuration</p> <code>True</code>"},{"location":"api/config/#anfis_toolbox.config.create_config_from_preset","title":"create_config_from_preset","text":"<pre><code>create_config_from_preset(preset_name: str) -&gt; ANFISConfig\n</code></pre> <p>Create configuration from predefined preset.</p> <p>Parameters:</p> Name Type Description Default <code>preset_name</code> <code>str</code> <p>Name of predefined configuration</p> required <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>ANFISConfig object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If preset name not found</p>"},{"location":"api/config/#anfis_toolbox.config.list_presets","title":"list_presets","text":"<pre><code>list_presets() -&gt; dict[str, str]\n</code></pre> <p>List available predefined configurations.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping preset names to descriptions</p>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer","title":"anfis_toolbox.layers.MembershipLayer","text":"<pre><code>MembershipLayer(input_mfs: dict)\n</code></pre> <p>Membership layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This is the first layer of ANFIS that applies membership functions to input variables. Each input variable has multiple membership functions that transform crisp input values into fuzzy membership degrees.</p> <p>This layer serves as the fuzzification stage, converting crisp inputs into fuzzy sets that can be processed by subsequent ANFIS layers.</p> <p>Attributes:</p> Name Type Description <code>input_mfs</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.</p> <code>input_names</code> <code>list</code> <p>List of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_mfs</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.              Format: {input_name: [MembershipFunction, ...]}</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, input_mfs: dict):\n    \"\"\"Initializes the membership layer with input membership functions.\n\n    Parameters:\n        input_mfs (dict): Dictionary mapping input names to lists of membership functions.\n                         Format: {input_name: [MembershipFunction, ...]}\n    \"\"\"\n    self.input_mfs = input_mfs\n    self.input_names = list(input_mfs.keys())\n    self.n_inputs = len(input_mfs)\n    self.mf_per_input = [len(mfs) for mfs in input_mfs.values()]\n    self.last: dict[str, Any] = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.membership_functions","title":"membership_functions  <code>property</code>","text":"<pre><code>membership_functions: dict\n</code></pre> <p>Alias for input_mfs to provide a standardized interface.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.</p>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.backward","title":"backward","text":"<pre><code>backward(gradients: dict) -&gt; dict\n</code></pre> <p>Performs backward pass to compute gradients for membership functions.</p> <p>Parameters:</p> Name Type Description Default <code>gradients</code> <code>dict</code> <p>Dictionary mapping input names to gradient arrays.              Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Nested structure with parameter gradients mirroring <code>model.get_gradients()</code>.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, gradients: dict) -&gt; dict:\n    \"\"\"Performs backward pass to compute gradients for membership functions.\n\n    Parameters:\n        gradients (dict): Dictionary mapping input names to gradient arrays.\n                         Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n\n    Returns:\n        dict: Nested structure with parameter gradients mirroring ``model.get_gradients()``.\n    \"\"\"\n    param_grads: dict[str, list[dict[str, float]]] = {}\n\n    for name in self.input_names:\n        mfs = self.input_mfs[name]\n        grad_array = gradients[name]\n        mf_param_grads: list[dict[str, float]] = []\n\n        for mf_idx, mf in enumerate(mfs):\n            prev = {key: float(value) for key, value in mf.gradients.items()}\n            mf_gradient = grad_array[:, mf_idx]\n            mf.backward(mf_gradient)\n            updated = mf.gradients\n            delta = {key: float(updated[key] - prev.get(key, 0.0)) for key in updated}\n            mf_param_grads.append(delta)\n\n        param_grads[name] = mf_param_grads\n\n    return {\"membership\": param_grads}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; dict\n</code></pre> <p>Performs forward pass to compute membership degrees for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data with shape (batch_size, n_inputs).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to membership degree arrays.  Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; dict:\n    \"\"\"Performs forward pass to compute membership degrees for all inputs.\n\n    Parameters:\n        x (np.ndarray): Input data with shape (batch_size, n_inputs).\n\n    Returns:\n        dict: Dictionary mapping input names to membership degree arrays.\n             Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n    \"\"\"\n    _batch_size = x.shape[0]\n    membership_outputs = {}\n\n    # Compute membership degrees for each input variable\n    for i, name in enumerate(self.input_names):\n        mfs = self.input_mfs[name]\n        # Apply each membership function to the i-th input\n        mu_values = []\n        for mf in mfs:\n            mu = mf(x[:, i])  # (batch_size,)\n            mu_values.append(mu)\n\n        # Stack membership values for all MFs of this input\n        membership_outputs[name] = np.stack(mu_values, axis=-1)  # (batch_size, n_mfs)\n\n    # Cache values for backward pass\n    self.last = {\"x\": x, \"membership_outputs\": membership_outputs}\n\n    return membership_outputs\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets all membership functions to their initial state.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets all membership functions to their initial state.\n\n    Returns:\n        None\n    \"\"\"\n    for name in self.input_names:\n        for mf in self.input_mfs[name]:\n            mf.reset()\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer","title":"anfis_toolbox.layers.RuleLayer","text":"<pre><code>RuleLayer(\n    input_names: list,\n    mf_per_input: list,\n    rules: Sequence[Sequence[int]] | None = None,\n)\n</code></pre> <p>Rule layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer computes the rule strengths (firing strengths) by applying the T-norm (typically product) operation to the membership degrees of all input variables for each rule.</p> <p>This is the second layer of ANFIS that takes membership degrees from the MembershipLayer and computes rule activations.</p> <p>Attributes:</p> Name Type Description <code>input_names</code> <code>list</code> <p>List of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input.</p> <code>rules</code> <code>list</code> <p>List of all possible rule combinations.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_names</code> <code>list</code> <p>List of input variable names.</p> required <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input variable.</p> required <code>rules</code> <code>Sequence[Sequence[int]] | None</code> <p>Optional explicit rule set where each rule is a sequence of membership-function indices, one per input. When <code>None</code>, the full Cartesian product of membership functions is used.</p> <code>None</code> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(\n    self,\n    input_names: list,\n    mf_per_input: list,\n    rules: Sequence[Sequence[int]] | None = None,\n):\n    \"\"\"Initializes the rule layer with input configuration.\n\n    Parameters:\n        input_names (list): List of input variable names.\n        mf_per_input (list): Number of membership functions per input variable.\n        rules (Sequence[Sequence[int]] | None): Optional explicit rule set where each\n            rule is a sequence of membership-function indices, one per input. When\n            ``None``, the full Cartesian product of membership functions is used.\n    \"\"\"\n    self.input_names = input_names\n    self.n_inputs = len(input_names)\n    self.mf_per_input = list(mf_per_input)\n\n    if rules is None:\n        # Generate all possible rule combinations (Cartesian product)\n        self.rules = [tuple(rule) for rule in product(*[range(n) for n in self.mf_per_input])]\n    else:\n        validated_rules: list[tuple[int, ...]] = []\n        for idx, rule in enumerate(rules):\n            if len(rule) != self.n_inputs:\n                raise ValueError(\n                    \"Each rule must specify exactly one membership index per input. \"\n                    f\"Rule at position {idx} has length {len(rule)} while {self.n_inputs} were expected.\"\n                )\n            normalized_rule: list[int] = []\n            for input_idx, mf_idx in enumerate(rule):\n                max_mf = self.mf_per_input[input_idx]\n                if not 0 &lt;= mf_idx &lt; max_mf:\n                    raise ValueError(\n                        \"Rule membership index out of range. \"\n                        f\"Received {mf_idx} for input {input_idx} with {max_mf} membership functions.\"\n                    )\n                normalized_rule.append(int(mf_idx))\n            validated_rules.append(tuple(normalized_rule))\n\n        if not validated_rules:\n            raise ValueError(\"At least one rule must be provided when specifying custom rules.\")\n        self.rules = validated_rules\n\n    self.n_rules = len(self.rules)\n\n    self.last: dict[str, Any] = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer.backward","title":"backward","text":"<pre><code>backward(dL_dw: ndarray) -&gt; dict\n</code></pre> <p>Performs backward pass to compute gradients for membership functions.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dw</code> <code>ndarray</code> <p>Gradient of loss with respect to rule strengths.                Shape: (batch_size, n_rules)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to gradient arrays for membership functions.  Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dw: np.ndarray) -&gt; dict:\n    \"\"\"Performs backward pass to compute gradients for membership functions.\n\n    Parameters:\n        dL_dw (np.ndarray): Gradient of loss with respect to rule strengths.\n                           Shape: (batch_size, n_rules)\n\n    Returns:\n        dict: Dictionary mapping input names to gradient arrays for membership functions.\n             Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n    \"\"\"\n    batch_size = dL_dw.shape[0]\n    mu = self.last[\"mu\"]  # (batch_size, n_inputs, n_mfs)\n\n    # Initialize gradient accumulators for each input's membership functions\n    gradients = {}\n    for i, name in enumerate(self.input_names):\n        n_mfs = self.mf_per_input[i]\n        gradients[name] = np.zeros((batch_size, n_mfs))\n\n    # Compute gradients for each rule\n    for rule_idx, rule in enumerate(self.rules):\n        for input_idx, mf_idx in enumerate(rule):\n            name = self.input_names[input_idx]\n\n            # Compute partial derivative: d(rule_strength)/d(mu_ij)\n            # This is the product of all other membership degrees in the rule\n            other_factors = []\n            for j, j_mf in enumerate(rule):\n                if j == input_idx:\n                    continue  # Skip the current input\n                other_factors.append(mu[:, j, j_mf])\n\n            # Product of other factors (or 1 if no other factors)\n            partial = np.prod(other_factors, axis=0) if other_factors else np.ones(batch_size)\n\n            # Apply chain rule: dL/dmu = dL/dw * dw/dmu\n            gradients[name][:, mf_idx] += dL_dw[:, rule_idx] * partial\n\n    return gradients\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer.forward","title":"forward","text":"<pre><code>forward(membership_outputs: dict) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to compute rule strengths.</p> <p>Parameters:</p> Name Type Description Default <code>membership_outputs</code> <code>dict</code> <p>Dictionary mapping input names to membership degree arrays.                      Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Rule strengths with shape (batch_size, n_rules).</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, membership_outputs: dict) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to compute rule strengths.\n\n    Parameters:\n        membership_outputs (dict): Dictionary mapping input names to membership degree arrays.\n                                 Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n\n    Returns:\n        np.ndarray: Rule strengths with shape (batch_size, n_rules).\n    \"\"\"\n    # Convert membership outputs to array format for easier processing\n    mu_list = []\n    for name in self.input_names:\n        mu_list.append(membership_outputs[name])  # (batch_size, n_mfs)\n    mu = np.stack(mu_list, axis=1)  # (batch_size, n_inputs, n_mfs)\n\n    _batch_size = mu.shape[0]\n\n    # Compute rule activations (firing strengths)\n    rule_activations = []\n    for rule in self.rules:\n        rule_mu = []\n        # Get membership degree for each input in this rule\n        for input_idx, mf_idx in enumerate(rule):\n            rule_mu.append(mu[:, input_idx, mf_idx])  # (batch_size,)\n        # Apply T-norm (product) to get rule strength\n        rule_strength = np.prod(rule_mu, axis=0)  # (batch_size,)\n        rule_activations.append(rule_strength)\n\n    rule_activations = np.stack(rule_activations, axis=1)  # (batch_size, n_rules)\n\n    # Cache values for backward pass\n    self.last = {\"membership_outputs\": membership_outputs, \"mu\": mu, \"rule_activations\": rule_activations}\n\n    return rule_activations\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer","title":"anfis_toolbox.layers.NormalizationLayer","text":"<pre><code>NormalizationLayer()\n</code></pre> <p>Normalization layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer normalizes the rule strengths (firing strengths) to ensure they sum to 1.0 for each sample in the batch. This is a crucial step in ANFIS as it converts rule strengths to normalized rule weights.</p> <p>The normalization formula is: norm_w_i = w_i / sum(w_j for all j)</p> <p>Attributes:</p> Name Type Description <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the normalization layer.\"\"\"\n    self.last: dict[str, Any] = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer.backward","title":"backward","text":"<pre><code>backward(dL_dnorm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs backward pass to compute gradients for original rule weights.</p> <p>The gradient computation uses the quotient rule for derivatives: If norm_w_i = w_i / sum_w, then: - d(norm_w_i)/d(w_i) = (sum_w - w_i) / sum_w\u00b2 - d(norm_w_i)/d(w_j) = -w_j / sum_w\u00b2 for j \u2260 i</p> <p>Parameters:</p> Name Type Description Default <code>dL_dnorm_w</code> <code>ndarray</code> <p>Gradient of loss with respect to normalized weights.                     Shape: (batch_size, n_rules)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Gradient of loss with respect to original weights.        Shape: (batch_size, n_rules)</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dnorm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs backward pass to compute gradients for original rule weights.\n\n    The gradient computation uses the quotient rule for derivatives:\n    If norm_w_i = w_i / sum_w, then:\n    - d(norm_w_i)/d(w_i) = (sum_w - w_i) / sum_w\u00b2\n    - d(norm_w_i)/d(w_j) = -w_j / sum_w\u00b2 for j \u2260 i\n\n    Parameters:\n        dL_dnorm_w (np.ndarray): Gradient of loss with respect to normalized weights.\n                                Shape: (batch_size, n_rules)\n\n    Returns:\n        np.ndarray: Gradient of loss with respect to original weights.\n                   Shape: (batch_size, n_rules)\n    \"\"\"\n    w = self.last[\"w\"]  # (batch_size, n_rules)\n    sum_w = self.last[\"sum_w\"]  # (batch_size, 1)\n\n    # Jacobian-vector product without building the full Jacobian:\n    # (J^T g)_j = (sum_w * g_j - (g \u00b7 w)) / sum_w^2\n    g = dL_dnorm_w  # (batch_size, n_rules)\n    s = sum_w  # (batch_size, 1)\n    gw_dot = np.sum(g * w, axis=1, keepdims=True)  # (batch_size, 1)\n    dL_dw = (s * g - gw_dot) / (s**2)  # (batch_size, n_rules)\n\n    return dL_dw\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer.forward","title":"forward","text":"<pre><code>forward(w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to normalize rule weights.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>Rule strengths with shape (batch_size, n_rules).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized rule weights with shape (batch_size, n_rules).        Each row sums to 1.0.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to normalize rule weights.\n\n    Parameters:\n        w (np.ndarray): Rule strengths with shape (batch_size, n_rules).\n\n    Returns:\n        np.ndarray: Normalized rule weights with shape (batch_size, n_rules).\n                   Each row sums to 1.0.\n    \"\"\"\n    # Add small epsilon to avoid division by zero\n    sum_w = np.sum(w, axis=1, keepdims=True) + 1e-8\n    norm_w = w / sum_w\n\n    # Cache values for backward pass\n    self.last = {\"w\": w, \"sum_w\": sum_w, \"norm_w\": norm_w}\n    return norm_w\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer","title":"anfis_toolbox.layers.ConsequentLayer","text":"<pre><code>ConsequentLayer(n_rules: int, n_inputs: int)\n</code></pre> <p>Consequent layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer implements the consequent part of fuzzy rules in ANFIS. Each rule has a linear consequent function of the form: f_i(x) = p_i * x_1 + q_i * x_2 + ... + r_i (TSK model)</p> <p>The final output is computed as a weighted sum: y = \u03a3(w_i * f_i(x)) where w_i are normalized rule weights</p> <p>Attributes:</p> Name Type Description <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>parameters</code> <code>ndarray</code> <p>Linear parameters for each rule with shape (n_rules, n_inputs + 1).                     Each row contains [p_i, q_i, ..., r_i] for rule i.</p> <code>gradients</code> <code>ndarray</code> <p>Accumulated gradients for parameters.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules.</p> required <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, n_rules: int, n_inputs: int):\n    \"\"\"Initializes the consequent layer with random linear parameters.\n\n    Parameters:\n        n_rules (int): Number of fuzzy rules.\n        n_inputs (int): Number of input variables.\n    \"\"\"\n    # Each rule has (n_inputs + 1) parameters: p_i, q_i, ..., r_i (including bias)\n    self.n_rules = n_rules\n    self.n_inputs = n_inputs\n    self.parameters = np.random.randn(n_rules, n_inputs + 1)\n    self.gradients = np.zeros_like(self.parameters)\n    self.last: dict[str, Any] = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Performs backward pass to compute gradients for parameters and inputs.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of loss with respect to layer output.                Shape: (batch_size, 1)</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(dL_dnorm_w, dL_dx) where: - dL_dnorm_w: Gradient w.r.t. normalized weights, shape (batch_size, n_rules) - dL_dx: Gradient w.r.t. input x, shape (batch_size, n_inputs)</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Performs backward pass to compute gradients for parameters and inputs.\n\n    Parameters:\n        dL_dy (np.ndarray): Gradient of loss with respect to layer output.\n                           Shape: (batch_size, 1)\n\n    Returns:\n        tuple: (dL_dnorm_w, dL_dx) where:\n            - dL_dnorm_w: Gradient w.r.t. normalized weights, shape (batch_size, n_rules)\n            - dL_dx: Gradient w.r.t. input x, shape (batch_size, n_inputs)\n    \"\"\"\n    X_aug = self.last[\"X_aug\"]  # (batch_size, n_inputs + 1)\n    norm_w = self.last[\"norm_w\"]  # (batch_size, n_rules)\n    f = self.last[\"f\"]  # (batch_size, n_rules)\n\n    batch_size = X_aug.shape[0]\n\n    # Compute gradients for consequent parameters\n    self.gradients = np.zeros_like(self.parameters)\n\n    for i in range(self.n_rules):\n        # Gradient of y_hat w.r.t. parameters of rule i: norm_w_i * x_aug\n        for b in range(batch_size):\n            self.gradients[i] += dL_dy[b, 0] * norm_w[b, i] * X_aug[b]\n\n    # Compute gradient of loss w.r.t. normalized weights\n    # dy/dnorm_w_i = f_i(x), so dL/dnorm_w_i = dL/dy * f_i(x)\n    dL_dnorm_w = dL_dy * f  # (batch_size, n_rules)\n\n    # Compute gradient of loss w.r.t. input x (for backpropagation to previous layers)\n    dL_dx = np.zeros((batch_size, self.n_inputs))\n\n    for b in range(batch_size):\n        for i in range(self.n_rules):\n            # dy/dx = norm_w_i * parameters_i[:-1] (excluding bias term)\n            dL_dx[b] += dL_dy[b, 0] * norm_w[b, i] * self.parameters[i, :-1]\n\n    return dL_dnorm_w, dL_dx\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray, norm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to compute the final ANFIS output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data with shape (batch_size, n_inputs).</p> required <code>norm_w</code> <code>ndarray</code> <p>Normalized rule weights with shape (batch_size, n_rules).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Final ANFIS output with shape (batch_size, 1).</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray, norm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to compute the final ANFIS output.\n\n    Parameters:\n        x (np.ndarray): Input data with shape (batch_size, n_inputs).\n        norm_w (np.ndarray): Normalized rule weights with shape (batch_size, n_rules).\n\n    Returns:\n        np.ndarray: Final ANFIS output with shape (batch_size, 1).\n    \"\"\"\n    batch_size = x.shape[0]\n\n    # Augment input with bias term (column of ones)\n    X_aug = np.hstack([x, np.ones((batch_size, 1))])  # (batch_size, n_inputs + 1)\n\n    # Compute consequent function f_i(x) for each rule\n    # f[b, i] = p_i * x[b, 0] + q_i * x[b, 1] + ... + r_i\n    f = X_aug @ self.parameters.T  # (batch_size, n_rules)\n\n    # Compute final output as weighted sum: y = \u03a3(w_i * f_i(x))\n    y_hat = np.sum(norm_w * f, axis=1, keepdims=True)  # (batch_size, 1)\n\n    # Cache values for backward pass\n    self.last = {\"X_aug\": X_aug, \"norm_w\": norm_w, \"f\": f}\n\n    return y_hat\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets gradients and cached values.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets gradients and cached values.\n\n    Returns:\n        None\n    \"\"\"\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer","title":"anfis_toolbox.layers.ClassificationConsequentLayer","text":"<pre><code>ClassificationConsequentLayer(\n    n_rules: int,\n    n_inputs: int,\n    n_classes: int,\n    random_state: int | None = None,\n)\n</code></pre> <p>Consequent layer that produces per-class logits for classification.</p> <p>Each rule i has a vector of class logits with a linear function of inputs: f_i(x) = W_i x + b_i, where W_i has shape (n_classes, n_inputs) and b_i (n_classes,). We store parameters as a single array of shape (n_rules, n_classes, n_inputs + 1).</p> <p>Parameters:</p> Name Type Description Default <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules in the layer.</p> required <code>n_inputs</code> <code>int</code> <p>Number of input features.</p> required <code>n_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>random_state</code> <code>int | None</code> <p>Random seed for parameter initialization.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_rules</code> <code>int</code> <p>Stores the number of fuzzy rules.</p> <code>n_inputs</code> <code>int</code> <p>Stores the number of input features.</p> <code>n_classes</code> <code>int</code> <p>Stores the number of output classes.</p> <code>parameters</code> <code>ndarray</code> <p>Randomly initialized parameters for each rule, class, and input (including bias).</p> <code>gradients</code> <code>ndarray</code> <p>Gradient values initialized to zeros, matching the shape of parameters.</p> <code>last</code> <code>dict</code> <p>Dictionary for storing intermediate results or state.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, n_rules: int, n_inputs: int, n_classes: int, random_state: int | None = None):\n    \"\"\"Initializes the layer with the specified number of rules, inputs, and classes.\n\n    Args:\n        n_rules (int): Number of fuzzy rules in the layer.\n        n_inputs (int): Number of input features.\n        n_classes (int): Number of output classes.\n        random_state (int | None): Random seed for parameter initialization.\n\n    Attributes:\n        n_rules (int): Stores the number of fuzzy rules.\n        n_inputs (int): Stores the number of input features.\n        n_classes (int): Stores the number of output classes.\n\n\n        parameters (np.ndarray): Randomly initialized parameters for each rule, class, and input (including bias).\n        gradients (np.ndarray): Gradient values initialized to zeros, matching the shape of parameters.\n        last (dict): Dictionary for storing intermediate results or state.\n    \"\"\"\n    self.n_rules = n_rules\n    self.n_inputs = n_inputs\n    self.n_classes = n_classes\n    if random_state is None:\n        self.parameters = np.random.randn(n_rules, n_classes, n_inputs + 1)\n    else:\n        rng = np.random.default_rng(random_state)\n        self.parameters = rng.normal(size=(n_rules, n_classes, n_inputs + 1))\n    self.gradients = np.zeros_like(self.parameters)\n    self.last: dict[str, Any] = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.backward","title":"backward","text":"<pre><code>backward(dL_dlogits: ndarray)\n</code></pre> <p>Computes the backward pass for the classification consequent layer.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dlogits: np.ndarray):\n    \"\"\"Computes the backward pass for the classification consequent layer.\"\"\"\n    X_aug = self.last[\"X_aug\"]  # (b, d+1)\n    norm_w = self.last[\"norm_w\"]  # (b, r)\n    f = self.last[\"f\"]  # (b, r, k)\n\n    # Gradients w.r.t. per-rule parameters\n    self.gradients = np.zeros_like(self.parameters)\n    # dL/df_{brk} = dL/dlogits_{bk} * norm_w_{br}\n    dL_df = dL_dlogits[:, None, :] * norm_w[:, :, None]  # (b, r, k)\n    # Accumulate over batch: grad[r,k,d] = sum_b dL_df[b,r,k] * X_aug[b,d]\n    self.gradients = np.einsum(\"brk,bd-&gt;rkd\", dL_df, X_aug)\n\n    # dL/dnorm_w: sum_k dL/dlogits_{bk} * f_{brk}\n    dL_dnorm_w = np.einsum(\"bk,brk-&gt;br\", dL_dlogits, f)\n\n    # dL/dx: sum_r sum_k dL/dlogits_{bk} * norm_w_{br} * W_{r,k,:}\n    W = self.parameters[:, :, :-1]  # (r,k,d)\n    dL_dx = np.einsum(\"bk,br,rkd-&gt;bd\", dL_dlogits, norm_w, W)\n    return dL_dnorm_w, dL_dx\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray, norm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes the forward pass for the classification consequent layer.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray, norm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the forward pass for the classification consequent layer.\"\"\"\n    batch = x.shape[0]\n    X_aug = np.hstack([x, np.ones((batch, 1))])  # (b, d+1)\n    # Compute per-rule class logits: (b, r, k)\n    f = np.einsum(\"bd,rkd-&gt;brk\", X_aug, self.parameters)\n    # Weighted sum over rules -&gt; logits (b, k)\n    logits = np.einsum(\"br,brk-&gt;bk\", norm_w, f)\n    self.last = {\"X_aug\": X_aug, \"norm_w\": norm_w, \"f\": f}\n    return logits\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the gradients and cached values.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets the gradients and cached values.\"\"\"\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/logging/","title":"Logging Utilities","text":"<p>Helper functions that configure logging for training workflows.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config","title":"anfis_toolbox.logging_config","text":"<p>Logging configuration for ANFIS toolbox.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config.disable_training_logs","title":"disable_training_logs","text":"<pre><code>disable_training_logs() -&gt; None\n</code></pre> <p>Disable training progress logs.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config.enable_training_logs","title":"enable_training_logs","text":"<pre><code>enable_training_logs() -&gt; None\n</code></pre> <p>Enable training progress logs with a simple format.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n    level: str = \"INFO\", format_string: str | None = None\n) -&gt; None\n</code></pre> <p>Setup logging configuration for ANFIS toolbox.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL').</p> <code>'INFO'</code> <code>format_string</code> <code>str</code> <p>Custom format string for log messages.</p> <code>None</code>"},{"location":"api/losses/","title":"Losses API","text":"<p>This module provides loss functions and their gradients used during ANFIS model training.</p>"},{"location":"api/losses/#anfis_toolbox.losses","title":"anfis_toolbox.losses","text":"<p>Loss functions and their gradients for ANFIS Toolbox.</p> <p>This module centralizes the loss definitions used during training to make it explicit which objective is being optimized. Trainers can import from here so the chosen loss is clear in one place.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss","title":"CrossEntropyLoss","text":"<p>               Bases: <code>LossFunction</code></p> <p>Categorical cross-entropy loss operating on logits.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss.gradient","title":"gradient","text":"<pre><code>gradient(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Delegate to :func:<code>cross_entropy_grad</code> for gradient computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss.loss","title":"loss","text":"<pre><code>loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Delegate to :func:<code>cross_entropy_loss</code> for value computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss.prepare_targets","title":"prepare_targets","text":"<pre><code>prepare_targets(\n    y: Any, *, model: Any | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Convert labels or one-hot encodings into dense float matrices.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction","title":"LossFunction","text":"<p>Base interface for losses used by trainers.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction.gradient","title":"gradient","text":"<pre><code>gradient(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return the gradient of the loss with respect to the predictions.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction.loss","title":"loss","text":"<pre><code>loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Compute the scalar loss for the given targets and predictions.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction.prepare_targets","title":"prepare_targets","text":"<pre><code>prepare_targets(\n    y: Any, *, model: Any | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Return targets in a format compatible with forward/gradient computations.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss","title":"MSELoss","text":"<p>               Bases: <code>LossFunction</code></p> <p>Mean squared error loss packaged for trainer consumption.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss.gradient","title":"gradient","text":"<pre><code>gradient(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Delegate to :func:<code>mse_grad</code> for gradient computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss.loss","title":"loss","text":"<pre><code>loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Delegate to :func:<code>mse_loss</code> for value computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss.prepare_targets","title":"prepare_targets","text":"<pre><code>prepare_targets(\n    y: Any, *, model: Any | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Convert 1D targets into column vectors expected by MSE computations.</p>"},{"location":"api/losses/#anfis_toolbox.losses.cross_entropy_grad","title":"cross_entropy_grad","text":"<pre><code>cross_entropy_grad(\n    y_true: ndarray, logits: ndarray\n) -&gt; np.ndarray\n</code></pre> <p>Gradient of cross-entropy w.r.t logits.</p> <p>Accepts integer labels (n,) or one-hot (n,k). Returns gradient with the same shape as logits: (n,k).</p>"},{"location":"api/losses/#anfis_toolbox.losses.cross_entropy_loss","title":"cross_entropy_loss","text":"<pre><code>cross_entropy_loss(\n    y_true: ndarray, logits: ndarray\n) -&gt; float\n</code></pre> <p>Cross-entropy loss from labels (int or one-hot) and logits.</p> <p>This delegates to metrics.cross_entropy for the scalar value.</p>"},{"location":"api/losses/#anfis_toolbox.losses.mse_grad","title":"mse_grad","text":"<pre><code>mse_grad(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Gradient of MSE w.r.t. predictions.</p> <p>d/dy_pred MSE = 2 * (y_pred - y_true) / n</p>"},{"location":"api/losses/#anfis_toolbox.losses.mse_loss","title":"mse_loss","text":"<pre><code>mse_loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Mean squared error (MSE) loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>Array-like true targets of shape (n, d) or (n,).</p> required <code>y_pred</code> <code>ndarray</code> <p>Array-like predictions of same shape as y_true.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar MSE value.</p>"},{"location":"api/losses/#anfis_toolbox.losses.resolve_loss","title":"resolve_loss","text":"<pre><code>resolve_loss(\n    loss: str | LossFunction | None,\n) -&gt; LossFunction\n</code></pre> <p>Resolve user-provided loss spec into a concrete <code>LossFunction</code> instance.</p>"},{"location":"api/losses/#regression-losses","title":"Regression Losses","text":"<p>Functions for regression tasks (continuous output prediction).</p> <ul> <li><code>mse_loss()</code> - Mean squared error loss</li> <li><code>mse_grad()</code> - Gradient of MSE loss</li> </ul>"},{"location":"api/losses/#classification-losses","title":"Classification Losses","text":"<p>Functions for classification tasks (discrete output prediction).</p> <ul> <li><code>cross_entropy_loss()</code> - Cross-entropy loss</li> <li><code>cross_entropy_grad()</code> - Gradient of cross-entropy loss</li> </ul>"},{"location":"api/membership-functions/","title":"Membership Functions","text":""},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF","title":"anfis_toolbox.membership.GaussianMF","text":"<pre><code>GaussianMF(mean: float = 0.0, sigma: float = 1.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Gaussian Membership Function.</p> <p>Implements a Gaussian (bell-shaped) membership function using the formula: \u03bc(x) = exp(-((x - mean)\u00b2 / (2 * sigma\u00b2)))</p> <p>This function is commonly used in fuzzy logic systems due to its smooth and differentiable properties.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>Mean of the Gaussian (center). Defaults to 0.0.</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation (width). Defaults to 1.0.</p> <code>1.0</code> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, mean: float = 0.0, sigma: float = 1.0):\n    \"\"\"Initialize with mean and standard deviation.\n\n    Args:\n        mean: Mean of the Gaussian (center). Defaults to 0.0.\n        sigma: Standard deviation (width). Defaults to 1.0.\n    \"\"\"\n    super().__init__()\n    self.parameters = {\"mean\": mean, \"sigma\": sigma}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients w.r.t. parameters given upstream gradient.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss with respect to the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients w.r.t. parameters given upstream gradient.\n\n    Args:\n        dL_dy: Gradient of the loss with respect to the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    mean = self.parameters[\"mean\"]\n    sigma = self.parameters[\"sigma\"]\n\n    x = self.last_input\n    y = self.last_output\n\n    z = (x - mean) / sigma\n\n    # Derivatives of the Gaussian function\n    dy_dmean = -y * z / sigma\n    dy_dsigma = y * (z**2) / sigma\n\n    # Gradient with respect to mean\n    dL_dmean = np.sum(dL_dy * dy_dmean)\n\n    # Gradient with respect to sigma\n    dL_dsigma = np.sum(dL_dy * dy_dsigma)\n\n    # Update gradients\n    self.gradients[\"mean\"] += dL_dmean\n    self.gradients[\"sigma\"] += dL_dsigma\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute Gaussian membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of Gaussian membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Gaussian membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of Gaussian membership values.\n    \"\"\"\n    mean = self.parameters[\"mean\"]\n    sigma = self.parameters[\"sigma\"]\n    self.last_input = x\n    self.last_output = np.exp(-((x - mean) ** 2) / (2 * sigma**2))\n    return self.last_output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF","title":"anfis_toolbox.membership.Gaussian2MF","text":"<pre><code>Gaussian2MF(\n    sigma1: float = 1.0,\n    c1: float = 0.0,\n    sigma2: float = 1.0,\n    c2: float = 0.0,\n)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Gaussian combination Membership Function (two-sided Gaussian).</p> <p>This membership function uses Gaussian tails on both sides with an optional flat region in the middle.</p> <p>Parameters:</p> Name Type Description Default <code>sigma1</code> <code>float</code> <p>Standard deviation of the left Gaussian tail (must be &gt; 0).</p> <code>1.0</code> <code>c1</code> <code>float</code> <p>Center of the left Gaussian tail.</p> <code>0.0</code> <code>sigma2</code> <code>float</code> <p>Standard deviation of the right Gaussian tail (must be &gt; 0).</p> <code>1.0</code> <code>c2</code> <code>float</code> <p>Center of the right Gaussian tail. Must satisfy c1 &lt;= c2.</p> <code>0.0</code> <p>Definition (with c1 &lt;= c2):     - For x &lt; c1: \u03bc(x) = exp(-((x - c1)^2) / (2*sigma1^2))     - For c1 &lt;= x &lt;= c2: \u03bc(x) = 1     - For x &gt; c2: \u03bc(x) = exp(-((x - c2)^2) / (2*sigma2^2))</p> <p>Special case (c1 == c2): asymmetric Gaussian centered at c1 with sigma1 on the left side and sigma2 on the right side (no flat region).</p> <p>Parameters:</p> Name Type Description Default <code>sigma1</code> <code>float</code> <p>Standard deviation of the first Gaussian. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>c1</code> <code>float</code> <p>Center of the first Gaussian. Defaults to 0.0.</p> <code>0.0</code> <code>sigma2</code> <code>float</code> <p>Standard deviation of the second Gaussian. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>c2</code> <code>float</code> <p>Center of the second Gaussian. Must satisfy c1 &lt;= c2. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sigma1 or sigma2 are not positive.</p> <code>ValueError</code> <p>If c1 &gt; c2.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the parameters 'sigma1', 'c1', 'sigma2', 'c2'.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing the gradients for each parameter, initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, sigma1: float = 1.0, c1: float = 0.0, sigma2: float = 1.0, c2: float = 0.0):\n    \"\"\"Initialize the membership function with two Gaussian components.\n\n    Args:\n        sigma1 (float, optional): Standard deviation of the first Gaussian. Must be positive. Defaults to 1.0.\n        c1 (float, optional): Center of the first Gaussian. Defaults to 0.0.\n        sigma2 (float, optional): Standard deviation of the second Gaussian. Must be positive. Defaults to 1.0.\n        c2 (float, optional): Center of the second Gaussian. Must satisfy c1 &lt;= c2. Defaults to 0.0.\n\n    Raises:\n        ValueError: If sigma1 or sigma2 are not positive.\n        ValueError: If c1 &gt; c2.\n\n    Attributes:\n        parameters (dict): Dictionary containing the parameters 'sigma1', 'c1', 'sigma2', 'c2'.\n        gradients (dict): Dictionary containing the gradients for each parameter, initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if sigma1 &lt;= 0:\n        raise ValueError(f\"Parameter 'sigma1' must be positive, got sigma1={sigma1}\")\n    if sigma2 &lt;= 0:\n        raise ValueError(f\"Parameter 'sigma2' must be positive, got sigma2={sigma2}\")\n    if c1 &gt; c2:\n        raise ValueError(f\"Parameters must satisfy c1 &lt;= c2, got c1={c1}, c2={c2}\")\n\n    self.parameters = {\"sigma1\": float(sigma1), \"c1\": float(c1), \"sigma2\": float(sigma2), \"c2\": float(c2)}\n    self.gradients = {\"sigma1\": 0.0, \"c1\": 0.0, \"sigma2\": 0.0, \"c2\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate parameter gradients for the two-sided Gaussian.</p> <p>The flat middle region contributes no gradients.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Upstream gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate parameter gradients for the two-sided Gaussian.\n\n    The flat middle region contributes no gradients.\n\n    Args:\n        dL_dy: Upstream gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    s1 = self.parameters[\"sigma1\"]\n    c1 = self.parameters[\"c1\"]\n    s2 = self.parameters[\"sigma2\"]\n    c2 = self.parameters[\"c2\"]\n\n    # Regions\n    left_mask = x &lt; c1\n    mid_mask = (x &gt;= c1) &amp; (x &lt;= c2)\n    right_mask = x &gt; c2\n\n    # Left Gaussian tail contributions (treat like a GaussianMF on that region)\n    if np.any(left_mask):\n        xl = x[left_mask]\n        yl = np.exp(-((xl - c1) ** 2) / (2.0 * s1 * s1))\n        z1 = (xl - c1) / s1\n        # Match GaussianMF derivative conventions\n        dmu_dc1 = yl * z1 / s1\n        dmu_dsigma1 = yl * (z1**2) / s1\n\n        dL_dc1 = np.sum(dL_dy[left_mask] * dmu_dc1)\n        dL_dsigma1 = np.sum(dL_dy[left_mask] * dmu_dsigma1)\n\n        self.gradients[\"c1\"] += float(dL_dc1)\n        self.gradients[\"sigma1\"] += float(dL_dsigma1)\n\n    # Mid region (flat) contributes no gradients\n    _ = mid_mask  # placeholder to document intentional no-op\n\n    # Right Gaussian tail contributions\n    if np.any(right_mask):\n        xr = x[right_mask]\n        yr = np.exp(-((xr - c2) ** 2) / (2.0 * s2 * s2))\n        z2 = (xr - c2) / s2\n        dmu_dc2 = yr * z2 / s2\n        dmu_dsigma2 = yr * (z2**2) / s2\n\n        dL_dc2 = np.sum(dL_dy[right_mask] * dmu_dc2)\n        dL_dsigma2 = np.sum(dL_dy[right_mask] * dmu_dsigma2)\n\n        self.gradients[\"c2\"] += float(dL_dc2)\n        self.gradients[\"sigma2\"] += float(dL_dsigma2)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute two-sided Gaussian membership values.</p> <p>The input space is divided by c1 and c2 into: - x &lt; c1: left Gaussian tail with sigma1 centered at c1 - c1 &lt;= x &lt;= c2: flat region (1.0) - x &gt; c2: right Gaussian tail with sigma2 centered at c2</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership degrees for each input value.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute two-sided Gaussian membership values.\n\n    The input space is divided by c1 and c2 into:\n    - x &lt; c1: left Gaussian tail with sigma1 centered at c1\n    - c1 &lt;= x &lt;= c2: flat region (1.0)\n    - x &gt; c2: right Gaussian tail with sigma2 centered at c2\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Membership degrees for each input value.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n\n    s1 = self.parameters[\"sigma1\"]\n    c1 = self.parameters[\"c1\"]\n    s2 = self.parameters[\"sigma2\"]\n    c2 = self.parameters[\"c2\"]\n\n    y = np.zeros_like(x, dtype=float)\n\n    # Regions\n    left_mask = x &lt; c1\n    mid_mask = (x &gt;= c1) &amp; (x &lt;= c2)\n    right_mask = x &gt; c2\n\n    if np.any(left_mask):\n        xl = x[left_mask]\n        y[left_mask] = np.exp(-((xl - c1) ** 2) / (2.0 * s1 * s1))\n\n    if np.any(mid_mask):\n        y[mid_mask] = 1.0\n\n    if np.any(right_mask):\n        xr = x[right_mask]\n        y[right_mask] = np.exp(-((xr - c2) ** 2) / (2.0 * s2 * s2))\n\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF","title":"anfis_toolbox.membership.BellMF","text":"<pre><code>BellMF(a: float = 1.0, b: float = 2.0, c: float = 0.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Bell-shaped (Generalized Bell) Membership Function.</p> <p>Implements a bell-shaped membership function using the formula: \u03bc(x) = 1 / (1 + |((x - c) / a)|^(2b))</p> <p>This function is a generalization of the Gaussian function and provides more flexibility in controlling the shape through the 'b' parameter. It's particularly useful when you need asymmetric membership functions or want to fine-tune the slope characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Width parameter (positive). Controls the width of the curve.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Slope parameter (positive). Controls the steepness of the curve.</p> <code>2.0</code> <code>c</code> <code>float</code> <p>Center parameter. Controls the center position of the curve.</p> <code>0.0</code> Note <p>Parameters 'a' and 'b' must be positive for a valid bell function.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Width parameter (must be positive). Defaults to 1.0.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Slope parameter (must be positive). Defaults to 2.0.</p> <code>2.0</code> <code>c</code> <code>float</code> <p>Center parameter. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' or 'b' are not positive.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float = 1.0, b: float = 2.0, c: float = 0.0):\n    \"\"\"Initialize with width, slope, and center parameters.\n\n    Args:\n        a: Width parameter (must be positive). Defaults to 1.0.\n        b: Slope parameter (must be positive). Defaults to 2.0.\n        c: Center parameter. Defaults to 0.0.\n\n    Raises:\n        ValueError: If 'a' or 'b' are not positive.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if a &lt;= 0:\n        raise ValueError(f\"Parameter 'a' must be positive, got a={a}\")\n\n    if b &lt;= 0:\n        raise ValueError(f\"Parameter 'b' must be positive, got b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients given upstream gradient.</p> <p>Analytical gradients: - \u2202\u03bc/\u2202a: width - \u2202\u03bc/\u2202b: steepness - \u2202\u03bc/\u2202c: center</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients given upstream gradient.\n\n    Analytical gradients:\n    - \u2202\u03bc/\u2202a: width\n    - \u2202\u03bc/\u2202b: steepness\n    - \u2202\u03bc/\u2202c: center\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n\n    x = self.last_input\n    y = self.last_output  # This is \u03bc(x)\n\n    # Intermediate calculations\n    normalized = (x - c) / a\n    abs_normalized = np.abs(normalized)\n\n    # Avoid division by zero and numerical issues\n    # Only compute gradients where abs_normalized &gt; epsilon\n    epsilon = 1e-12\n    valid_mask = abs_normalized &gt; epsilon\n\n    if not np.any(valid_mask):\n        # If all values are at the peak (x \u2248 c), gradients are zero\n        return\n\n    # Initialize gradients\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n\n    # Only compute where we have valid values\n    x_valid = x[valid_mask]\n    y_valid = y[valid_mask]\n    dL_dy_valid = dL_dy[valid_mask]\n    normalized_valid = (x_valid - c) / a\n    abs_normalized_valid = np.abs(normalized_valid)\n\n    # Power term: |normalized|^(2b)\n    power_term_valid = np.power(abs_normalized_valid, 2 * b)\n\n    # For the bell function \u03bc = 1/(1 + z) where z = |normalized|^(2b)\n    # \u2202\u03bc/\u2202z = -1/(1 + z)\u00b2 = -\u03bc\u00b2\n    dmu_dz = -y_valid * y_valid\n\n    # Chain rule: \u2202L/\u2202param = \u2202L/\u2202\u03bc \u00d7 \u2202\u03bc/\u2202z \u00d7 \u2202z/\u2202param\n\n    # \u2202z/\u2202a = \u2202(|normalized|^(2b))/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 \u2202|normalized|/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 \u2202normalized/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (-(x-c)/a\u00b2)\n    # = -2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (x-c)/a\u00b2\n\n    sign_normalized = np.sign(normalized_valid)\n    dz_da = -2 * b * np.power(abs_normalized_valid, 2 * b - 1) * sign_normalized * (x_valid - c) / (a * a)\n    dL_da += np.sum(dL_dy_valid * dmu_dz * dz_da)\n\n    # \u2202z/\u2202b = \u2202(|normalized|^(2b))/\u2202b\n    # = |normalized|^(2b) \u00d7 ln(|normalized|) \u00d7 2\n    # But ln(|normalized|) can be problematic near zero, so we use a safe version\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        ln_abs_normalized = np.log(abs_normalized_valid)\n        ln_abs_normalized = np.where(np.isfinite(ln_abs_normalized), ln_abs_normalized, 0.0)\n\n    dz_db = 2 * power_term_valid * ln_abs_normalized\n    dL_db += np.sum(dL_dy_valid * dmu_dz * dz_db)\n\n    # \u2202z/\u2202c = \u2202(|normalized|^(2b))/\u2202c\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 \u2202normalized/\u2202c\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (-1/a)\n    # = -2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) / a\n\n    dz_dc = -2 * b * np.power(abs_normalized_valid, 2 * b - 1) * sign_normalized / a\n    dL_dc += np.sum(dL_dy_valid * dmu_dz * dz_dc)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute bell membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of bell membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute bell membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of bell membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n\n    self.last_input = x\n\n    # Compute the bell function: \u03bc(x) = 1 / (1 + |((x - c) / a)|^(2b))\n    # To avoid numerical issues, we use the absolute value and handle edge cases\n\n    # Compute (x - c) / a\n    normalized = (x - c) / a\n\n    # Compute |normalized|^(2b)\n    # Use np.abs to handle negative values properly\n    abs_normalized = np.abs(normalized)\n\n    # Handle the case where abs_normalized is very close to zero\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        power_term = np.power(abs_normalized, 2 * b)\n        # Replace any inf or nan with a very large number to make output close to 0\n        power_term = np.where(np.isfinite(power_term), power_term, 1e10)\n\n    # Compute the final result\n    output = 1.0 / (1.0 + power_term)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF","title":"anfis_toolbox.membership.SigmoidalMF","text":"<pre><code>SigmoidalMF(a: float = 1.0, c: float = 0.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Sigmoidal Membership Function.</p> <p>Implements a sigmoidal (S-shaped) membership function using the formula: \u03bc(x) = 1 / (1 + exp(-a(x - c)))</p> <p>This function provides a smooth S-shaped curve that transitions from 0 to 1. It's particularly useful for modeling gradual transitions and is commonly used in neural networks and fuzzy systems.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Slope parameter. Controls the steepness of the sigmoid.        - Positive values: standard sigmoid (0 \u2192 1 as x increases)        - Negative values: inverted sigmoid (1 \u2192 0 as x increases)        - Larger |a|: steeper transition</p> <code>1.0</code> <code>c</code> <code>float</code> <p>Center parameter. Controls the inflection point where \u03bc\u00a9 = 0.5.</p> <code>0.0</code> Note <p>Parameter 'a' cannot be zero (would result in constant function).</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Slope parameter (cannot be zero). Defaults to 1.0.</p> <code>1.0</code> <code>c</code> <code>float</code> <p>Center parameter (inflection point). Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is zero.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float = 1.0, c: float = 0.0):\n    \"\"\"Initialize the sigmoidal membership function.\n\n    Args:\n        a: Slope parameter (cannot be zero). Defaults to 1.0.\n        c: Center parameter (inflection point). Defaults to 0.0.\n\n    Raises:\n        ValueError: If 'a' is zero.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if a == 0:\n        raise ValueError(f\"Parameter 'a' cannot be zero, got a={a}\")\n\n    self.parameters = {\"a\": float(a), \"c\": float(c)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients given upstream gradient.</p> <p>For \u03bc(x) = 1/(1 + exp(-a(x-c))): - \u2202\u03bc/\u2202a = \u03bc(x)(1-\u03bc(x))(x-c) - \u2202\u03bc/\u2202c = -a\u03bc(x)(1-\u03bc(x))</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients given upstream gradient.\n\n    For \u03bc(x) = 1/(1 + exp(-a(x-c))):\n    - \u2202\u03bc/\u2202a = \u03bc(x)(1-\u03bc(x))(x-c)\n    - \u2202\u03bc/\u2202c = -a\u03bc(x)(1-\u03bc(x))\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    a = self.parameters[\"a\"]\n    c = self.parameters[\"c\"]\n\n    x = self.last_input\n    y = self.last_output  # This is \u03bc(x)\n\n    # For sigmoid: \u2202\u03bc/\u2202z = \u03bc(1-\u03bc) where z = -a(x-c)\n    # This is a fundamental property of the sigmoid function\n    dmu_dz = y * (1.0 - y)\n\n    # Chain rule: \u2202L/\u2202param = \u2202L/\u2202\u03bc \u00d7 \u2202\u03bc/\u2202z \u00d7 \u2202z/\u2202param\n\n    # For z = a(x-c):\n    # \u2202z/\u2202a = (x-c)\n    # \u2202z/\u2202c = -a\n\n    # Gradient w.r.t. 'a'\n    dz_da = x - c\n    dL_da = np.sum(dL_dy * dmu_dz * dz_da)\n\n    # Gradient w.r.t. 'c'\n    dz_dc = -a\n    dL_dc = np.sum(dL_dy * dmu_dz * dz_dc)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute sigmoidal membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of sigmoidal membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute sigmoidal membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of sigmoidal membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    c = self.parameters[\"c\"]\n\n    self.last_input = x\n\n    # Compute the sigmoid function: \u03bc(x) = 1 / (1 + exp(-a(x - c)))\n    # To avoid numerical overflow, we use a stable implementation\n\n    # Compute a(x - c) (note: not -a(x - c))\n    z = a * (x - c)\n\n    # Use stable sigmoid implementation to avoid overflow\n    # Standard sigmoid: \u03c3(z) = 1 / (1 + exp(-z))\n    # For numerical stability:\n    # If z &gt;= 0: \u03c3(z) = 1 / (1 + exp(-z))\n    # If z &lt; 0: \u03c3(z) = exp(z) / (1 + exp(z))\n\n    output = np.zeros_like(x, dtype=float)\n\n    # Case 1: z &gt;= 0 (standard case)\n    mask_pos = z &gt;= 0\n    if np.any(mask_pos):\n        output[mask_pos] = 1.0 / (1.0 + np.exp(-z[mask_pos]))\n\n    # Case 2: z &lt; 0 (to avoid exp overflow)\n    mask_neg = z &lt; 0\n    if np.any(mask_neg):\n        exp_z = np.exp(z[mask_neg])\n        output[mask_neg] = exp_z / (1.0 + exp_z)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF","title":"anfis_toolbox.membership.DiffSigmoidalMF","text":"<pre><code>DiffSigmoidalMF(a1: float, c1: float, a2: float, c2: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Difference of two sigmoidal functions.</p> <p>Implements y = s1(x) - s2(x), where each s is a logistic curve with its own slope and center parameters.</p> <p>Parameters:</p> Name Type Description Default <code>a1</code> <code>float</code> <p>The first 'a' parameter for the membership function.</p> required <code>c1</code> <code>float</code> <p>The first 'c' parameter for the membership function.</p> required <code>a2</code> <code>float</code> <p>The second 'a' parameter for the membership function.</p> required <code>c2</code> <code>float</code> <p>The second 'c' parameter for the membership function.</p> required <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the membership function parameters.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for each parameter, initialized to 0.0.</p> <code>last_input</code> <code>dict</code> <p>Stores the last input value (initially None).</p> <code>last_output</code> <code>dict</code> <p>Stores the last output value (initially None).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a1: float, c1: float, a2: float, c2: float):\n    \"\"\"Initializes the membership function with two sets of parameters.\n\n    Args:\n        a1 (float): The first 'a' parameter for the membership function.\n        c1 (float): The first 'c' parameter for the membership function.\n        a2 (float): The second 'a' parameter for the membership function.\n        c2 (float): The second 'c' parameter for the membership function.\n\n    Attributes:\n        parameters (dict): Dictionary containing the membership function parameters.\n        gradients (dict): Dictionary containing gradients for each parameter, initialized to 0.0.\n        last_input: Stores the last input value (initially None).\n        last_output: Stores the last output value (initially None).\n    \"\"\"\n    super().__init__()\n    self.parameters = {\n        \"a1\": float(a1),\n        \"c1\": float(c1),\n        \"a2\": float(a2),\n        \"c2\": float(c2),\n    }\n    self.gradients = dict.fromkeys(self.parameters, 0.0)\n    self.last_input = None\n    self.last_output = None\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients w.r.t. parameters and optionally input.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>np.ndarray | None: Gradient of the loss w.r.t. the input, if available.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients w.r.t. parameters and optionally input.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        np.ndarray | None: Gradient of the loss w.r.t. the input, if available.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n    s1, s2 = self._s1, self._s2\n\n    # Sigmoid derivatives\n    ds1_da1 = (x - c1) * s1 * (1 - s1)\n    ds1_dc1 = -a1 * s1 * (1 - s1)\n    ds2_da2 = (x - c2) * s2 * (1 - s2)\n    ds2_dc2 = -a2 * s2 * (1 - s2)\n\n    # Parameter gradients\n    self.gradients[\"a1\"] += float(np.sum(dL_dy * ds1_da1))\n    self.gradients[\"c1\"] += float(np.sum(dL_dy * ds1_dc1))\n    self.gradients[\"a2\"] += float(np.sum(dL_dy * -ds2_da2))\n    self.gradients[\"c2\"] += float(np.sum(dL_dy * -ds2_dc2))\n\n    # Gradient w.r.t. input (optional, for chaining)\n    dmu_dx = a1 * s1 * (1 - s1) - a2 * s2 * (1 - s2)\n    return dL_dy * dmu_dx\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute y = s1(x) - s2(x).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values for the input.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute y = s1(x) - s2(x).\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Membership values for the input.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n\n    s1 = 1.0 / (1.0 + np.exp(-a1 * (x - c1)))\n    s2 = 1.0 / (1.0 + np.exp(-a2 * (x - c2)))\n    y = s1 - s2\n\n    self.last_output = y\n    self._s1, self._s2 = s1, s2  # store for backward\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF","title":"anfis_toolbox.membership.ProdSigmoidalMF","text":"<pre><code>ProdSigmoidalMF(a1: float, c1: float, a2: float, c2: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Product of two sigmoidal functions.</p> <p>Implements \u03bc(x) = s1(x) * s2(x) with separate parameters for each sigmoid.</p> <p>Parameters:</p> Name Type Description Default <code>a1</code> <code>float</code> <p>The first parameter for the membership function.</p> required <code>c1</code> <code>float</code> <p>The second parameter for the membership function.</p> required <code>a2</code> <code>float</code> <p>The third parameter for the membership function.</p> required <code>c2</code> <code>float</code> <p>The fourth parameter for the membership function.</p> required <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the membership function parameters.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for each parameter, initialized to 0.0.</p> <code>last_input</code> <code>dict</code> <p>Stores the last input value (initialized to None).</p> <code>last_output</code> <code>dict</code> <p>Stores the last output value (initialized to None).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a1: float, c1: float, a2: float, c2: float):\n    \"\"\"Initializes the membership function with specified parameters.\n\n    Args:\n        a1 (float): The first parameter for the membership function.\n        c1 (float): The second parameter for the membership function.\n        a2 (float): The third parameter for the membership function.\n        c2 (float): The fourth parameter for the membership function.\n\n    Attributes:\n        parameters (dict): Dictionary containing the membership function parameters.\n        gradients (dict): Dictionary containing gradients for each parameter, initialized to 0.0.\n        last_input: Stores the last input value (initialized to None).\n        last_output: Stores the last output value (initialized to None).\n    \"\"\"\n    super().__init__()\n    self.parameters = {\n        \"a1\": float(a1),\n        \"c1\": float(c1),\n        \"a2\": float(a2),\n        \"c2\": float(c2),\n    }\n    self.gradients = dict.fromkeys(self.parameters, 0.0)\n    self.last_input = None\n    self.last_output = None\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients and optionally return input gradient.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>np.ndarray | None: Gradient of the loss w.r.t. the input, if available.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients and optionally return input gradient.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        np.ndarray | None: Gradient of the loss w.r.t. the input, if available.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n    s1, s2 = self._s1, self._s2\n\n    # derivatives of sigmoids w.r.t. parameters\n    ds1_da1 = (x - c1) * s1 * (1 - s1)\n    ds1_dc1 = -a1 * s1 * (1 - s1)\n    ds2_da2 = (x - c2) * s2 * (1 - s2)\n    ds2_dc2 = -a2 * s2 * (1 - s2)\n\n    # parameter gradients using product rule\n    self.gradients[\"a1\"] += float(np.sum(dL_dy * ds1_da1 * s2))\n    self.gradients[\"c1\"] += float(np.sum(dL_dy * ds1_dc1 * s2))\n    self.gradients[\"a2\"] += float(np.sum(dL_dy * s1 * ds2_da2))\n    self.gradients[\"c2\"] += float(np.sum(dL_dy * s1 * ds2_dc2))\n\n    # gradient w.r.t. input (optional)\n    dmu_dx = a1 * s1 * (1 - s1) * s2 + a2 * s2 * (1 - s2) * s1\n    return dL_dy * dmu_dx\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes the membership value(s) for input x using the product of two sigmoidal functions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array to the membership function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array after applying the membership function.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the membership value(s) for input x using the product of two sigmoidal functions.\n\n    Args:\n        x (np.ndarray): Input array to the membership function.\n\n    Returns:\n        np.ndarray: Output array after applying the membership function.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n\n    s1 = 1.0 / (1.0 + np.exp(-a1 * (x - c1)))\n    s2 = 1.0 / (1.0 + np.exp(-a2 * (x - c2)))\n    y = s1 * s2\n\n    self.last_output = y\n    self._s1, self._s2 = s1, s2  # store for backward\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF","title":"anfis_toolbox.membership.SShapedMF","text":"<pre><code>SShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>S-shaped Membership Function.</p> <p>Smoothly transitions from 0 to 1 between two parameters a and b using the smoothstep polynomial S(t) = 3t\u00b2 - 2t\u00b3. Commonly used in fuzzy logic for gradual onset of membership.</p> <p>Definition with a &lt; b: - \u03bc(x) = 0, for x \u2264 a - \u03bc(x) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a), for a &lt; x &lt; b - \u03bc(x) = 1, for x \u2265 b</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot (start of transition from 0).</p> required <code>b</code> <code>float</code> <p>Right shoulder (end of transition at 1).</p> required Note <p>Requires a &lt; b.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter, must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter, must be greater than 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter, must be less than 'b'.\n        b (float): The second parameter, must be greater than 'a'.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a and b using analytical derivatives.</p> <p>Uses S(t) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a) on the transition region.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a and b using analytical derivatives.\n\n    Uses S(t) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a) on the transition region.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    # Only transition region contributes to parameter gradients\n    mask = (x &gt;= a) &amp; (x &lt;= b)\n    if not (np.any(mask) and b != a):\n        return\n\n    x_t = x[mask]\n    dL_dy_t = dL_dy[mask]\n    t = (x_t - a) / (b - a)\n\n    # dS/dt = 6*t*(1-t)\n    dS_dt = _dsmoothstep_dt(t)\n\n    # dt/da and dt/db\n    dt_da = (x_t - b) / (b - a) ** 2\n    dt_db = -(x_t - a) / (b - a) ** 2\n\n    dS_da = dS_dt * dt_da\n    dS_db = dS_dt * dt_db\n\n    self.gradients[\"a\"] += float(np.sum(dL_dy_t * dS_da))\n    self.gradients[\"b\"] += float(np.sum(dL_dy_t * dS_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute S-shaped membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute S-shaped membership values.\"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # Right side (x \u2265 b): \u03bc = 1\n    mask_right = x &gt;= b\n    y[mask_right] = 1.0\n\n    # Transition region (a &lt; x &lt; b): \u03bc = smoothstep(t)\n    mask_trans = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_trans):\n        x_t = x[mask_trans]\n        t = (x_t - a) / (b - a)\n        y[mask_trans] = _smoothstep(t)\n\n    # Left side (x \u2264 a) remains 0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF","title":"anfis_toolbox.membership.LinSShapedMF","text":"<pre><code>LinSShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Linear S-shaped saturation Membership Function.</p> Piecewise linear ramp from 0 to 1 between parameters a and b <ul> <li>\u03bc(x) = 0, for x \u2264 a</li> <li>\u03bc(x) = (x - a) / (b - a), for a &lt; x &lt; b</li> <li>\u03bc(x) = 1, for x \u2265 b</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot (start of transition from 0).</p> required <code>b</code> <code>float</code> <p>Right shoulder (end of transition at 1). Requires a &lt; b.</p> required <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter, must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter, must be greater than 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter, must be less than 'b'.\n        b (float): The second parameter, must be greater than 'a'.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for 'a' and 'b' in the ramp region.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for 'a' and 'b' in the ramp region.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    d = b - a\n    if d == 0:\n        return\n    # Only ramp region contributes to parameter gradients\n    mask = (x &gt; a) &amp; (x &lt; b)\n    if not np.any(mask):\n        return\n    xm = x[mask]\n    g = dL_dy[mask]\n    # \u03bc = (x-a)/d with d = b-a\n    # \u2202\u03bc/\u2202a = -(1/d) + (x-a)/d^2\n    dmu_da = -(1.0 / d) + (xm - a) / (d * d)\n    # \u2202\u03bc/\u2202b = -(x-a)/d^2\n    dmu_db = -((xm - a) / (d * d))\n    self.gradients[\"a\"] += float(np.sum(g * dmu_da))\n    self.gradients[\"b\"] += float(np.sum(g * dmu_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute linear S-shaped membership values for x.</p> <p>The rules based on a and b: - x &gt;= b: 1.0 (right saturated) - a &lt; x &lt; b: linear ramp from 0 to 1 - x &lt;= a: 0.0 (left)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array with membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute linear S-shaped membership values for x.\n\n    The rules based on a and b:\n    - x &gt;= b: 1.0 (right saturated)\n    - a &lt; x &lt; b: linear ramp from 0 to 1\n    - x &lt;= a: 0.0 (left)\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Output array with membership values.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    y = np.zeros_like(x, dtype=float)\n    # right saturated region\n    mask_right = x &gt;= b\n    y[mask_right] = 1.0\n    # linear ramp\n    mask_mid = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_mid):\n        y[mask_mid] = (x[mask_mid] - a) / (b - a)\n    # left stays 0\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF","title":"anfis_toolbox.membership.ZShapedMF","text":"<pre><code>ZShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Z-shaped Membership Function.</p> <p>Smoothly transitions from 1 to 0 between two parameters a and b using the smoothstep polynomial S(t) = 3t\u00b2 - 2t\u00b3 (Z = 1 - S). Commonly used in fuzzy logic as the complement of the S-shaped function.</p> <p>Definition with a &lt; b: - \u03bc(x) = 1, for x \u2264 a - \u03bc(x) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a), for a &lt; x &lt; b - \u03bc(x) = 0, for x \u2265 b</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left shoulder (start of transition).</p> required <code>b</code> <code>float</code> <p>Right foot (end of transition).</p> required Note <p>Requires a &lt; b. In the degenerate case a == b, the function becomes an instantaneous drop at x=a.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Lower bound parameter.</p> required <code>b</code> <code>float</code> <p>Upper bound parameter.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a is not less than b.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters a and b.\n\n    Args:\n        a: Lower bound parameter.\n        b: Upper bound parameter.\n\n    Raises:\n        ValueError: If a is not less than b.\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a and b using analytical derivatives.</p> <p>Uses Z(t) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a) on the transition region.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a and b using analytical derivatives.\n\n    Uses Z(t) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a) on the transition region.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    # Only transition region contributes to parameter gradients\n    mask = (x &gt;= a) &amp; (x &lt;= b)\n    if not (np.any(mask) and b != a):\n        return\n\n    x_t = x[mask]\n    dL_dy_t = dL_dy[mask]\n    t = (x_t - a) / (b - a)\n\n    # dZ/dt = -dS/dt = 6*t*(t-1)\n    dZ_dt = -_dsmoothstep_dt(t)\n\n    # dt/da and dt/db\n    dt_da = (x_t - b) / (b - a) ** 2\n    dt_db = -(x_t - a) / (b - a) ** 2\n\n    dZ_da = dZ_dt * dt_da\n    dZ_db = dZ_dt * dt_db\n\n    self.gradients[\"a\"] += float(np.sum(dL_dy_t * dZ_da))\n    self.gradients[\"b\"] += float(np.sum(dL_dy_t * dZ_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute Z-shaped membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Z-shaped membership values.\"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # Left side (x \u2264 a): \u03bc = 1\n    mask_left = x &lt;= a\n    y[mask_left] = 1.0\n\n    # Transition region (a &lt; x &lt; b): \u03bc = 1 - smoothstep(t)\n    mask_trans = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_trans):\n        x_t = x[mask_trans]\n        t = (x_t - a) / (b - a)\n        y[mask_trans] = 1.0 - _smoothstep(t)\n\n    # Right side (x \u2265 b) remains 0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF","title":"anfis_toolbox.membership.LinZShapedMF","text":"<pre><code>LinZShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Linear Z-shaped saturation Membership Function.</p> Piecewise linear ramp from 1 to 0 between parameters a and b <ul> <li>\u03bc(x) = 1, for x \u2264 a</li> <li>\u03bc(x) = (b - x) / (b - a), for a &lt; x &lt; b</li> <li>\u03bc(x) = 0, for x \u2265 b</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left shoulder (end of saturation at 1).</p> required <code>b</code> <code>float</code> <p>Right foot (end of transition to 0). Requires a &lt; b.</p> required <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter of the membership function. Must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter of the membership function.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter of the membership function. Must be less than 'b'.\n        b (float): The second parameter of the membership function.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for 'a' and 'b'.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for 'a' and 'b'.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    d = b - a\n    if d == 0:\n        return\n    mask = (x &gt; a) &amp; (x &lt; b)\n    if not np.any(mask):\n        return\n    xm = x[mask]\n    g = dL_dy[mask]\n    # \u03bc = (b-x)/(b-a)\n    # \u2202\u03bc/\u2202a = (b-x)/(d^2)\n    # \u2202\u03bc/\u2202b = (x-a)/(d^2)\n    dmu_da = (b - xm) / (d * d)\n    dmu_db = (xm - a) / (d * d)\n    self.gradients[\"a\"] += float(np.sum(g * dmu_da))\n    self.gradients[\"b\"] += float(np.sum(g * dmu_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute linear Z-shaped membership values for x.</p> <p>Rules: - x &lt;= a: 1.0 (left saturated) - a &lt; x &lt; b: linear ramp from 1 to 0 - x &gt;= b: 0.0 (right)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output membership values for each input.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute linear Z-shaped membership values for x.\n\n    Rules:\n    - x &lt;= a: 1.0 (left saturated)\n    - a &lt; x &lt; b: linear ramp from 1 to 0\n    - x &gt;= b: 0.0 (right)\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Output membership values for each input.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    y = np.zeros_like(x, dtype=float)\n\n    # left saturated region\n    mask_left = x &lt;= a\n    y[mask_left] = 1.0\n    # linear ramp\n    mask_mid = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_mid):\n        y[mask_mid] = (b - x[mask_mid]) / (b - a)\n    # right stays 0\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF","title":"anfis_toolbox.membership.PiMF","text":"<pre><code>PiMF(a: float, b: float, c: float, d: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Pi-shaped membership function.</p> <p>The Pi-shaped membership function is characterized by a trapezoidal-like shape with smooth S-shaped transitions on both sides. It is defined by four parameters that control the shape and position:</p> <p>Mathematical definition: \u03bc(x) = S(x; a, b) for x \u2208 [a, b]      = 1 for x \u2208 [b, c]      = Z(x; c, d) for x \u2208 [c, d]      = 0 elsewhere</p> <p>Where: - S(x; a, b) is an S-shaped function from 0 to 1 - Z(x; c, d) is a Z-shaped function from 1 to 0</p> <p>The S and Z functions use smooth cubic splines for differentiability: S(x; a, b) = 2*((x-a)/(b-a))^3 for x \u2208 [a, (a+b)/2]            = 1 - 2*((b-x)/(b-a))^3 for x \u2208 [(a+b)/2, b]</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot of the function (where function starts rising from 0)</p> required <code>b</code> <code>float</code> <p>Left shoulder of the function (where function reaches 1)</p> required <code>c</code> <code>float</code> <p>Right shoulder of the function (where function starts falling from 1)</p> required <code>d</code> <code>float</code> <p>Right foot of the function (where function reaches 0)</p> required Note <p>Parameters must satisfy: a &lt; b \u2264 c &lt; d</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot parameter.</p> required <code>b</code> <code>float</code> <p>Left shoulder parameter.</p> required <code>c</code> <code>float</code> <p>Right shoulder parameter.</p> required <code>d</code> <code>float</code> <p>Right foot parameter.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters don't satisfy a &lt; b \u2264 c &lt; d.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float, d: float):\n    \"\"\"Initialize the Pi-shaped membership function.\n\n    Args:\n        a: Left foot parameter.\n        b: Left shoulder parameter.\n        c: Right shoulder parameter.\n        d: Right foot parameter.\n\n    Raises:\n        ValueError: If parameters don't satisfy a &lt; b \u2264 c &lt; d.\n    \"\"\"\n    super().__init__()\n\n    # Parameter validation\n    if not (a &lt; b &lt;= c &lt; d):\n        raise ValueError(f\"Parameters must satisfy a &lt; b \u2264 c &lt; d, got a={a}, b={b}, c={c}, d={d}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"d\": float(d)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0, \"c\": 0.0, \"d\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients for backpropagation.</p> <p>Analytical gradients are computed by region: - S-function: gradients w.r.t. a, b - Z-function: gradients w.r.t. c, d - Flat region: no gradients</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of loss w.r.t. function output.</p> required Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients for backpropagation.\n\n    Analytical gradients are computed by region:\n    - S-function: gradients w.r.t. a, b\n    - Z-function: gradients w.r.t. c, d\n    - Flat region: no gradients\n\n    Args:\n        dL_dy: Gradient of loss w.r.t. function output.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b, c, d = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"], self.parameters[\"d\"]\n\n    # Initialize gradients\n    grad_a = grad_b = grad_c = grad_d = 0.0\n\n    # S-function gradients [a, b]\n    mask_s = (x &gt;= a) &amp; (x &lt;= b)\n    if np.any(mask_s) and b != a:\n        x_s = x[mask_s]\n        dL_dy_s = dL_dy[mask_s]\n        t = (x_s - a) / (b - a)\n\n        # Calculate parameter derivatives\n        dt_da = (x_s - b) / (b - a) ** 2  # Correct derivative\n        dt_db = -(x_s - a) / (b - a) ** 2\n\n        # For smoothstep S(t) = 3*t\u00b2 - 2*t\u00b3, derivative is dS/dt = 6*t - 6*t\u00b2 = 6*t*(1-t)\n        dS_dt = _dsmoothstep_dt(t)\n\n        # Apply chain rule: dS/da = dS/dt * dt/da\n        dS_da = dS_dt * dt_da\n        dS_db = dS_dt * dt_db\n\n        grad_a += np.sum(dL_dy_s * dS_da)\n        grad_b += np.sum(dL_dy_s * dS_db)\n\n    # Z-function gradients [c, d]\n    mask_z = (x &gt;= c) &amp; (x &lt;= d)\n    if np.any(mask_z) and d != c:\n        x_z = x[mask_z]\n        dL_dy_z = dL_dy[mask_z]\n        t = (x_z - c) / (d - c)\n\n        # Calculate parameter derivatives\n        dt_dc = (x_z - d) / (d - c) ** 2  # Correct derivative\n        dt_dd = -(x_z - c) / (d - c) ** 2\n\n        # For Z(t) = 1 - S(t) = 1 - (3*t\u00b2 - 2*t\u00b3), derivative is dZ/dt = -dS/dt = -6*t*(1-t) = 6*t*(t-1)\n        dZ_dt = -_dsmoothstep_dt(t)\n\n        # Apply chain rule: dZ/dc = dZ/dt * dt/dc\n        dZ_dc = dZ_dt * dt_dc\n        dZ_dd = dZ_dt * dt_dd\n\n        grad_c += np.sum(dL_dy_z * dZ_dc)\n        grad_d += np.sum(dL_dy_z * dZ_dd)\n\n    # Accumulate gradients\n    self.gradients[\"a\"] += grad_a\n    self.gradients[\"b\"] += grad_b\n    self.gradients[\"c\"] += grad_c\n    self.gradients[\"d\"] += grad_d\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the Pi-shaped membership function.</p> <p>Combines S and Z functions for smooth transitions: - Rising edge: S-function from a to b - Flat top: constant 1 from b to c - Falling edge: Z-function from c to d - Outside: 0</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values \u03bc(x) \u2208 [0, 1].</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the Pi-shaped membership function.\n\n    Combines S and Z functions for smooth transitions:\n    - Rising edge: S-function from a to b\n    - Flat top: constant 1 from b to c\n    - Falling edge: Z-function from c to d\n    - Outside: 0\n\n    Args:\n        x: Input values.\n\n    Returns:\n        np.ndarray: Membership values \u03bc(x) \u2208 [0, 1].\n    \"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b, c, d = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"], self.parameters[\"d\"]\n\n    # Initialize output\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # S-function for rising edge [a, b]\n    mask_s = (x &gt;= a) &amp; (x &lt;= b)\n    if np.any(mask_s):\n        x_s = x[mask_s]\n        # Avoid division by zero\n        if b != a:\n            t = (x_s - a) / (b - a)  # Normalize to [0, 1]\n\n            # Smooth S-function using smoothstep: S(t) = 3*t\u00b2 - 2*t\u00b3\n            # This is continuous and differentiable across the entire [0,1] interval\n            y_s = _smoothstep(t)\n\n            y[mask_s] = y_s\n        else:\n            # Degenerate case: instant transition\n            y[mask_s] = 1.0\n\n    # Flat region [b, c]: \u03bc(x) = 1\n    mask_flat = (x &gt;= b) &amp; (x &lt;= c)\n    y[mask_flat] = 1.0\n\n    # Z-function for falling edge [c, d]\n    mask_z = (x &gt;= c) &amp; (x &lt;= d)\n    if np.any(mask_z):\n        x_z = x[mask_z]\n        # Avoid division by zero\n        if d != c:\n            t = (x_z - c) / (d - c)  # Normalize to [0, 1]\n\n            # Smooth Z-function (inverted smoothstep): Z(t) = 1 - S(t) = 1 - (3*t\u00b2 - 2*t\u00b3)\n            # This is continuous and differentiable, going from 1 to 0\n            y_z = 1 - _smoothstep(t)\n\n            y[mask_z] = y_z\n        else:\n            # Degenerate case: instant transition\n            y[mask_z] = 0.0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF","title":"anfis_toolbox.membership.TriangularMF","text":"<pre><code>TriangularMF(a: float, b: float, c: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Triangular Membership Function.</p> <p>Implements a triangular membership function using piecewise linear segments: \u03bc(x) = { 0,           x \u2264 a or x \u2265 c        { (x-a)/(b-a), a &lt; x &lt; b        { (c-x)/(c-b), b \u2264 x &lt; c</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point of the triangle.</p> required <code>b</code> <code>float</code> <p>Peak point of the triangle (\u03bc(b) = 1).</p> required <code>c</code> <code>float</code> <p>Right base point of the triangle.</p> required Note <p>Must satisfy: a \u2264 b \u2264 c</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point (must satisfy a \u2264 b).</p> required <code>b</code> <code>float</code> <p>Peak point (must satisfy a \u2264 b \u2264 c).</p> required <code>c</code> <code>float</code> <p>Right base point (must satisfy b \u2264 c).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters do not satisfy a \u2264 b \u2264 c or if a == c (zero width).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float):\n    \"\"\"Initialize the triangular membership function.\n\n    Args:\n        a: Left base point (must satisfy a \u2264 b).\n        b: Peak point (must satisfy a \u2264 b \u2264 c).\n        c: Right base point (must satisfy b \u2264 c).\n\n    Raises:\n        ValueError: If parameters do not satisfy a \u2264 b \u2264 c or if a == c (zero width).\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt;= b &lt;= c):\n        raise ValueError(f\"Triangular MF parameters must satisfy a \u2264 b \u2264 c, got a={a}, b={b}, c={c}\")\n    if a == c:\n        raise ValueError(\"Parameters 'a' and 'c' cannot be equal (zero width triangle)\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c)}\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a, b, c given upstream gradient.</p> <p>Computes analytical derivatives for the rising (a, b) and falling (b, c) regions and sums them over the batch.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. \u03bc(x); same shape or broadcastable to output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a, b, c given upstream gradient.\n\n    Computes analytical derivatives for the rising (a, b) and falling (b, c)\n    regions and sums them over the batch.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. \u03bc(x); same shape or broadcastable to output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    a, b, c = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"]\n    x = self.last_input\n\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n\n    # Left slope: a &lt; x &lt; b\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        if np.any(left_mask):\n            x_left = x[left_mask]\n            dL_dy_left = dL_dy[left_mask]\n\n            # \u2202\u03bc/\u2202a = (x - b) / (b - a)^2\n            dmu_da_left = (x_left - b) / ((b - a) ** 2)\n            dL_da += np.sum(dL_dy_left * dmu_da_left)\n\n            # \u2202\u03bc/\u2202b = -(x - a) / (b - a)^2\n            dmu_db_left = -(x_left - a) / ((b - a) ** 2)\n            dL_db += np.sum(dL_dy_left * dmu_db_left)\n\n    # Right slope: b &lt; x &lt; c\n    if c &gt; b:\n        right_mask = (x &gt; b) &amp; (x &lt; c)\n        if np.any(right_mask):\n            x_right = x[right_mask]\n            dL_dy_right = dL_dy[right_mask]\n\n            # \u2202\u03bc/\u2202b = (x - c) / (c - b)^2\n            dmu_db_right = (x_right - c) / ((c - b) ** 2)\n            dL_db += np.sum(dL_dy_right * dmu_db_right)\n\n            # \u2202\u03bc/\u2202c = (x - b) / (c - b)^2\n            dmu_dc_right = (x_right - b) / ((c - b) ** 2)\n            dL_dc += np.sum(dL_dy_right * dmu_dc_right)\n\n    # Update gradients\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute triangular membership values \u03bc(x).</p> <p>Uses piecewise linear segments defined by (a, b, c): - 0 outside [a, c] - rising slope in (a, b) - peak 1 at x == b - falling slope in (b, c)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values in [0, 1] with the same shape as x.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute triangular membership values \u03bc(x).\n\n    Uses piecewise linear segments defined by (a, b, c):\n    - 0 outside [a, c]\n    - rising slope in (a, b)\n    - peak 1 at x == b\n    - falling slope in (b, c)\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Membership values in [0, 1] with the same shape as x.\n    \"\"\"\n    a, b, c = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"]\n    self.last_input = x\n\n    output = np.zeros_like(x, dtype=float)\n\n    # Left slope\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        output[left_mask] = (x[left_mask] - a) / (b - a)\n\n    # Peak\n    peak_mask = x == b\n    output[peak_mask] = 1.0\n\n    # Right slope\n    if c &gt; b:\n        right_mask = (x &gt; b) &amp; (x &lt; c)\n        output[right_mask] = (c - x[right_mask]) / (c - b)\n\n    # Clip for numerical stability\n    output = np.clip(output, 0.0, 1.0)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF","title":"anfis_toolbox.membership.TrapezoidalMF","text":"<pre><code>TrapezoidalMF(a: float, b: float, c: float, d: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Trapezoidal Membership Function.</p> <p>Implements a trapezoidal membership function using piecewise linear segments: \u03bc(x) = { 0,           x \u2264 a or x \u2265 d        { (x-a)/(b-a), a &lt; x &lt; b        { 1,           b \u2264 x \u2264 c        { (d-x)/(d-c), c &lt; x &lt; d</p> <p>This function is commonly used in fuzzy logic systems when you need a plateau region of full membership, providing robustness to noise and uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point of the trapezoid (lower support bound).</p> required <code>b</code> <code>float</code> <p>Left peak point (start of plateau where \u03bc(x) = 1).</p> required <code>c</code> <code>float</code> <p>Right peak point (end of plateau where \u03bc(x) = 1).</p> required <code>d</code> <code>float</code> <p>Right base point of the trapezoid (upper support bound).</p> required Note <p>Parameters must satisfy: a \u2264 b \u2264 c \u2264 d for a valid trapezoidal function.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point (\u03bc(a) = 0).</p> required <code>b</code> <code>float</code> <p>Left peak point (\u03bc(b) = 1, start of plateau).</p> required <code>c</code> <code>float</code> <p>Right peak point (\u03bc\u00a9 = 1, end of plateau).</p> required <code>d</code> <code>float</code> <p>Right base point (\u03bc(d) = 0).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters don't satisfy a \u2264 b \u2264 c \u2264 d.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float, d: float):\n    \"\"\"Initialize the trapezoidal membership function.\n\n    Args:\n        a: Left base point (\u03bc(a) = 0).\n        b: Left peak point (\u03bc(b) = 1, start of plateau).\n        c: Right peak point (\u03bc(c) = 1, end of plateau).\n        d: Right base point (\u03bc(d) = 0).\n\n    Raises:\n        ValueError: If parameters don't satisfy a \u2264 b \u2264 c \u2264 d.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if not (a &lt;= b &lt;= c &lt;= d):\n        raise ValueError(f\"Trapezoidal MF parameters must satisfy a \u2264 b \u2264 c \u2264 d, got a={a}, b={b}, c={c}, d={d}\")\n\n    if a == d:\n        raise ValueError(\"Parameters 'a' and 'd' cannot be equal (zero width trapezoid)\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"d\": float(d)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients for parameters based on upstream loss gradient.</p> <p>Analytical gradients for the piecewise linear function: - \u2202\u03bc/\u2202a: left slope - \u2202\u03bc/\u2202b: left slope and plateau transition - \u2202\u03bc/\u2202c: right slope and plateau transition - \u2202\u03bc/\u2202d: right slope</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients for parameters based on upstream loss gradient.\n\n    Analytical gradients for the piecewise linear function:\n    - \u2202\u03bc/\u2202a: left slope\n    - \u2202\u03bc/\u2202b: left slope and plateau transition\n    - \u2202\u03bc/\u2202c: right slope and plateau transition\n    - \u2202\u03bc/\u2202d: right slope\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n    d = self.parameters[\"d\"]\n\n    x = self.last_input\n\n    # Initialize gradients\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n    dL_dd = 0.0\n\n    # Left slope region: a &lt; x &lt; b, \u03bc(x) = (x-a)/(b-a)\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        if np.any(left_mask):\n            x_left = x[left_mask]\n            dL_dy_left = dL_dy[left_mask]\n\n            # \u2202\u03bc/\u2202a = -1/(b-a) for left slope\n            dmu_da_left = -1.0 / (b - a)\n            dL_da += np.sum(dL_dy_left * dmu_da_left)\n\n            # \u2202\u03bc/\u2202b = -(x-a)/(b-a)\u00b2 for left slope\n            dmu_db_left = -(x_left - a) / ((b - a) ** 2)\n            dL_db += np.sum(dL_dy_left * dmu_db_left)\n\n    # Plateau region: b \u2264 x \u2264 c, \u03bc(x) = 1\n    # No gradients for plateau region (constant function)\n\n    # Right slope region: c &lt; x &lt; d, \u03bc(x) = (d-x)/(d-c)\n    if d &gt; c:\n        right_mask = (x &gt; c) &amp; (x &lt; d)\n        if np.any(right_mask):\n            x_right = x[right_mask]\n            dL_dy_right = dL_dy[right_mask]\n\n            # \u2202\u03bc/\u2202c = (x-d)/(d-c)\u00b2 for right slope\n            dmu_dc_right = (x_right - d) / ((d - c) ** 2)\n            dL_dc += np.sum(dL_dy_right * dmu_dc_right)\n\n            # \u2202\u03bc/\u2202d = (x-c)/(d-c)\u00b2 for right slope (derivative of (d-x)/(d-c) w.r.t. d)\n            dmu_dd_right = (x_right - c) / ((d - c) ** 2)\n            dL_dd += np.sum(dL_dy_right * dmu_dd_right)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n    self.gradients[\"d\"] += dL_dd\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute trapezoidal membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array containing the trapezoidal membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute trapezoidal membership values.\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Array containing the trapezoidal membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n    d = self.parameters[\"d\"]\n\n    self.last_input = x\n\n    # Initialize output with zeros\n    output = np.zeros_like(x)\n\n    # Left slope: (x - a) / (b - a) for a &lt; x &lt; b\n    if b &gt; a:  # Avoid division by zero\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        output[left_mask] = (x[left_mask] - a) / (b - a)\n\n    # Plateau: \u03bc(x) = 1 for b \u2264 x \u2264 c\n    plateau_mask = (x &gt;= b) &amp; (x &lt;= c)\n    output[plateau_mask] = 1.0\n\n    # Right slope: (d - x) / (d - c) for c &lt; x &lt; d\n    if d &gt; c:  # Avoid division by zero\n        right_mask = (x &gt; c) &amp; (x &lt; d)\n        output[right_mask] = (d - x[right_mask]) / (d - c)\n\n    # Values outside [a, d] are already zero\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/metrics/","title":"Metrics API","text":"<p>This module provides comprehensive metrics for evaluating ANFIS models across regression, classification, and clustering tasks.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics","title":"anfis_toolbox.metrics","text":"<p>Common metrics utilities for ANFIS Toolbox.</p> <p>This module provides lightweight, dependency-free metrics that are useful for training and evaluating ANFIS models.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics","title":"ANFISMetrics","text":"<p>Metrics calculator utilities for ANFIS models.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics.classification_metrics","title":"classification_metrics  <code>staticmethod</code>","text":"<pre><code>classification_metrics(\n    y_true: ArrayLike,\n    y_pred: ArrayLike | None = None,\n    *,\n    y_proba: ArrayLike | None = None,\n    logits: ArrayLike | None = None,\n) -&gt; dict[str, MetricValue]\n</code></pre> <p>Return common classification metrics for encoded targets and predictions.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics.model_complexity_metrics","title":"model_complexity_metrics  <code>staticmethod</code>","text":"<pre><code>model_complexity_metrics(model: ANFIS) -&gt; dict[str, int]\n</code></pre> <p>Compute structural statistics for an ANFIS model instance.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics.regression_metrics","title":"regression_metrics  <code>staticmethod</code>","text":"<pre><code>regression_metrics(\n    y_true: ArrayLike, y_pred: ArrayLike\n) -&gt; dict[str, MetricValue]\n</code></pre> <p>Return a suite of regression metrics for predictions vs. targets.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport","title":"MetricReport  <code>dataclass</code>","text":"<pre><code>MetricReport(\n    task: Literal[\"regression\", \"classification\"],\n    _values: Mapping[str, MetricValue],\n)\n</code></pre> <p>Immutable container exposing computed metrics by key or attribute.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(item: str) -&gt; MetricValue\n</code></pre> <p>Allow attribute-style access to stored metrics.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; MetricValue\n</code></pre> <p>Provide dictionary-style access to metric values.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Sanitize stored NumPy scalars/arrays to prevent accidental mutation.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Expose the metric key iterator from the backing mapping.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, MetricValue]\n</code></pre> <p>Return a shallow copy of the underlying metric mapping.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.accuracy","title":"accuracy","text":"<pre><code>accuracy(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute accuracy from integer/one-hot labels and logits/probabilities.</p> <p>y_pred can be class indices (n,), logits (n,k), or probabilities (n,k). y_true can be class indices (n,) or one-hot (n,k).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.balanced_accuracy_score","title":"balanced_accuracy_score","text":"<pre><code>balanced_accuracy_score(\n    y_true: ArrayLike, y_pred: ArrayLike\n) -&gt; float\n</code></pre> <p>Return the macro-average recall, balancing performance across classes.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.classification_entropy","title":"classification_entropy","text":"<pre><code>classification_entropy(\n    U: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Classification Entropy (CE). Lower is better (crisper).</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Membership matrix of shape (n_samples, n_clusters).</p> required <code>epsilon</code> <code>float</code> <p>Small constant to avoid log(0).</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>CE value as float.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.compute_metrics","title":"compute_metrics","text":"<pre><code>compute_metrics(\n    y_true: ArrayLike,\n    *,\n    y_pred: ArrayLike | None = None,\n    y_proba: ArrayLike | None = None,\n    logits: ArrayLike | None = None,\n    task: Literal[\n        \"auto\", \"regression\", \"classification\"\n    ] = \"auto\",\n    metrics: Sequence[str] | None = None,\n    custom_metrics: Mapping[str, MetricFn] | None = None,\n) -&gt; MetricReport\n</code></pre> <p>Compute regression or classification metrics and return a report.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(\n    y_true, logits: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute mean cross-entropy from integer labels or one-hot vs logits.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of shape (n_samples,) of integer class labels, or     one-hot array of shape (n_samples, n_classes).</p> required <code>logits</code> <code>ndarray</code> <p>Array-like raw scores, shape (n_samples, n_classes).</p> required <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>Mean cross-entropy (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.explained_variance_score","title":"explained_variance_score","text":"<pre><code>explained_variance_score(\n    y_true, y_pred, epsilon: float = _EPSILON\n) -&gt; float\n</code></pre> <p>Compute the explained variance score for regression predictions.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.log_loss","title":"log_loss","text":"<pre><code>log_loss(\n    y_true, y_prob: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute mean log loss from integer/one-hot labels and probabilities.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_absolute_error","title":"mean_absolute_error","text":"<pre><code>mean_absolute_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean absolute error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values, shape (...,)</p> required <code>y_pred</code> <p>Array-like of predicted values, same shape as y_true</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of absolute differences over all elements as a float.</p> Notes <ul> <li>Inputs are coerced to NumPy arrays with dtype=float.</li> <li>Broadcasting follows NumPy semantics. If shapes are not compatible   for element-wise subtraction, a ValueError will be raised by NumPy.</li> </ul>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_absolute_percentage_error","title":"mean_absolute_percentage_error","text":"<pre><code>mean_absolute_percentage_error(\n    y_true,\n    y_pred,\n    epsilon: float = 1e-12,\n    *,\n    ignore_zero_targets: bool = False,\n) -&gt; float\n</code></pre> <p>Compute the mean absolute percentage error (MAPE) in percent.</p> <p>MAPE = mean( abs((y_true - y_pred) / max(abs(y_true), epsilon)) ) * 100</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values.</p> required <code>y_pred</code> <p>Array-like of predicted values, broadcastable to y_true.</p> required <code>epsilon</code> <code>float</code> <p>Small constant to avoid division by zero when y_true == 0.</p> <code>1e-12</code> <code>ignore_zero_targets</code> <code>bool</code> <p>When True, drop samples where |y_true| &lt;= epsilon; if all targets are (near) zero, returns <code>np.inf</code> to signal undefined percentage.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>MAPE value as a percentage (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_bias_error","title":"mean_bias_error","text":"<pre><code>mean_bias_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean signed error, positive when predictions overshoot.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_squared_error","title":"mean_squared_error","text":"<pre><code>mean_squared_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values, shape (...,)</p> required <code>y_pred</code> <p>Array-like of predicted values, same shape as y_true</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of squared differences over all elements as a float.</p> Notes <ul> <li>Inputs are coerced to NumPy arrays with dtype=float.</li> <li>Broadcasting follows NumPy semantics. If shapes are not compatible   for element-wise subtraction, a ValueError will be raised by NumPy.</li> </ul>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_squared_logarithmic_error","title":"mean_squared_logarithmic_error","text":"<pre><code>mean_squared_logarithmic_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean squared logarithmic error (MSLE).</p> <p>Requires non-negative inputs. Uses log1p for numerical stability: MSLE = mean( (log1p(y_true) - log1p(y_pred))^2 ).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.median_absolute_error","title":"median_absolute_error","text":"<pre><code>median_absolute_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Return the median absolute deviation between predictions and targets.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.partition_coefficient","title":"partition_coefficient","text":"<pre><code>partition_coefficient(U: ndarray) -&gt; float\n</code></pre> <p>Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Membership matrix of shape (n_samples, n_clusters).</p> required <p>Returns:</p> Type Description <code>float</code> <p>PC value as float.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.pearson_correlation","title":"pearson_correlation","text":"<pre><code>pearson_correlation(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the Pearson correlation coefficient r.</p> <p>Returns 0.0 when the standard deviation of either input is ~0 (undefined r).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.precision_recall_f1","title":"precision_recall_f1","text":"<pre><code>precision_recall_f1(\n    y_true: ArrayLike,\n    y_pred: ArrayLike,\n    average: Literal[\"macro\", \"micro\", \"binary\"] = \"macro\",\n) -&gt; tuple[float, float, float]\n</code></pre> <p>Compute precision, recall, and F1 score with the requested averaging.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.quick_evaluate","title":"quick_evaluate","text":"<pre><code>quick_evaluate(\n    model: object,\n    X_test: ndarray,\n    y_test: ndarray,\n    print_results: bool = True,\n    task: Literal[\n        \"auto\", \"regression\", \"classification\"\n    ] = \"auto\",\n) -&gt; dict[str, float]\n</code></pre> <p>Evaluate a trained ANFIS model or estimator on test data.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.r2_score","title":"r2_score","text":"<pre><code>r2_score(y_true, y_pred, epsilon: float = 1e-12) -&gt; float\n</code></pre> <p>Compute the coefficient of determination R^2.</p> <p>R^2 = 1 - SS_res / SS_tot, where SS_res = sum((y - y_hat)^2) and SS_tot = sum((y - mean(y))^2). If SS_tot is ~0 (constant target), returns 1.0 when predictions match the constant target (SS_res ~0), otherwise 0.0.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.root_mean_squared_error","title":"root_mean_squared_error","text":"<pre><code>root_mean_squared_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the root mean squared error (RMSE).</p> <p>This is simply the square root of mean_squared_error.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.softmax","title":"softmax","text":"<pre><code>softmax(logits: ndarray, axis: int = -1) -&gt; np.ndarray\n</code></pre> <p>Compute a numerically stable softmax along a given axis.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.symmetric_mean_absolute_percentage_error","title":"symmetric_mean_absolute_percentage_error","text":"<pre><code>symmetric_mean_absolute_percentage_error(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the symmetric mean absolute percentage error (SMAPE) in percent.</p> <p>SMAPE = mean( 200 * |y_true - y_pred| / (|y_true| + |y_pred|) ) with an epsilon added to denominator to avoid division by zero.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values.</p> required <code>y_pred</code> <p>Array-like of predicted values, broadcastable to y_true.</p> required <code>epsilon</code> <code>float</code> <p>Small constant added to denominator to avoid division by zero.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>SMAPE value as a percentage (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.xie_beni_index","title":"xie_beni_index","text":"<pre><code>xie_beni_index(\n    X: ndarray,\n    U: ndarray,\n    C: ndarray,\n    m: float = 2.0,\n    epsilon: float = 1e-12,\n) -&gt; float\n</code></pre> <p>Xie-Beni index (XB). Lower is better.</p> <p>XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data array, shape (n_samples, n_features) or (n_samples,).</p> required <code>U</code> <code>ndarray</code> <p>Membership matrix, shape (n_samples, n_clusters).</p> required <code>C</code> <code>ndarray</code> <p>Cluster centers, shape (n_clusters, n_features).</p> required <code>m</code> <code>float</code> <p>Fuzzifier (&gt;1).</p> <code>2.0</code> <code>epsilon</code> <code>float</code> <p>Small constant to avoid division by zero.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>XB value as float (np.inf when centers &lt; 2).</p>"},{"location":"api/metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Functions for evaluating regression model performance:</p> <ul> <li><code>mean_squared_error()</code> - Mean squared error</li> <li><code>mean_absolute_error()</code> - Mean absolute error</li> <li><code>root_mean_squared_error()</code> - Root mean squared error</li> <li><code>mean_absolute_percentage_error()</code> - Mean absolute percentage error</li> <li><code>symmetric_mean_absolute_percentage_error()</code> - Symmetric MAPE</li> <li><code>r2_score()</code> - Coefficient of determination</li> <li><code>explained_variance_score()</code> - Explained variance of predictions</li> <li><code>median_absolute_error()</code> - Median absolute error (robust to outliers)</li> <li><code>mean_bias_error()</code> - Mean prediction bias</li> <li><code>pearson_correlation()</code> - Pearson correlation coefficient</li> <li><code>mean_squared_logarithmic_error()</code> - Mean squared logarithmic error</li> </ul>"},{"location":"api/metrics/#classification-metrics","title":"Classification Metrics","text":"<p>Functions for evaluating classification model performance:</p> <ul> <li><code>softmax()</code> - Numerically stable softmax</li> <li><code>cross_entropy()</code> - Cross-entropy loss</li> <li><code>log_loss()</code> - Log loss</li> <li><code>accuracy()</code> - Classification accuracy</li> <li><code>balanced_accuracy_score()</code> - Macro average of per-class recall</li> <li><code>precision_recall_f1()</code> - Precision, recall, and F1 with macro/micro/binary averaging</li> </ul>"},{"location":"api/metrics/#clustering-validation","title":"Clustering Validation","text":"<p>Functions for evaluating fuzzy clustering quality:</p> <ul> <li><code>partition_coefficient()</code> - Bezdek's partition coefficient</li> <li><code>classification_entropy()</code> - Classification entropy</li> <li><code>xie_beni_index()</code> - Xie-Beni validity index</li> </ul>"},{"location":"api/metrics/#metric-reports-automation","title":"Metric Reports &amp; Automation","text":"<ul> <li><code>compute_metrics()</code> - One-stop helper that infers the task (regression vs. classification) and returns a <code>MetricReport</code></li> <li><code>MetricReport</code> - Read-only container with attribute/dict-style access and a <code>.to_dict()</code> export</li> </ul>"},{"location":"api/models/","title":"ANFIS Models","text":""},{"location":"api/models/#anfis_toolbox.model.ANFIS","title":"anfis_toolbox.model.ANFIS  <code>module-attribute</code>","text":"<pre><code>ANFIS = TSKANFIS\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier","title":"anfis_toolbox.model.ANFISClassifier  <code>module-attribute</code>","text":"<pre><code>ANFISClassifier = TSKANFISClassifier\n</code></pre>"},{"location":"api/optim/","title":"Optimization","text":""},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer","title":"anfis_toolbox.optim.base.BaseTrainer","text":"<p>               Bases: <code>ABC</code></p> <p>Shared training loop for ANFIS trainers.</p>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.compute_loss","title":"compute_loss  <code>abstractmethod</code>","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Compute loss for the provided data without mutating the model.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:  # pragma: no cover - abstract\n    \"\"\"Compute loss for the provided data without mutating the model.\"\"\"\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.fit","title":"fit","text":"<pre><code>fit(\n    model,\n    X: ndarray,\n    y: ndarray,\n    *,\n    validation_data: tuple[ndarray, ndarray] | None = None,\n    validation_frequency: int = 1,\n) -&gt; TrainingHistory\n</code></pre> <p>Train <code>model</code> on <code>(X, y)</code> and optionally evaluate on validation data.</p> <p>Returns a dictionary containing the per-epoch training losses and, when <code>validation_data</code> is provided, the validation losses (aligned with the training epochs; epochs without validation are recorded as <code>None</code>).</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>def fit(\n    self,\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    *,\n    validation_data: tuple[np.ndarray, np.ndarray] | None = None,\n    validation_frequency: int = 1,\n) -&gt; TrainingHistory:\n    \"\"\"Train ``model`` on ``(X, y)`` and optionally evaluate on validation data.\n\n    Returns a dictionary containing the per-epoch training losses and, when\n    ``validation_data`` is provided, the validation losses (aligned with the\n    training epochs; epochs without validation are recorded as ``None``).\n    \"\"\"\n    if validation_frequency &lt; 1:\n        raise ValueError(\"validation_frequency must be &gt;= 1\")\n\n    X_train, y_train = self._prepare_training_data(model, X, y)\n    state = self.init_state(model, X_train, y_train)\n\n    prepared_val: tuple[np.ndarray, np.ndarray] | None = None\n    if validation_data is not None:\n        prepared_val = self._prepare_validation_data(model, *validation_data)\n\n    epochs = int(getattr(self, \"epochs\", 1))\n    batch_size = getattr(self, \"batch_size\", None)\n    shuffle = bool(getattr(self, \"shuffle\", True))\n    verbose = bool(getattr(self, \"verbose\", False))\n\n    train_history: list[float] = []\n    val_history: list[float | None] = [] if prepared_val is not None else []\n\n    n_samples = X_train.shape[0]\n    for epoch_idx in range(epochs):\n        epoch_losses: list[float] = []\n        if batch_size is None:\n            loss, state = self.train_step(model, X_train, y_train, state)\n            epoch_losses.append(float(loss))\n        else:\n            indices = np.arange(n_samples)\n            if shuffle:\n                np.random.shuffle(indices)\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                batch_idx = indices[start:end]\n                loss, state = self.train_step(\n                    model,\n                    X_train[batch_idx],\n                    y_train[batch_idx],\n                    state,\n                )\n                epoch_losses.append(float(loss))\n\n        epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        train_history.append(epoch_loss)\n\n        val_loss: float | None = None\n        if prepared_val is not None:\n            if (epoch_idx + 1) % validation_frequency == 0:\n                X_val, y_val = prepared_val\n                val_loss = float(self.compute_loss(model, X_val, y_val))\n            val_history.append(val_loss)\n\n        self._log_epoch(epoch_idx, epoch_loss, val_loss, verbose)\n\n    result: TrainingHistory = {\"train\": train_history}\n    if prepared_val is not None:\n        result[\"val\"] = val_history\n    return result\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.init_state","title":"init_state  <code>abstractmethod</code>","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize and return any optimizer-specific state.</p> <p>Called once before training begins. Trainers that don't require state may return None.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to be trained.</p> required <code>X</code> <code>ndarray</code> <p>The full training inputs.</p> required <code>y</code> <code>ndarray</code> <p>The full training targets.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Optimizer state (or None) to be threaded through <code>train_step</code>.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef init_state(self, model, X: np.ndarray, y: np.ndarray):  # pragma: no cover - abstract\n    \"\"\"Initialize and return any optimizer-specific state.\n\n    Called once before training begins. Trainers that don't require state may\n    return None.\n\n    Parameters:\n        model: The model to be trained.\n        X (np.ndarray): The full training inputs.\n        y (np.ndarray): The full training targets.\n\n    Returns:\n        Any: Optimizer state (or None) to be threaded through ``train_step``.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.train_step","title":"train_step  <code>abstractmethod</code>","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform a single training step on a batch and return (loss, new_state).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to be trained.</p> required <code>Xb</code> <code>ndarray</code> <p>A batch of inputs.</p> required <code>yb</code> <code>ndarray</code> <p>A batch of targets.</p> required <code>state</code> <p>Optimizer state produced by <code>init_state</code>.</p> required <p>Returns:</p> Type Description <p>tuple[float, Any]: The batch loss and the updated optimizer state.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):  # pragma: no cover - abstract\n    \"\"\"Perform a single training step on a batch and return (loss, new_state).\n\n    Parameters:\n        model: The model to be trained.\n        Xb (np.ndarray): A batch of inputs.\n        yb (np.ndarray): A batch of targets.\n        state: Optimizer state produced by ``init_state``.\n\n    Returns:\n        tuple[float, Any]: The batch loss and the updated optimizer state.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer","title":"anfis_toolbox.optim.hybrid.HybridTrainer  <code>dataclass</code>","text":"<pre><code>HybridTrainer(\n    learning_rate: float = 0.01,\n    epochs: int = 100,\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Original Jang (1993) hybrid training: LSM for consequents + GD for antecedents.</p> Notes <p>This trainer assumes a single-output regression head. It is not compatible with :class:<code>~anfis_toolbox.model.TSKANFISClassifier</code> or the high-level :class:<code>~anfis_toolbox.classifier.ANFISClassifier</code> facade.</p>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Compute the hybrid MSE loss on prepared data without side effects.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute the hybrid MSE loss on prepared data without side effects.\"\"\"\n    X_arr, y_arr = self._prepare_data(X, y)\n    membership_outputs = model.membership_layer.forward(X_arr)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n    preds = model.consequent_layer.forward(X_arr, normalized_weights)\n    return float(mse_loss(y_arr, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Hybrid trainer doesn't maintain optimizer state; returns None.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Hybrid trainer doesn't maintain optimizer state; returns None.\"\"\"\n    return None\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one hybrid step on a batch and return (loss, state).</p> <p>Equivalent to one iteration of the hybrid algorithm on the given batch.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one hybrid step on a batch and return (loss, state).\n\n    Equivalent to one iteration of the hybrid algorithm on the given batch.\n    \"\"\"\n    Xb, yb = self._prepare_data(Xb, yb)\n    # Forward to get normalized weights\n    membership_outputs = model.membership_layer.forward(Xb)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n\n    # Build LSM system for batch\n    ones_col = np.ones((Xb.shape[0], 1), dtype=float)\n    x_bar = np.concatenate([Xb, ones_col], axis=1)\n    A_blocks = [normalized_weights[:, j : j + 1] * x_bar for j in range(model.n_rules)]\n    A = np.concatenate(A_blocks, axis=1)\n    try:\n        regularization = 1e-6 * np.eye(A.shape[1])\n        ATA_reg = A.T @ A + regularization\n        theta = np.linalg.solve(ATA_reg, A.T @ yb.flatten())\n    except np.linalg.LinAlgError:\n        logging.getLogger(__name__).warning(\"Matrix singular in LSM, using pseudo-inverse\")\n        theta = np.linalg.pinv(A) @ yb.flatten()\n    model.consequent_layer.parameters = theta.reshape(model.n_rules, model.n_inputs + 1)\n\n    # Loss and backward for antecedents only\n    y_pred = model.consequent_layer.forward(Xb, normalized_weights)\n    loss = mse_loss(yb, y_pred)\n    dL_dy = mse_grad(yb, y_pred)\n    dL_dnorm_w, _ = model.consequent_layer.backward(dL_dy)\n    dL_dw = model.normalization_layer.backward(dL_dnorm_w)\n    gradients = model.rule_layer.backward(dL_dw)\n    model.membership_layer.backward(gradients)\n    model._apply_membership_gradients(self.learning_rate)\n    return float(loss), state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer","title":"anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer  <code>dataclass</code>","text":"<pre><code>HybridAdamTrainer(\n    learning_rate: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Hybrid training: LSM for consequents + Adam for antecedents.</p> Notes <p>This variant also targets the regression ANFIS. It is not compatible with the classification head (:class:<code>~anfis_toolbox.model.TSKANFISClassifier</code>) or :class:<code>~anfis_toolbox.classifier.ANFISClassifier</code>.</p>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Evaluate mean squared error on provided data without updates.</p> Source code in <code>anfis_toolbox/optim/hybrid_adam.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Evaluate mean squared error on provided data without updates.\"\"\"\n    X_arr, y_arr = self._prepare_data(X, y)\n    membership_outputs = model.membership_layer.forward(X_arr)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n    preds = model.consequent_layer.forward(X_arr, normalized_weights)\n    return float(mse_loss(y_arr, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize Adam moment tensors for membership parameters.</p> Source code in <code>anfis_toolbox/optim/hybrid_adam.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize Adam moment tensors for membership parameters.\"\"\"\n    params = model.get_parameters()\n    zero_struct = _zeros_like_structure(params)[\"membership\"]\n    return {\"m\": deepcopy(zero_struct), \"v\": deepcopy(zero_struct), \"t\": 0}\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Execute one hybrid iteration combining LSM and Adam updates.</p> Source code in <code>anfis_toolbox/optim/hybrid_adam.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Execute one hybrid iteration combining LSM and Adam updates.\"\"\"\n    model.reset_gradients()\n    Xb, yb = self._prepare_data(Xb, yb)\n    membership_outputs = model.membership_layer.forward(Xb)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n    ones_col = np.ones((Xb.shape[0], 1), dtype=float)\n    x_bar = np.concatenate([Xb, ones_col], axis=1)\n    A_blocks = [normalized_weights[:, j : j + 1] * x_bar for j in range(model.n_rules)]\n    A = np.concatenate(A_blocks, axis=1)\n    try:\n        regularization = 1e-6 * np.eye(A.shape[1])\n        ATA_reg = A.T @ A + regularization\n        theta = np.linalg.solve(ATA_reg, A.T @ yb.flatten())\n    except np.linalg.LinAlgError:\n        logging.getLogger(__name__).warning(\"Matrix singular in LSM, using pseudo-inverse\")\n        theta = np.linalg.pinv(A) @ yb.flatten()\n    model.consequent_layer.parameters = theta.reshape(model.n_rules, model.n_inputs + 1)\n\n    # Adam for antecedents\n    y_pred = model.consequent_layer.forward(Xb, normalized_weights)\n    loss = mse_loss(yb, y_pred)\n    dL_dy = mse_grad(yb, y_pred)\n    dL_dnorm_w, _ = model.consequent_layer.backward(dL_dy)\n    dL_dw = model.normalization_layer.backward(dL_dnorm_w)\n    gradients = model.rule_layer.backward(dL_dw)\n    grad_struct = model.membership_layer.backward(gradients)\n    self._apply_adam_update(model, grad_struct, state)\n    return float(loss), state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer","title":"anfis_toolbox.optim.sgd.SGDTrainer  <code>dataclass</code>","text":"<pre><code>SGDTrainer(\n    learning_rate: float = 0.01,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Stochastic gradient descent trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Step size for gradient descent.</p> <code>0.01</code> <code>epochs</code> <code>int</code> <p>Number of passes over the data.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>Mini-batch size; if None uses full batch.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data each epoch.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress (delegated to model logging settings).</p> <code>False</code> Notes <p>Uses the configurable loss provided via <code>loss</code> (defaults to mean squared error). The selected loss is responsible for adapting target shapes via <code>prepare_targets</code>. When used with <code>ANFISClassifier</code> and <code>loss=\"cross_entropy\"</code> it trains on logits with the appropriate softmax gradient.</p>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Return the loss for <code>(X, y)</code> without mutating <code>model</code>.</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Return the loss for ``(X, y)`` without mutating ``model``.\"\"\"\n    preds = model.forward(X)\n    return float(self._loss_fn.loss(y, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>SGD has no persistent optimizer state; returns None.</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"SGD has no persistent optimizer state; returns None.\"\"\"\n    return None\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one SGD step on a batch and return (loss, state).</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one SGD step on a batch and return (loss, state).\"\"\"\n    loss = self._compute_loss_backward_and_update(model, Xb, yb)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer","title":"anfis_toolbox.optim.adam.AdamTrainer  <code>dataclass</code>","text":"<pre><code>AdamTrainer(\n    learning_rate: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Adam optimizer-based trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Base step size (alpha).</p> <code>0.001</code> <code>beta1</code> <code>float</code> <p>Exponential decay rate for the first moment estimates.</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>Exponential decay rate for the second moment estimates.</p> <code>0.999</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <code>epochs</code> <code>int</code> <p>Number of passes over the dataset.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>If None, use full-batch; otherwise mini-batches of this size.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at each epoch when using mini-batches.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>False</code> Notes <p>Supports configurable losses via the <code>loss</code> parameter. Defaults to mean squared error for regression, but can minimize other differentiable objectives such as categorical cross-entropy when used with <code>ANFISClassifier</code>.</p>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Evaluate the configured loss on <code>(X, y)</code> without updating parameters.</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Evaluate the configured loss on ``(X, y)`` without updating parameters.\"\"\"\n    preds = model.forward(X)\n    return float(self._loss_fn.loss(y, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize Adam's first and second moments and time step.</p> <p>Returns a dict with keys: params, m, v, t.</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize Adam's first and second moments and time step.\n\n    Returns a dict with keys: params, m, v, t.\n    \"\"\"\n    params = model.get_parameters()\n    return {\n        \"params\": params,\n        \"m\": _zeros_like_structure(params),\n        \"v\": _zeros_like_structure(params),\n        \"t\": 0,\n    }\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>One Adam step on a batch; returns (loss, updated_state).</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"One Adam step on a batch; returns (loss, updated_state).\"\"\"\n    loss, grads = self._compute_loss_and_grads(model, Xb, yb)\n    t_new = self._apply_adam_step(model, state[\"params\"], grads, state[\"m\"], state[\"v\"], state[\"t\"])\n    state[\"t\"] = t_new\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer","title":"anfis_toolbox.optim.rmsprop.RMSPropTrainer  <code>dataclass</code>","text":"<pre><code>RMSPropTrainer(\n    learning_rate: float = 0.001,\n    rho: float = 0.9,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>RMSProp optimizer-based trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Base step size (alpha).</p> <code>0.001</code> <code>rho</code> <code>float</code> <p>Exponential decay rate for the squared gradient moving average.</p> <code>0.9</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <code>epochs</code> <code>int</code> <p>Number of passes over the dataset.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>If None, use full-batch; otherwise mini-batches of this size.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at each epoch when using mini-batches.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>False</code> Notes <p>Supports configurable losses via the <code>loss</code> parameter. Defaults to mean squared error for regression tasks but can be switched to other differentiable objectives such as categorical cross-entropy when training <code>ANFISClassifier</code> models.</p>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Return the current loss value for <code>(X, y)</code> without modifying state.</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Return the current loss value for ``(X, y)`` without modifying state.\"\"\"\n    preds = model.forward(X)\n    return float(self._loss_fn.loss(y, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize RMSProp caches for consequents and membership scalars.</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize RMSProp caches for consequents and membership scalars.\"\"\"\n    params = model.get_parameters()\n    return {\"params\": params, \"cache\": _zeros_like_structure(params)}\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>One RMSProp step on a batch; returns (loss, updated_state).</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"One RMSProp step on a batch; returns (loss, updated_state).\"\"\"\n    loss, grads = self._compute_loss_and_grads(model, Xb, yb)\n    self._apply_rmsprop_step(model, state[\"params\"], state[\"cache\"], grads)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer","title":"anfis_toolbox.optim.pso.PSOTrainer  <code>dataclass</code>","text":"<pre><code>PSOTrainer(\n    swarm_size: int = 20,\n    inertia: float = 0.7,\n    cognitive: float = 1.5,\n    social: float = 1.5,\n    epochs: int = 100,\n    init_sigma: float = 0.1,\n    clamp_velocity: None | tuple[float, float] = None,\n    clamp_position: None | tuple[float, float] = None,\n    random_state: None | int = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Particle Swarm Optimization (PSO) trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>swarm_size</code> <code>int</code> <p>Number of particles.</p> <code>20</code> <code>inertia</code> <code>float</code> <p>Inertia weight (w).</p> <code>0.7</code> <code>cognitive</code> <code>float</code> <p>Cognitive coefficient (c1).</p> <code>1.5</code> <code>social</code> <code>float</code> <p>Social coefficient (c2).</p> <code>1.5</code> <code>epochs</code> <code>int</code> <p>Number of iterations of the swarm update.</p> <code>100</code> <code>init_sigma</code> <code>float</code> <p>Std-dev for initializing particle positions around current params.</p> <code>0.1</code> <code>clamp_velocity</code> <code>None | tuple[float, float]</code> <p>Optional (min, max) to clip velocities element-wise.</p> <code>None</code> <code>clamp_position</code> <code>None | tuple[float, float]</code> <p>Optional (min, max) to clip positions element-wise.</p> <code>None</code> <code>random_state</code> <code>None | int</code> <p>Seed for RNG to ensure determinism.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>False</code> Notes <p>Optimizes the loss specified by <code>loss</code> (defaulting to mean squared error) by searching directly in parameter space without gradients. With <code>ANFISClassifier</code> you can set <code>loss=\"cross_entropy\"</code> to optimize categorical cross-entropy on logits.</p>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Evaluate the swarm's current parameters on <code>(X, y)</code> without mutation.</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Evaluate the swarm's current parameters on ``(X, y)`` without mutation.\"\"\"\n    return self._evaluate_loss(model, X, y)\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize PSO swarm state and return as a dict.</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize PSO swarm state and return as a dict.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    rng = np.random.default_rng(self.random_state)\n    base_params = model.get_parameters()\n    theta0, meta = _flatten_params(base_params)\n    D = theta0.size\n    positions = theta0[None, :] + self.init_sigma * rng.normal(size=(self.swarm_size, D))\n    velocities = np.zeros((self.swarm_size, D), dtype=float)\n    # Initialize personal/global bests on provided data\n    personal_best_pos = positions.copy()\n    personal_best_val = np.empty(self.swarm_size, dtype=float)\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, base_params)\n        with self._temporary_parameters(model, params_i):\n            personal_best_val[i] = self._evaluate_loss(model, X, y)\n    g_idx = int(np.argmin(personal_best_val))\n    global_best_pos = personal_best_pos[g_idx].copy()\n    global_best_val = float(personal_best_val[g_idx])\n    return {\n        \"meta\": meta,\n        \"template\": base_params,\n        \"positions\": positions,\n        \"velocities\": velocities,\n        \"pbest_pos\": personal_best_pos,\n        \"pbest_val\": personal_best_val,\n        \"gbest_pos\": global_best_pos,\n        \"gbest_val\": global_best_val,\n        \"rng\": rng,\n    }\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one PSO iteration over the swarm on a batch and return (best_loss, state).</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one PSO iteration over the swarm on a batch and return (best_loss, state).\"\"\"\n    positions = state[\"positions\"]\n    velocities = state[\"velocities\"]\n    personal_best_pos = state[\"pbest_pos\"]\n    personal_best_val = state[\"pbest_val\"]\n    global_best_pos = state[\"gbest_pos\"]\n    global_best_val = state[\"gbest_val\"]\n    meta = state[\"meta\"]\n    template = state[\"template\"]\n    rng = state[\"rng\"]\n\n    D = positions.shape[1]\n    r1 = rng.random(size=(self.swarm_size, D))\n    r2 = rng.random(size=(self.swarm_size, D))\n    cognitive_term = self.cognitive * r1 * (personal_best_pos - positions)\n    social_term = self.social * r2 * (global_best_pos[None, :] - positions)\n    velocities = self.inertia * velocities + cognitive_term + social_term\n    if self.clamp_velocity is not None:\n        vmin, vmax = self.clamp_velocity\n        velocities = np.clip(velocities, vmin, vmax)\n    positions = positions + velocities\n    if self.clamp_position is not None:\n        pmin, pmax = self.clamp_position\n        positions = np.clip(positions, pmin, pmax)\n\n    # Evaluate swarm and update bests\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, template)\n        with self._temporary_parameters(model, params_i):\n            val = self._evaluate_loss(model, Xb, yb)\n        if val &lt; personal_best_val[i]:\n            personal_best_val[i] = val\n            personal_best_pos[i] = positions[i].copy()\n            if val &lt; global_best_val:\n                global_best_val = float(val)\n                global_best_pos = positions[i].copy()\n\n    # Update state and set model to global best\n    state.update(\n        {\n            \"positions\": positions,\n            \"velocities\": velocities,\n            \"pbest_pos\": personal_best_pos,\n            \"pbest_val\": personal_best_val,\n            \"gbest_pos\": global_best_pos,\n            \"gbest_val\": global_best_val,\n        }\n    )\n    best_params = _unflatten_params(global_best_pos, meta, template)\n    model.set_parameters(best_params)\n    return float(global_best_val), state\n</code></pre>"},{"location":"api/regressor/","title":"Regressor API","text":"<p><code>ANFISRegressor</code> provides the high-level fa\u00e7ade for regression workflows, combining membership-function generation, rule construction, and optimization with familiar estimator semantics.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor","title":"anfis_toolbox.regressor.ANFISRegressor","text":"<pre><code>ANFISRegressor(\n    *,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str\n    | BaseTrainer\n    | type[BaseTrainer]\n    | None = \"hybrid\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimatorLike</code>, <code>FittedMixin</code>, <code>RegressorMixinLike</code></p> <p>Adaptive Neuro-Fuzzy regressor with a scikit-learn style API.</p> <p>The estimator manages membership-function synthesis, rule construction, and trainer selection so you can focus on calling :meth:<code>fit</code>, :meth:<code>predict</code>, and :meth:<code>evaluate</code> with familiar NumPy-like data structures.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor--examples","title":"Examples:","text":"<p>reg = ANFISRegressor() reg.fit(X, y) ANFISRegressor(...) reg.predict(X[:1]) array([...])</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor--parameters","title":"Parameters","text":"<p>n_mfs : int, default=3     Default number of membership functions per input. mf_type : str, default=\"gaussian\"     Default membership function family used for automatically generated     membership functions. Supported values include <code>\"gaussian\"</code>,     <code>\"triangular\"</code>, <code>\"bell\"</code>, and other names exposed by the     membership catalogue. init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Strategy used when inferring membership functions from data. <code>None</code>     falls back to <code>\"grid\"</code>. overlap : float, default=0.5     Controls overlap when generating membership functions automatically. margin : float, default=0.10     Margin added around observed data ranges during automatic     initialization. inputs_config : Mapping, optional     Per-input overrides. Keys may be feature names (when <code>X</code> is a     :class:<code>pandas.DataFrame</code>) or integer indices. Values may be:</p> <pre><code>* ``dict`` with keys among ``{\"n_mfs\", \"mf_type\", \"init\", \"overlap\",\n  \"margin\", \"range\", \"membership_functions\", \"mfs\"}``.\n* A list/tuple of :class:`MembershipFunction` instances for full control.\n* ``None`` for defaults.\n</code></pre> <p>random_state : int, optional     Random state forwarded to FCM-based initialization and any stochastic     optimizers. optimizer : str, BaseTrainer, type[BaseTrainer], or None, default=\"hybrid\"     Trainer identifier or instance used for fitting. Strings map to entries     in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to \"hybrid\". optimizer_params : Mapping, optional     Additional keyword arguments forwarded to the trainer constructor. learning_rate, epochs, batch_size, shuffle, verbose : optional scalars     Common trainer hyper-parameters provided for convenience. When the     selected trainer supports the parameter it is included automatically. loss : str or LossFunction, optional     Custom loss forwarded to trainers that expose a <code>loss</code> parameter. rules : Sequence[Sequence[int]] | None, optional     Explicit fuzzy rule indices to use instead of the full Cartesian product. Each     rule lists the membership-function index per input. <code>None</code> keeps the default     exhaustive rule set.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor--parameters","title":"Parameters","text":"<p>n_mfs : int, default=3     Default number of membership functions allocated to each input when     they are inferred from data. mf_type : str, default=\"gaussian\"     Membership function family used for automatically generated     membership functions. Supported names mirror the ones exported in     :mod:<code>anfis_toolbox.membership</code> (e.g. <code>\"gaussian\"</code>,     <code>\"triangular\"</code>, <code>\"bell\"</code>). init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Initialization strategy employed when synthesizing membership     functions from the training data. <code>None</code> falls back to     <code>\"grid\"</code>. overlap : float, default=0.5     Desired overlap between neighbouring membership functions during     automatic construction. margin : float, default=0.10     Extra range added around the observed feature minima/maxima when     performing grid initialization. inputs_config : Mapping, optional     Per-feature overrides for membership configuration. Keys may be     feature names (e.g. when <code>X</code> is a :class:<code>pandas.DataFrame</code>),     integer indices, or <code>\"x{i}\"</code> aliases. Values accept dictionaries     with membership keywords (e.g. <code>\"n_mfs\"</code>, <code>\"mf_type\"</code>,     <code>\"init\"</code>), explicit membership function lists, or scalars for     simple overrides. <code>None</code> entries keep defaults. random_state : int, optional     Seed propagated to stochastic components such as FCM-based     initialization and optimizers that rely on randomness. optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"hybrid\"     Trainer identifier or instance used for fitting. String aliases are     looked up in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to     <code>\"hybrid\"</code>. optimizer_params : Mapping, optional     Extra keyword arguments forwarded to the trainer constructor when a     string identifier or class is supplied. learning_rate, epochs, batch_size, shuffle, verbose : optional     Convenience hyper-parameters that are injected into the selected     trainer when supported. <code>shuffle</code> accepts <code>False</code> to disable     randomisation. loss : str | LossFunction, optional     Custom loss forwarded to trainers exposing a <code>loss</code> parameter.     <code>None</code> keeps the trainer default (typically mean squared error). rules : Sequence[Sequence[int]] | None, optional     Optional explicit fuzzy rule definitions. Each rule lists the     membership index for every input. <code>None</code> uses the full Cartesian     product of configured membership functions.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.10,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str | BaseTrainer | type[BaseTrainer] | None = \"hybrid\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n) -&gt; None:\n    \"\"\"Construct an :class:`ANFISRegressor` with the provided hyper-parameters.\n\n    Parameters\n    ----------\n    n_mfs : int, default=3\n        Default number of membership functions allocated to each input when\n        they are inferred from data.\n    mf_type : str, default=\"gaussian\"\n        Membership function family used for automatically generated\n        membership functions. Supported names mirror the ones exported in\n        :mod:`anfis_toolbox.membership` (e.g. ``\"gaussian\"``,\n        ``\"triangular\"``, ``\"bell\"``).\n    init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"\n        Initialization strategy employed when synthesizing membership\n        functions from the training data. ``None`` falls back to\n        ``\"grid\"``.\n    overlap : float, default=0.5\n        Desired overlap between neighbouring membership functions during\n        automatic construction.\n    margin : float, default=0.10\n        Extra range added around the observed feature minima/maxima when\n        performing grid initialization.\n    inputs_config : Mapping, optional\n        Per-feature overrides for membership configuration. Keys may be\n        feature names (e.g. when ``X`` is a :class:`pandas.DataFrame`),\n        integer indices, or ``\"x{i}\"`` aliases. Values accept dictionaries\n        with membership keywords (e.g. ``\"n_mfs\"``, ``\"mf_type\"``,\n        ``\"init\"``), explicit membership function lists, or scalars for\n        simple overrides. ``None`` entries keep defaults.\n    random_state : int, optional\n        Seed propagated to stochastic components such as FCM-based\n        initialization and optimizers that rely on randomness.\n    optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"hybrid\"\n        Trainer identifier or instance used for fitting. String aliases are\n        looked up in :data:`TRAINER_REGISTRY`. ``None`` defaults to\n        ``\"hybrid\"``.\n    optimizer_params : Mapping, optional\n        Extra keyword arguments forwarded to the trainer constructor when a\n        string identifier or class is supplied.\n    learning_rate, epochs, batch_size, shuffle, verbose : optional\n        Convenience hyper-parameters that are injected into the selected\n        trainer when supported. ``shuffle`` accepts ``False`` to disable\n        randomisation.\n    loss : str | LossFunction, optional\n        Custom loss forwarded to trainers exposing a ``loss`` parameter.\n        ``None`` keeps the trainer default (typically mean squared error).\n    rules : Sequence[Sequence[int]] | None, optional\n        Optional explicit fuzzy rule definitions. Each rule lists the\n        membership index for every input. ``None`` uses the full Cartesian\n        product of configured membership functions.\n    \"\"\"\n    self.n_mfs = int(n_mfs)\n    self.mf_type = str(mf_type)\n    self.init = None if init is None else str(init)\n    self.overlap = float(overlap)\n    self.margin = float(margin)\n    self.inputs_config = dict(inputs_config) if inputs_config is not None else None\n    self.random_state = random_state\n    self.optimizer = optimizer\n    self.optimizer_params = dict(optimizer_params) if optimizer_params is not None else None\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    self.verbose = verbose\n    self.loss = loss\n    self.rules = None if rules is None else tuple(tuple(int(idx) for idx in rule) for rule in rules)\n\n    # Fitted attributes (initialised later)\n    self.model_: LowLevelANFIS | None = None\n    self.optimizer_: BaseTrainer | None = None\n    self.feature_names_in_: list[str] | None = None\n    self.n_features_in_: int | None = None\n    self.training_history_: TrainingHistory | None = None\n    self.input_specs_: list[dict[str, Any]] | None = None\n    self.rules_: list[tuple[int, ...]] | None = None\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return a formatted representation summarising configuration and fitted artefacts.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a formatted representation summarising configuration and fitted artefacts.\"\"\"\n    return format_estimator_repr(\n        type(self).__name__,\n        self._repr_config_pairs(),\n        self._repr_children_entries(),\n    )\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    X,\n    y,\n    *,\n    return_dict: bool = True,\n    print_results: bool = True,\n)\n</code></pre> <p>Evaluate predictive performance on a dataset.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate--parameters","title":"Parameters","text":"<p>X : array-like     Evaluation inputs with shape <code>(n_samples, n_features)</code>. y : array-like     Ground-truth targets aligned with <code>X</code>. return_dict : bool, default=True     When <code>True</code>, return the computed metric dictionary. When     <code>False</code>, only perform side effects (such as printing) and return     <code>None</code>. print_results : bool, default=True     Log a human-readable summary to stdout. Set to <code>False</code> to     suppress printing.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate--returns","title":"Returns:","text":"<p>dict[str, float] | None     Regression metrics including mean squared error, root mean squared     error, mean absolute error, and :math:<code>R^2</code> when <code>return_dict</code> is     <code>True</code>; otherwise <code>None</code>.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate--raises","title":"Raises:","text":"<p>RuntimeError     If called before <code>fit</code>. ValueError     When <code>X</code> and <code>y</code> disagree on the sample count.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def evaluate(self, X, y, *, return_dict: bool = True, print_results: bool = True):\n    \"\"\"Evaluate predictive performance on a dataset.\n\n    Parameters\n    ----------\n    X : array-like\n        Evaluation inputs with shape ``(n_samples, n_features)``.\n    y : array-like\n        Ground-truth targets aligned with ``X``.\n    return_dict : bool, default=True\n        When ``True``, return the computed metric dictionary. When\n        ``False``, only perform side effects (such as printing) and return\n        ``None``.\n    print_results : bool, default=True\n        Log a human-readable summary to stdout. Set to ``False`` to\n        suppress printing.\n\n    Returns:\n    -------\n    dict[str, float] | None\n        Regression metrics including mean squared error, root mean squared\n        error, mean absolute error, and :math:`R^2` when ``return_dict`` is\n        ``True``; otherwise ``None``.\n\n    Raises:\n    ------\n    RuntimeError\n        If called before ``fit``.\n    ValueError\n        When ``X`` and ``y`` disagree on the sample count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr, _ = _ensure_2d_array(X)\n    y_vec = _ensure_vector(y)\n    preds = self.predict(X_arr)\n    metrics = ANFISMetrics.regression_metrics(y_vec, preds)\n    if print_results:\n\n        def _is_effectively_nan(value: Any) -&gt; bool:\n            if value is None:\n                return True\n            if isinstance(value, (float, np.floating)):\n                return bool(np.isnan(value))\n            if isinstance(value, (int, np.integer)):\n                return False\n            if isinstance(value, np.ndarray):\n                if value.size == 0:\n                    return False\n                if np.issubdtype(value.dtype, np.number):\n                    return bool(np.isnan(value.astype(float)).all())\n                return False\n            return False\n\n        print(\"ANFISRegressor evaluation:\")  # noqa: T201\n        for key, value in metrics.items():\n            if _is_effectively_nan(value):\n                continue\n            if isinstance(value, (float, np.floating)):\n                display_value = f\"{float(value):.6f}\"\n                print(f\"  {key}: {display_value}\")  # noqa: T201\n            elif isinstance(value, (int, np.integer)):\n                print(f\"  {key}: {int(value)}\")  # noqa: T201\n            elif isinstance(value, np.ndarray):\n                array_repr = np.array2string(value, precision=6, suppress_small=True)\n                if \"\\n\" in array_repr:\n                    indented = \"\\n    \".join(array_repr.splitlines())\n                    print(f\"  {key}:\\n    {indented}\")  # noqa: T201\n                else:\n                    print(f\"  {key}: {array_repr}\")  # noqa: T201\n            else:\n                print(f\"  {key}: {value}\")  # noqa: T201\n    return metrics if return_dict else None\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    *,\n    validation_data: tuple[ndarray, ndarray] | None = None,\n    validation_frequency: int = 1,\n    verbose: bool | None = None,\n    **fit_params: Any,\n)\n</code></pre> <p>Fit the ANFIS regressor on labelled data.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit--parameters","title":"Parameters","text":"<p>X : array-like     Training inputs with shape <code>(n_samples, n_features)</code>. y : array-like     Target values aligned with <code>X</code>. One-dimensional vectors are     accepted and reshaped internally. validation_data : tuple[np.ndarray, np.ndarray], optional     Optional validation split supplied to the underlying trainer. Both     arrays must already be numeric and share the same row count. validation_frequency : int, default=1     Frequency (in epochs) at which validation loss is evaluated when     <code>validation_data</code> is provided. verbose : bool, optional     Override the estimator's <code>verbose</code> flag for this fit call. When     supplied, the value is stored on the estimator and forwarded to the     trainer configuration. **fit_params : Any     Arbitrary keyword arguments forwarded to the trainer <code>fit</code>     method.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit--returns","title":"Returns:","text":"<p>ANFISRegressor     Reference to <code>self</code> for fluent-style chaining.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit--raises","title":"Raises:","text":"<p>ValueError     If <code>X</code> and <code>y</code> contain a different number of samples. ValueError     If validation frequency is less than one. TypeError     If the configured trainer returns an object that is not a     <code>dict</code>-like training history.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    *,\n    validation_data: tuple[np.ndarray, np.ndarray] | None = None,\n    validation_frequency: int = 1,\n    verbose: bool | None = None,\n    **fit_params: Any,\n):\n    \"\"\"Fit the ANFIS regressor on labelled data.\n\n    Parameters\n    ----------\n    X : array-like\n        Training inputs with shape ``(n_samples, n_features)``.\n    y : array-like\n        Target values aligned with ``X``. One-dimensional vectors are\n        accepted and reshaped internally.\n    validation_data : tuple[np.ndarray, np.ndarray], optional\n        Optional validation split supplied to the underlying trainer. Both\n        arrays must already be numeric and share the same row count.\n    validation_frequency : int, default=1\n        Frequency (in epochs) at which validation loss is evaluated when\n        ``validation_data`` is provided.\n    verbose : bool, optional\n        Override the estimator's ``verbose`` flag for this fit call. When\n        supplied, the value is stored on the estimator and forwarded to the\n        trainer configuration.\n    **fit_params : Any\n        Arbitrary keyword arguments forwarded to the trainer ``fit``\n        method.\n\n    Returns:\n    -------\n    ANFISRegressor\n        Reference to ``self`` for fluent-style chaining.\n\n    Raises:\n    ------\n    ValueError\n        If ``X`` and ``y`` contain a different number of samples.\n    ValueError\n        If validation frequency is less than one.\n    TypeError\n        If the configured trainer returns an object that is not a\n        ``dict``-like training history.\n    \"\"\"\n    X_arr, feature_names = _ensure_2d_array(X)\n    y_vec = _ensure_vector(y)\n    if X_arr.shape[0] != y_vec.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples.\")\n\n    self.feature_names_in_ = feature_names\n    self.n_features_in_ = X_arr.shape[1]\n    self.input_specs_ = self._resolve_input_specs(feature_names)\n\n    if verbose is not None:\n        self.verbose = bool(verbose)\n\n    _ensure_training_logging(self.verbose)\n    model = self._build_model(X_arr, feature_names)\n    self.model_ = model\n    trainer = self._instantiate_trainer()\n    self.optimizer_ = trainer\n    trainer_kwargs: dict[str, Any] = dict(fit_params)\n    if validation_data is not None:\n        trainer_kwargs.setdefault(\"validation_data\", validation_data)\n    if validation_data is not None or validation_frequency != 1:\n        trainer_kwargs.setdefault(\"validation_frequency\", validation_frequency)\n\n    history = trainer.fit(model, X_arr, y_vec, **trainer_kwargs)\n    if not isinstance(history, dict):\n        raise TypeError(\"Trainer.fit must return a TrainingHistory dictionary\")\n    self.training_history_ = history\n    self.rules_ = model.rules\n\n    self._mark_fitted()\n    return self\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.get_rules","title":"get_rules","text":"<pre><code>get_rules() -&gt; tuple[tuple[int, ...], ...]\n</code></pre> <p>Return the fuzzy rule index combinations used by the fitted model.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.get_rules--returns","title":"Returns:","text":"<p>tuple[tuple[int, ...], ...]     Immutable tuple containing one tuple per fuzzy rule, where each     inner tuple lists the membership index chosen for each input.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.get_rules--raises","title":"Raises:","text":"<p>RuntimeError     If invoked before the estimator is fitted.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def get_rules(self) -&gt; tuple[tuple[int, ...], ...]:\n    \"\"\"Return the fuzzy rule index combinations used by the fitted model.\n\n    Returns:\n    -------\n    tuple[tuple[int, ...], ...]\n        Immutable tuple containing one tuple per fuzzy rule, where each\n        inner tuple lists the membership index chosen for each input.\n\n    Raises:\n    ------\n    RuntimeError\n        If invoked before the estimator is fitted.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"rules_\"])\n    if not self.rules_:\n        return ()\n    return tuple(tuple(rule) for rule in self.rules_)\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: str | Path) -&gt; ANFISRegressor\n</code></pre> <p>Load a pickled estimator from <code>filepath</code> and validate its type.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str | Path) -&gt; ANFISRegressor:\n    \"\"\"Load a pickled estimator from ``filepath`` and validate its type.\"\"\"\n    path = Path(filepath)\n    with path.open(\"rb\") as stream:\n        estimator = pickle.load(stream)\n    if not isinstance(estimator, cls):\n        raise TypeError(f\"Expected pickled {cls.__name__} instance, got {type(estimator).__name__}.\")\n    return estimator\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict regression targets for the provided samples.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like     Samples to evaluate. Accepts one-dimensional arrays (interpreted as     a single sample) or matrices with shape <code>(n_samples, n_features)</code>.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict--returns","title":"Returns:","text":"<p>np.ndarray     Vector of predictions with shape <code>(n_samples,)</code>.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict--raises","title":"Raises:","text":"<p>RuntimeError     If the estimator has not been fitted yet. ValueError     When the supplied samples do not match the fitted feature count.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict regression targets for the provided samples.\n\n    Parameters\n    ----------\n    X : array-like\n        Samples to evaluate. Accepts one-dimensional arrays (interpreted as\n        a single sample) or matrices with shape ``(n_samples, n_features)``.\n\n    Returns:\n    -------\n    np.ndarray\n        Vector of predictions with shape ``(n_samples,)``.\n\n    Raises:\n    ------\n    RuntimeError\n        If the estimator has not been fitted yet.\n    ValueError\n        When the supplied samples do not match the fitted feature count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n    else:\n        X_arr, _ = _ensure_2d_array(X)\n\n    if self.n_features_in_ is None:\n        raise RuntimeError(\"Model must be fitted before calling predict.\")\n    if X_arr.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Feature mismatch: expected {self.n_features_in_}, got {X_arr.shape[1]}.\")\n\n    model = self.model_\n    if model is None:\n        raise RuntimeError(\"Model must be fitted before calling predict.\")\n    preds = model.predict(X_arr)\n    return np.asarray(preds, dtype=float).reshape(-1)\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.save","title":"save","text":"<pre><code>save(filepath: str | Path) -&gt; None\n</code></pre> <p>Serialize this estimator (and its fitted state) using <code>pickle</code>.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def save(self, filepath: str | Path) -&gt; None:\n    \"\"\"Serialize this estimator (and its fitted state) using ``pickle``.\"\"\"\n    path = Path(filepath)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"wb\") as stream:\n        pickle.dump(self, stream)\n</code></pre>"},{"location":"estimators/anfis-classifier/","title":"ANFISClassifier","text":"<p><code>ANFISClassifier</code> is the high-level entry point for training Adaptive Neuro-Fuzzy Inference Systems on multi-class tasks. It hides the low-level membership construction, rule synthesis, and trainer wiring behind a familiar scikit-learn style API (<code>fit</code>, <code>predict</code>, <code>predict_proba</code>, <code>evaluate</code>, <code>save</code>, <code>load</code>).</p>"},{"location":"estimators/anfis-classifier/#at-a-glance","title":"At a Glance","text":"<ul> <li>Works with NumPy arrays, array-like objects, or pandas DataFrames.</li> <li>Automatically generates membership functions per input (grid, FCM, or random).</li> <li>Supports custom membership definitions and rule subsets.</li> <li>Provides optimizers tailored for classification: <code>\"adam\"</code>, <code>\"rmsprop\"</code>,     <code>\"sgd\"</code>, <code>\"pso\"</code>.</li> <li>Ships with built-in evaluation (<code>evaluate</code>) and persistence (<code>save</code>, <code>load</code>).</li> </ul>"},{"location":"estimators/anfis-classifier/#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nfrom anfis_toolbox import ANFISClassifier\n\n# Synthetic binary classification data\nrng = np.random.default_rng(42)\nX = rng.normal(size=(240, 2))\ny = (X[:, 0] - 0.75 * X[:, 1] &gt; 0).astype(int)\n\nclf = ANFISClassifier(epochs=50, learning_rate=0.01, verbose=False)\nclf.fit(X, y)\n\nproba = clf.predict_proba([[0.2, -0.4]])\npred = clf.predict([[0.2, -0.4]])\nreport = clf.evaluate(X, y)\n</code></pre>"},{"location":"estimators/anfis-classifier/#core-workflow","title":"Core Workflow","text":"<ol> <li>Configure \u2013 Set global defaults (<code>n_classes</code>, <code>n_mfs</code>, <code>mf_type</code>, <code>optimizer</code>).</li> <li>Fit \u2013 Call <code>fit(X, y)</code> with optional validation data.</li> <li>Predict \u2013 Use <code>predict</code> or <code>predict_proba</code> for inference.</li> <li>Evaluate \u2013 Call <code>evaluate</code> to obtain accuracy, precision/recall/F1, and confusion matrix.</li> <li>Persist \u2013 Store or restore trained estimators via <code>save</code> / <code>load</code>.</li> </ol>"},{"location":"estimators/anfis-classifier/#model-equations","title":"Model Equations","text":"<p>Each fuzzy rule emits a Takagi\u2013Sugeno\u2013Kang consequent for every class:</p> \\[     \\text{Rule}_i:\\;\\text{if } x_1 \\text{ is } A_1^i \\land \\dots \\land x_n \\text{ is } A_n^i \\;\\text{then}\\; y_{i,k} = p_{0,k}^i + \\sum_{j=1}^n p_{j,k}^i x_j. \\] <p>The firing strength of rule \\(i\\) is the product of the memberships \\(w_i = \\prod_{j=1}^n \\mu_{A_j^i}(x_j)\\) and the normalised weights are \\(\\bar{w}_i = w_i / \\sum_{r=1}^R w_r\\). Class logits are the weighted sums \\(z_k = \\sum_{i=1}^R \\bar{w}_i y_{i,k}\\) and probabilities follow from the softmax: \\(p_k = \\exp(z_k) / \\sum_{j=1}^K \\exp(z_j)\\). Training minimises cross-entropy between the predicted probabilities and the target distribution.</p>"},{"location":"estimators/anfis-classifier/#key-parameters","title":"Key Parameters","text":"Parameter Description <code>n_classes</code> Number of classes (optional, inferred on first <code>fit</code>). <code>n_mfs</code> Default membership count per input (int). <code>mf_type</code> Membership family (<code>\"gaussian\"</code>, <code>\"triangular\"</code>, <code>\"bell\"</code>, etc.). <code>init</code> Membership initialization (<code>\"grid\"</code>, <code>\"fcm\"</code>, <code>\"random\"</code>, or <code>None</code>). <code>inputs_config</code> Per-input overrides (dict, list of membership functions, or <code>None</code>). <code>optimizer</code> Trainer identifier, subclass, or instance (<code>\"adam\"</code>, <code>\"sgd\"</code>, <code>\"rmsprop\"</code>, <code>\"pso\"</code>). <code>optimizer_params</code> Extra keyword arguments forwarded to the trainer constructor. <code>learning_rate</code>, <code>epochs</code>, <code>batch_size</code>, <code>shuffle</code>, <code>verbose</code> Convenience overrides passed to compatible trainers. <code>loss</code> Optional custom loss (string key or callable). <code>rules</code> Optional list of rule tuples limiting the rule set."},{"location":"estimators/anfis-classifier/#customizing-membership-functions","title":"Customizing Membership Functions","text":"<p>Use <code>inputs_config</code> to tailor membership families, counts, or ranges on a per-input basis. Keys may be column names (for pandas DataFrames), integer indices, or <code>\"x{i}\"</code> aliases.</p> <pre><code>import numpy as np\nfrom anfis_toolbox import ANFISClassifier\nfrom anfis_toolbox.membership import GaussianMF\n\nrng = np.random.default_rng(7)\nX_multi = rng.normal(size=(300, 2))\ny_multi = np.digitize(X_multi[:, 0] + 0.5 * X_multi[:, 1], bins=[-0.5, 0.5])\n\ninputs_config = {\n    0: {\n        \"mf_type\": \"triangular\",\n        \"n_mfs\": 3,\n        \"overlap\": 0.55,\n    },\n    1: {\n        \"membership_functions\": [\n            GaussianMF(mean=-1.0, sigma=0.4),\n            GaussianMF(mean=0.0, sigma=0.35),\n            GaussianMF(mean=1.2, sigma=0.45),\n        ]\n    },\n}\n\nclf = ANFISClassifier(n_classes=3, inputs_config=inputs_config, epochs=60, learning_rate=0.01)\nclf.fit(X_multi, y_multi)\n</code></pre> <p>Note</p> <p>Keep the number of membership functions consistent across inputs when mixing dictionary overrides and explicit membership lists. The example above configures three functions for each feature.</p> <p>The <code>X_multi</code> and <code>y_multi</code> arrays from the example are reused in the sections below.</p>"},{"location":"estimators/anfis-classifier/#choosing-an-optimizer","title":"Choosing an Optimizer","text":"<p>Pass a string alias, trainer class, or trainer instance:</p> <pre><code>clf = ANFISClassifier(optimizer=\"adam\", epochs=80, learning_rate=0.005)\nclf.fit(X, y)\n\nfrom anfis_toolbox.optim import RMSPropTrainer\n\nclf = ANFISClassifier(optimizer=RMSPropTrainer(learning_rate=0.001, epochs=120))\nclf.fit(X, y)\n</code></pre> <ul> <li><code>\"adam\"</code> (default): Adaptive gradient-based training.</li> <li><code>\"rmsprop\"</code>: Root-mean-square propagation.</li> <li><code>\"sgd\"</code>: Mini-batch stochastic gradient descent.</li> <li><code>\"pso\"</code>: Particle Swarm Optimisation for derivative-free updates.</li> </ul> <p>Hybrid optimisers that rely on least-squares refinements are limited to regression and are rejected by <code>ANFISClassifier</code>.</p>"},{"location":"estimators/anfis-classifier/#restricting-the-rule-base","title":"Restricting the Rule Base","text":"<p>Supply <code>rules</code> to freeze the rule combinations used during training.</p> <pre><code>selected_rules = [(0, 0), (1, 1), (2, 2)]\nclf = ANFISClassifier(n_classes=3, rules=selected_rules, epochs=40, learning_rate=0.01)\nclf.fit(X_multi, y_multi)\nassert tuple(clf.get_rules()) == tuple(selected_rules)\n</code></pre> <p>If <code>rules</code> is omitted, the full Cartesian product of membership indices is used.</p>"},{"location":"estimators/anfis-classifier/#evaluating-performance","title":"Evaluating Performance","text":"<p><code>evaluate</code> reports accuracy, precision/recall/F1 averages, balanced accuracy, and the confusion matrix. Disable printing with <code>print_results=False</code>.</p> <pre><code>metrics = clf.evaluate(X_test, y_test, print_results=False)\nprint(metrics[\"accuracy\"], metrics[\"macro_f1\"])\n\nproba = clf.predict_proba(X_test[:3])\nlabels = clf.predict(X_test[:3])\n</code></pre>"},{"location":"estimators/anfis-classifier/#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code>clf.fit(X, y)\nclf.save(\"artifacts/anfis-classifier.pkl\")\n\nfrom anfis_toolbox import ANFISClassifier\n\nloaded = ANFISClassifier.load(\"artifacts/anfis-classifier.pkl\")\npred = loaded.predict(X[:3])\n</code></pre> <p>The pickled artefact stores fitted membership functions, rule definitions, and training history, enabling reproducible deployments.</p>"},{"location":"estimators/anfis-classifier/#tips-troubleshooting","title":"Tips &amp; Troubleshooting","text":"<ul> <li>Input scale \u2013 Normalize or standardize features for smoother membership learning.</li> <li>Underfitting \u2013 Increase <code>n_mfs</code>, provide richer <code>inputs_config</code>, or allow more epochs.</li> <li>Overfitting \u2013 Reduce rule count, add validation data, or lower <code>epochs</code>.</li> <li>Imbalanced labels \u2013 Use class-balanced datasets or resampling strategies.</li> <li>Verbose logging \u2013 Set <code>verbose=True</code> during fitting to stream trainer progress.</li> </ul>"},{"location":"estimators/anfis-classifier/#further-reading","title":"Further Reading","text":"<ul> <li>API Reference \u2013 Classifier</li> <li>Membership Functions catalog</li> <li>Optimizer reference</li> <li>Jang, J.-S. R. (1993). ANFIS: Adaptive-network-based fuzzy inference system.</li> </ul>"},{"location":"estimators/anfis-regressor/","title":"ANFISRegressor","text":"<p><code>ANFISRegressor</code> is the high-level entry point for training Adaptive Neuro-Fuzzy Inference Systems on regression tasks. It hides the low-level membership construction, rule synthesis, and trainer wiring behind a familiar scikit-learn style API (<code>fit</code>, <code>predict</code>, <code>evaluate</code>, <code>save</code>, <code>load</code>).</p>"},{"location":"estimators/anfis-regressor/#at-a-glance","title":"At a Glance","text":"<ul> <li>Works with NumPy arrays, array-like objects, or pandas DataFrames.</li> <li>Automatically generates membership functions per input (grid, FCM, or random).</li> <li>Supports custom membership definitions and rule subsets.</li> <li>Provides multiple optimizers: <code>\"hybrid\"</code>, <code>\"adam\"</code>, <code>\"sgd\"</code>, <code>\"rmsprop\"</code>,     <code>\"pso\"</code>, <code>\"hybrid_adam\"</code>.</li> <li>Ships with built-in evaluation (<code>evaluate</code>) and persistence (<code>save</code>, <code>load</code>).</li> </ul>"},{"location":"estimators/anfis-regressor/#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nfrom anfis_toolbox import ANFISRegressor\n\n# Synthetic regression data\nrng = np.random.default_rng(0)\nX = rng.uniform(-2, 2, size=(200, 2))\ny = np.sin(X[:, 0]) + 0.5 * X[:, 1]\n\nreg = ANFISRegressor(optimizer=\"adam\", epochs=40, learning_rate=0.01)\nreg.fit(X, y)\n\npred = reg.predict([[0.4, -0.1]])\nreport = reg.evaluate(X, y)\n</code></pre>"},{"location":"estimators/anfis-regressor/#core-workflow","title":"Core Workflow","text":"<ol> <li>Configure \u2013 Set global defaults (<code>n_mfs</code>, <code>mf_type</code>, <code>init</code>, <code>optimizer</code>).</li> <li>Fit \u2013 Call <code>fit(X, y)</code> with optional validation data.</li> <li>Predict \u2013 Use <code>predict</code> for batch or single-sample inference.</li> <li>Evaluate \u2013 Call <code>evaluate</code> to obtain MSE, RMSE, MAE, and R\u00b2 metrics.</li> <li>Persist \u2013 Store or restore trained estimators via <code>save</code> / <code>load</code>.</li> </ol>"},{"location":"estimators/anfis-regressor/#model-equations","title":"Model Equations","text":"<p>Each fuzzy rule generated by <code>ANFISRegressor</code> follows a Takagi\u2013Sugeno\u2013Kang consequent of the form</p> \\[         ext{Rule}_i:\\;\\text{if } x_1 \\text{ is } A_1^i \\land \\dots \\land x_n \\text{ is } A_n^i \\;\\text{then}\\; y_i = p_0^i + \\sum_{j=1}^n p_j^i x_j. \\] <p>The firing strength of rule \\(i\\) is the product of the membership degrees for each input:</p> \\[ w_i = \\prod_{j=1}^n \\mu_{A_j^i}(x_j). \\] <p>After normalising the rule strengths, the overall prediction is</p> \\[ \\hat{y} = \\sum_{i=1}^R \\bar{w}_i \\, y_i, \\qquad \\bar{w}_i = \\frac{w_i}{\\sum_{k=1}^R w_k}. \\] <p>During fitting, the estimator couples gradient-based updates of the membership parameters with least-squares estimation of the consequent coefficients, matching the hybrid learning strategy popularised in the original ANFIS paper.</p>"},{"location":"estimators/anfis-regressor/#key-parameters","title":"Key Parameters","text":"Parameter Description <code>n_mfs</code> Default number of membership functions per input (int). <code>mf_type</code> Membership family (<code>\"gaussian\"</code>, <code>\"triangular\"</code>, <code>\"bell\"</code>, etc.). <code>init</code> Membership initialization (<code>\"grid\"</code>, <code>\"fcm\"</code>, <code>\"random\"</code>, or <code>None</code>). <code>inputs_config</code> Per-input overrides (dict, list of membership functions, or <code>None</code>). <code>optimizer</code> Trainer identifier, subclass, or instance (defaults to <code>\"hybrid\"</code>). <code>optimizer_params</code> Extra keyword arguments passed to the trainer. <code>learning_rate</code>, <code>epochs</code>, <code>batch_size</code>, <code>shuffle</code>, <code>verbose</code> Convenience overrides fed into compatible trainers. <code>loss</code> Optional custom loss (string key or callable). <code>rules</code> Optional list of rule tuples limiting the rule set."},{"location":"estimators/anfis-regressor/#customizing-membership-functions","title":"Customizing Membership Functions","text":"<p>Use <code>inputs_config</code> to tailor membership families, counts, or ranges on a per-input basis. Keys may be column names (for pandas DataFrames), integer indices, or <code>\"x{i}\"</code> aliases.</p> <pre><code>import numpy as np\nfrom anfis_toolbox import ANFISRegressor\nfrom anfis_toolbox.membership import GaussianMF\n\nrng = np.random.default_rng(21)\nX_custom = rng.uniform(-3, 3, size=(320, 2))\ny_custom = np.cos(X_custom[:, 0]) + 0.3 * X_custom[:, 1]\n\ninputs_config = {\n    0: {\n        \"mf_type\": \"triangular\",\n        \"n_mfs\": 4,\n        \"overlap\": 0.6,\n    },\n    1: {\n        \"membership_functions\": [\n            GaussianMF(mean=-1.2, sigma=0.45),\n            GaussianMF(mean=-0.2, sigma=0.35),\n            GaussianMF(mean=0.8, sigma=0.3),\n            GaussianMF(mean=1.7, sigma=0.4),\n        ]\n    },\n}\n\nreg = ANFISRegressor(inputs_config=inputs_config, epochs=60, learning_rate=0.01)\nreg.fit(X_custom, y_custom)\n</code></pre> <p>Note</p> <p>Keep the number of membership functions consistent across inputs when mixing dictionary overrides and explicit membership lists. The example above configures four functions for each feature.</p> <p>The <code>X_custom</code> and <code>y_custom</code> arrays from the example are reused in the sections below.</p>"},{"location":"estimators/anfis-regressor/#choosing-an-optimizer","title":"Choosing an Optimizer","text":"<p>Pass a string alias or a trainer class/instance:</p> <pre><code>reg = ANFISRegressor(optimizer=\"adam\", epochs=80, learning_rate=0.005)\nreg.fit(X, y)\n\nfrom anfis_toolbox.optim import RMSPropTrainer\n\nreg = ANFISRegressor(optimizer=RMSPropTrainer(learning_rate=0.001, epochs=120))\nreg.fit(X, y)\n</code></pre> <ul> <li><code>\"hybrid\"</code> / <code>\"hybrid_adam\"</code>: Combine least-squares consequents with gradient steps.</li> <li><code>\"adam\"</code>, <code>\"rmsprop\"</code>, <code>\"sgd\"</code>: Familiar gradient optimizers.</li> <li><code>\"pso\"</code>: Particle Swarm Optimization for derivative-free training.</li> </ul>"},{"location":"estimators/anfis-regressor/#restricting-the-rule-base","title":"Restricting the Rule Base","text":"<p>Supply <code>rules</code> to freeze the rule combinations explored during training.</p> <pre><code>selected_rules = [(0, 0), (1, 1), (2, 2)]\nreg = ANFISRegressor(rules=selected_rules, epochs=40, learning_rate=0.01)\nreg.fit(X_custom, y_custom)\nassert tuple(reg.get_rules()) == tuple(selected_rules)\n</code></pre> <p>If <code>rules</code> is omitted, the full Cartesian product of membership indices is used.</p>"},{"location":"estimators/anfis-regressor/#evaluating-performance","title":"Evaluating Performance","text":"<p><code>evaluate</code> reports regression metrics and can optionally skip printing.</p> <pre><code>metrics = reg.evaluate(X_test, y_test, print_results=False)\nprint(metrics[\"rmse\"], metrics[\"r2\"])\n</code></pre> <p>Metrics are returned as a dictionary; keys include <code>mse</code>, <code>rmse</code>, <code>mae</code>, and <code>r2</code>.</p>"},{"location":"estimators/anfis-regressor/#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code>reg.fit(X, y)\nreg.save(\"artifacts/anfis-regressor.pkl\")\n\nfrom anfis_toolbox import ANFISRegressor\n\nloaded = ANFISRegressor.load(\"artifacts/anfis-regressor.pkl\")\npred = loaded.predict(X[:3])\n</code></pre> <p>The pickled artifact stores fitted membership functions, rule definitions, and training history, enabling reproducible deployments.</p>"},{"location":"estimators/anfis-regressor/#tips-troubleshooting","title":"Tips &amp; Troubleshooting","text":"<ul> <li>Input scale \u2013 Normalize or standardize features for smoother membership learning.</li> <li>Underfitting \u2013 Increase <code>n_mfs</code>, provide richer <code>inputs_config</code>, or allow more epochs.</li> <li>Overfitting \u2013 Reduce rule count, add validation data, or lower <code>epochs</code>.</li> <li>Stalled training \u2013 Try a different optimizer or adjust <code>learning_rate</code>.</li> <li>Verbose logging \u2013 Set <code>verbose=True</code> during fitting to mirror trainer progress.</li> </ul>"},{"location":"estimators/anfis-regressor/#further-reading","title":"Further Reading","text":"<ul> <li>API Reference \u2013 Regressor</li> <li>Membership Functions catalog</li> <li>Optimizer reference</li> <li>Jang, J.-S. R. (1993). ANFIS: Adaptive-network-based fuzzy inference system.</li> </ul>"},{"location":"examples/classifier_basic/","title":"Classification","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn_samples = 500\ntheta = np.linspace(0, np.pi, n_samples // 2)\nx_out = np.c_[np.cos(theta), np.sin(theta)]\nx_in = np.c_[1 - np.cos(theta), 1 - np.sin(theta) - 0.5]\n\nX = np.vstack([x_out, x_in]) + np.random.normal(0, 0.1, (n_samples, 2))\ny = np.hstack([np.zeros(x_out.shape[0]), np.ones(x_in.shape[0])])\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\", alpha=0.7)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42) n_samples = 500 theta = np.linspace(0, np.pi, n_samples // 2) x_out = np.c_[np.cos(theta), np.sin(theta)] x_in = np.c_[1 - np.cos(theta), 1 - np.sin(theta) - 0.5]  X = np.vstack([x_out, x_in]) + np.random.normal(0, 0.1, (n_samples, 2)) y = np.hstack([np.zeros(x_out.shape[0]), np.ones(x_in.shape[0])])  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\", alpha=0.7) plt.show() In\u00a0[3]: Copied! <pre>train_ratio = 0.8\nidx = np.random.permutation(len(X))\nsplit = int(train_ratio * len(X))\nX_train, X_test = X[idx[:split]], X[idx[split:]]\ny_train, y_test = y[idx[:split]], y[idx[split:]]\n</pre> train_ratio = 0.8 idx = np.random.permutation(len(X)) split = int(train_ratio * len(X)) X_train, X_test = X[idx[:split]], X[idx[split:]] y_train, y_test = y[idx[:split]], y[idx[split:]] In\u00a0[16]: Copied! <pre>from anfis_toolbox import ANFISClassifier\n\nclassifier = ANFISClassifier(optimizer=\"adam\", batch_size=128, epochs=300, learning_rate=0.01, random_state=42)\n</pre> from anfis_toolbox import ANFISClassifier  classifier = ANFISClassifier(optimizer=\"adam\", batch_size=128, epochs=300, learning_rate=0.01, random_state=42) In\u00a0[17]: Copied! <pre>classifier.fit(X_train, y_train)\nresult = classifier.evaluate(X_test, y_test)\n</pre> classifier.fit(X_train, y_train) result = classifier.evaluate(X_test, y_test) <pre>ANFISClassifier evaluation:\n  accuracy: 0.980000\n  balanced_accuracy: 0.979968\n  precision_macro: 0.979968\n  recall_macro: 0.979968\n  f1_macro: 0.979968\n  precision_micro: 0.980000\n  recall_micro: 0.980000\n  f1_micro: 0.980000\n  confusion_matrix:\n    [[47  1]\n     [ 1 51]]\n  classes: [0 1]\n</pre> In\u00a0[19]: Copied! <pre>x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max()\ny_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()\n\nplus = .5\nx_min -= plus\nx_max += plus\ny_min -= plus\ny_max += plus\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 160),\n    np.linspace(y_min, y_max, 160)\n)\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Calcular probabilidades para cada ponto do grid\nprobabilities = classifier.predict_proba(grid_points)[:, 1]\n\nplt.figure(figsize=(8, 6))\n\n# Plot decision boundary as a contourf\nZ = probabilities.reshape(xx.shape)\ncontour = plt.contourf(xx, yy, Z, levels=10, cmap=\"coolwarm\", alpha=0.5, vmin=0, vmax=1)\n\n# Plot training and test points\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", edgecolor=\"k\", label=\"train\", marker=\".\", s=60, alpha=0.5)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"coolwarm\", edgecolor=\"k\", label=\"test\", marker=\"o\", alpha=0.5)\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max() y_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()  plus = .5 x_min -= plus x_max += plus y_min -= plus y_max += plus xx, yy = np.meshgrid(     np.linspace(x_min, x_max, 160),     np.linspace(y_min, y_max, 160) ) grid_points = np.c_[xx.ravel(), yy.ravel()]  # Calcular probabilidades para cada ponto do grid probabilities = classifier.predict_proba(grid_points)[:, 1]  plt.figure(figsize=(8, 6))  # Plot decision boundary as a contourf Z = probabilities.reshape(xx.shape) contour = plt.contourf(xx, yy, Z, levels=10, cmap=\"coolwarm\", alpha=0.5, vmin=0, vmax=1)  # Plot training and test points plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", edgecolor=\"k\", label=\"train\", marker=\".\", s=60, alpha=0.5) plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"coolwarm\", edgecolor=\"k\", label=\"test\", marker=\"o\", alpha=0.5)  plt.xlabel(\"x1\") plt.ylabel(\"x2\") plt.legend() plt.tight_layout() plt.show()"},{"location":"examples/classifier_basic/#classification","title":"Classification\u00b6","text":""},{"location":"examples/classifier_basic/#1-synthetic-dataset","title":"1. Synthetic dataset\u00b6","text":"<p>In this section, we create a synthetic dataset for a binary classification problem using pure NumPy operations. The dataset consists of two interleaved semicircles, resembling the well-known \u201ctwo moons\u201d pattern, which is often used to test nonlinear classification algorithms. The coordinates of each moon are generated using trigonometric functions, and a small amount of Gaussian noise is added to make the problem more realistic. Finally, we visualize the dataset using Matplotlib, where each class is displayed in a different color to highlight the nonlinear decision boundary between them.</p>"},{"location":"examples/classifier_basic/#2-traintest-split","title":"2. Train\u2013Test Split\u00b6","text":"<p>To evaluate a model\u2019s ability to generalize, we divide the dataset into training and testing subsets using pure NumPy operations. This manual split mimics the functionality of train_test_split from scikit-learn but avoids any external dependencies. First, we shuffle the dataset indices to ensure randomization, then select a portion (e.g., 80%) for training and the remaining samples for testing. This separation allows us to train the model on one subset and assess its performance on unseen data, providing a fair measure of its predictive capability.</p>"},{"location":"examples/classifier_basic/#3-configuring-the-anfis-classifier","title":"3. Configuring the ANFIS Classifier\u00b6","text":"<p>In this step, we configure the ANFISClassifier by defining its main hyperparameters. The parameter n_classes specifies the number of output classes in the classification problem. We also define the initialization strategy for the fuzzy membership functions and use the Adam optimizer \u2014 an adaptive gradient-based optimization algorithm that combines the advantages of momentum and RMSProp for faster and more stable convergence. Finally, we set the number of training epochs, batch size, and random seed to ensure reproducible results.</p>"},{"location":"examples/classifier_basic/#4-training-the-model","title":"4. Training the Model\u00b6","text":"<p>In this stage, we train the ANFISClassifier using the training set. The <code>fit()</code> method iteratively updates the model parameters using the Adam optimizer to minimize the classification error. After training, we evaluate the model on the test data with the `evaluate() method, which computes several performance metrics, including accuracy, precision, recall, and F1-scores (both macro and micro averages). The resulting confusion matrix shows that the classifier correctly distinguishes between the two classes, achieving an overall accuracy of 94%, demonstrating strong generalization on unseen samples.</p>"},{"location":"examples/classifier_basic/#6-visualizing-the-decision-boundary","title":"6. Visualizing the Decision Boundary\u00b6","text":"<p>To better understand the classifier\u2019s behavior, we visualize the decision boundary learned by the ANFIS model. We create a fine grid of points covering the entire feature space and use the trained classifier to predict the class probabilities for each point. These probabilities are then plotted as a smooth color map, where the transition between colors illustrates the regions assigned to each class. Finally, the training and testing samples are overlaid on the same plot, allowing us to clearly see how well the decision surface separates the two classes and how the model generalizes to unseen data.</p>"},{"location":"examples/classifier_iris/","title":"Iris Classification","text":"<p>This example is an extension of classifier_basics. Here we use the Iris dataset, a classic benchmark in machine learning composed of 150 samples from three flower species (Setosa, Versicolor, and Virginica).</p> <p>The dataset is loaded directly from <code>sklearn.datasets</code>, serving here to demonstrate the interoperability between scikit-learn and the ANFIS Toolbox. While this example uses <code>scikit-learn</code> only as a convenient data source, it is important to note that the ANFIS Toolbox itself does not depend on scikit-learn.</p> In\u00a0[27]: Copied! <pre>from sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa: F401\nimport numpy as np\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\n# Select 3 features for visualization (Petal length, Petal width, Sepal length)\nx_idx, y_idx, z_idx = 2, 3, 0  # indices for the features\n\nfig = plt.figure(figsize=(7, 5))\nax = fig.add_subplot(111, projection=\"3d\")\n\nscatter = ax.scatter(\n    X[:, x_idx],\n    X[:, y_idx],\n    X[:, z_idx],\n    c=y,\n    cmap=\"coolwarm\",\n    edgecolor=\"k\",\n    alpha=0.8,\n    s=70,\n)\n\nax.set_title(\"Iris Dataset: 3D Feature Distribution\", pad=12, fontsize=13)\nax.set_xlabel(feature_names[x_idx].capitalize(), fontsize=11, labelpad=8)\nax.set_ylabel(feature_names[y_idx].capitalize(), fontsize=11, labelpad=8)\nax.set_zlabel(feature_names[z_idx].capitalize(), fontsize=11, labelpad=8)\n\n# Legend\nhandles, _ = scatter.legend_elements()\nax.legend(handles, target_names, title=\"Classes\", fontsize=9, title_fontsize=10, loc=\"upper left\")\n\n# Light grid and camera angle\nax.grid(True, linestyle=\"--\", alpha=0.3)\nax.view_init(elev=25, azim=130)\n\nplt.tight_layout()\nplt.show()\n</pre> from sklearn.datasets import load_iris import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 import numpy as np  # Load dataset iris = load_iris() X = iris.data y = iris.target feature_names = iris.feature_names target_names = iris.target_names  # Select 3 features for visualization (Petal length, Petal width, Sepal length) x_idx, y_idx, z_idx = 2, 3, 0  # indices for the features  fig = plt.figure(figsize=(7, 5)) ax = fig.add_subplot(111, projection=\"3d\")  scatter = ax.scatter(     X[:, x_idx],     X[:, y_idx],     X[:, z_idx],     c=y,     cmap=\"coolwarm\",     edgecolor=\"k\",     alpha=0.8,     s=70, )  ax.set_title(\"Iris Dataset: 3D Feature Distribution\", pad=12, fontsize=13) ax.set_xlabel(feature_names[x_idx].capitalize(), fontsize=11, labelpad=8) ax.set_ylabel(feature_names[y_idx].capitalize(), fontsize=11, labelpad=8) ax.set_zlabel(feature_names[z_idx].capitalize(), fontsize=11, labelpad=8)  # Legend handles, _ = scatter.legend_elements() ax.legend(handles, target_names, title=\"Classes\", fontsize=9, title_fontsize=10, loc=\"upper left\")  # Light grid and camera angle ax.grid(True, linestyle=\"--\", alpha=0.3) ax.view_init(elev=25, azim=130)  plt.tight_layout() plt.show()  In\u00a0[29]: Copied! <pre># We'll name the features as X and y the target labels\nX, y = iris.data, iris.target\n</pre> # We'll name the features as X and y the target labels X, y = iris.data, iris.target In\u00a0[30]: Copied! <pre># Randomly shuffle and split the dataset into training (70%) and testing (30%) sets\nnp.random.seed(42)\ntrain_ratio = 0.7\nidx = np.random.permutation(len(X))\nsplit = int(train_ratio * len(X))\nX_train, X_test = X[idx[:split]], X[idx[split:]]\ny_train, y_test = y[idx[:split]], y[idx[split:]]\n</pre> # Randomly shuffle and split the dataset into training (70%) and testing (30%) sets np.random.seed(42) train_ratio = 0.7 idx = np.random.permutation(len(X)) split = int(train_ratio * len(X)) X_train, X_test = X[idx[:split]], X[idx[split:]] y_train, y_test = y[idx[:split]], y[idx[split:]] In\u00a0[59]: Copied! <pre>from anfis_toolbox import ANFISClassifier\n\nclf = ANFISClassifier(\n    n_classes=3,\n    batch_size=64,\n    random_state=42\n)\n</pre> from anfis_toolbox import ANFISClassifier  clf = ANFISClassifier(     n_classes=3,     batch_size=64,     random_state=42 ) In\u00a0[62]: Copied! <pre>clf.fit(X_train, y_train)\nmetrics = clf.evaluate(X_test, y_test)\n</pre> clf.fit(X_train, y_train) metrics = clf.evaluate(X_test, y_test) <pre>ANFISClassifier evaluation:\n  accuracy: 0.933333\n  balanced_accuracy: 0.929630\n  precision_macro: 0.945304\n  recall_macro: 0.929630\n  f1_macro: 0.937402\n  precision_micro: 0.933333\n  recall_micro: 0.933333\n  f1_micro: 0.933333\n  confusion_matrix:\n    [[ 9  0  1]\n     [ 0 17  0]\n     [ 0  2 16]]\n  classes: [0 1 2]\n</pre> In\u00a0[76]: Copied! <pre># Retrieve confusion matrix and class names\ncm = metrics[\"confusion_matrix\"]\nclass_names = iris.target_names\n\nfig, ax = plt.subplots(figsize=(5, 4))\nim = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n\n# Title and labels\nax.set_title(\"ANFIS Classifier - Confusion Matrix\", fontsize=13, pad=10)\nax.set_xlabel(\"Predicted Label\", fontsize=11)\nax.set_ylabel(\"True Label\", fontsize=11)\nax.set_xticks(np.arange(len(class_names)))\nax.set_yticks(np.arange(len(class_names)))\nax.set_xticklabels(class_names)\nax.set_yticklabels(class_names)\n\n# Annotate each cell with count\nfor i in range(len(class_names)):\n    for j in range(len(class_names)):\n        ax.text(\n            j, i, f\"{cm[i, j]}\",\n            ha=\"center\", va=\"center\",\n            color=\"black\" if cm[i, j] &lt; cm.max() / 2 else \"white\",\n            fontsize=9, weight=\"bold\"\n        )\n\n# Aesthetic adjustments\nplt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n</pre> # Retrieve confusion matrix and class names cm = metrics[\"confusion_matrix\"] class_names = iris.target_names  fig, ax = plt.subplots(figsize=(5, 4)) im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)  # Title and labels ax.set_title(\"ANFIS Classifier - Confusion Matrix\", fontsize=13, pad=10) ax.set_xlabel(\"Predicted Label\", fontsize=11) ax.set_ylabel(\"True Label\", fontsize=11) ax.set_xticks(np.arange(len(class_names))) ax.set_yticks(np.arange(len(class_names))) ax.set_xticklabels(class_names) ax.set_yticklabels(class_names)  # Annotate each cell with count for i in range(len(class_names)):     for j in range(len(class_names)):         ax.text(             j, i, f\"{cm[i, j]}\",             ha=\"center\", va=\"center\",             color=\"black\" if cm[i, j] &lt; cm.max() / 2 else \"white\",             fontsize=9, weight=\"bold\"         )  # Aesthetic adjustments plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04) plt.grid(False) plt.tight_layout() plt.show()"},{"location":"examples/classifier_iris/#iris-classification","title":"Iris Classification\u00b6","text":""},{"location":"examples/classifier_iris/#1-load-dataset","title":"1. Load dataset\u00b6","text":"<p>The dataset contains 150 samples from three flower species \u2014 Setosa, Versicolor, and Virginica. Each sample is described by four continuous features that capture the flower\u2019s morphology: sepal length, sepal width, petal length, and petal width.</p> <p>First, we\u2019ll load the dataset and visualize the class distribution using three of these features to get an intuitive 3D view of how the samples are separated in feature space.</p>"},{"location":"examples/classifier_iris/#2-prepare-data","title":"2. Prepare data\u00b6","text":"<p>Before training the model, we split the dataset into training and testing subsets to evaluate generalization performance. We\u2019ll use the standard 70/30 ratio, ensuring each class remains proportionally represented in both subsets.</p>"},{"location":"examples/classifier_iris/#3-configure-the-anfis-classifier","title":"3. Configure the ANFIS Classifier\u00b6","text":"<p>In this step, we use the standard configuration of the ANFISClassifier.</p> <p>Here, we only adjust a few parameters to better fit the Iris dataset:</p> <ul> <li><code>n_classes=3</code> \u2014 specifies the number of output classes corresponding to the three flower species;</li> <li><code>batch_size=64</code> \u2014 defines the number of samples processed per training step;</li> <li><code>random_state=42</code> \u2014 ensures reproducibility of the results.</li> </ul> <p>All other hyperparameters remain at their default values, including the initialization strategy for membership functions and the optimizer.</p>"},{"location":"examples/classifier_iris/#4-training-the-model","title":"4. Training the Model\u00b6","text":"<p>We now train the classifier using the training subset and evaluate its performance on the test set. The <code>fit</code> method performs the complete training loop \u2014 including membership function updates and consequent parameter optimization \u2014 while <code>evaluate</code> computes standard classification metrics such as accuracy, precision, and recall.</p> <p>The model achieved an overall accuracy of 93.3%, with balanced performance across all classes as indicated by a balanced accuracy of 92.9% and macro-averaged F1-score of 0.94. The confusion matrix shows that most samples were correctly classified, with only a few misclassifications between Versicolor and Virginica, which is expected given their similar feature distributions.</p>"},{"location":"examples/classifier_iris/#5-results-visualization","title":"5. Results Visualization\u00b6","text":"<p>The confusion matrix can be obtained directly from the <code>metrics</code> dictionary returned by the <code>evaluate()</code> method. It summarizes the correct and incorrect predictions for each class, helping identify which flower species are most frequently confused and complementing the numerical metrics reported above.</p>"},{"location":"examples/function_approximation/","title":"Function Approximation","text":"<p>The task is to construct an ANFIS model that approximates the two-input nonlinear sinc function, defined as:</p> <p>$$ z=sinc(x,y)=\\frac{sin(x)}{x}\\times\\frac{sin(y)}{y} $$</p> <p>The dataset consists of 100 input-output data pairs. These pairs are generated from the grid points of the input space defined by the range $[-10,10] \\times [-10,10]$.</p> In\u00a0[2]: Copied! <pre>import numpy as np\n\nnp.random.seed(42)\n\ny = x = np.linspace(-10, 10, 120)\nx, y = np.meshgrid(x, y)\nz = np.sinc(x) * np.sinc(y)\n</pre> import numpy as np  np.random.seed(42)  y = x = np.linspace(-10, 10, 120) x, y = np.meshgrid(x, y) z = np.sinc(x) * np.sinc(y) <p>The following 3D surface plot visualizes the synthetic dataset generated from the sinc function. The dataset comprises 100 input-output pairs sampled from a grid over the input space $[-10, 10] \\times [-10, 10]$. This visualization helps to understand the non-linear nature of the function that the ANFIS model will approximate.</p> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nsurf = ax.plot_surface(x, y, z, cmap='turbo')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.grid(False)\n\n\nfig.colorbar(surf, ax=ax, pad=0.1, shrink=0.5)\n\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d') surf = ax.plot_surface(x, y, z, cmap='turbo') ax.set_xlabel('X') ax.set_ylabel('Y') ax.set_zlabel('Z') ax.grid(False)   fig.colorbar(surf, ax=ax, pad=0.1, shrink=0.5)  plt.show() <p>Combine the input features into a single array and split the data into training and testing subsets to evaluate model performance consistently.</p> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import train_test_split\n\nX = np.c_[x.ravel(), y.ravel()]\ny = z.ravel()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Testing set size: {X_test.shape[0]}\")\n</pre> from sklearn.model_selection import train_test_split  X = np.c_[x.ravel(), y.ravel()] y = z.ravel()  # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)  print(f\"Training set size: {X_train.shape[0]}\") print(f\"Testing set size: {X_test.shape[0]}\") <pre>Training set size: 10080\nTesting set size: 4320\n</pre> <p>This block defines and trains an Adaptive Neuro-Fuzzy Inference System (ANFIS) regression model using the ANFISRegressor class from anfis_toolbox. The model employs 15 Gaussian membership functions (MFs) per input variable, initialized via Fuzzy C-Means (FCM) clustering to provide data-driven fuzzy partitions. Training is performed using a hybrid learning strategy that combines Adam-based gradient descent for premise parameters (the membership function parameters) with least-squares optimization for consequent parameters.</p> In\u00a0[10]: Copied! <pre>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor(n_mfs=15, optimizer=\"hybrid_adam\", epochs=15, batch_size=128, learning_rate=1e-2, random_state=42, init=\"fcm\", verbose=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test))\n</pre> from anfis_toolbox import ANFISRegressor  model = ANFISRegressor(n_mfs=15, optimizer=\"hybrid_adam\", epochs=15, batch_size=128, learning_rate=1e-2, random_state=42, init=\"fcm\", verbose=True) model.fit(X_train, y_train, validation_data=(X_test, y_test)) <pre>Epoch 1 - train_loss: 0.000096 - val_loss: 0.000095\nEpoch 2 - train_loss: 0.000090 - val_loss: 0.000089\nEpoch 3 - train_loss: 0.000085 - val_loss: 0.000083\nEpoch 4 - train_loss: 0.000080 - val_loss: 0.000078\nEpoch 5 - train_loss: 0.000076 - val_loss: 0.000074\nEpoch 6 - train_loss: 0.000072 - val_loss: 0.000070\nEpoch 7 - train_loss: 0.000069 - val_loss: 0.000067\nEpoch 8 - train_loss: 0.000066 - val_loss: 0.000064\nEpoch 9 - train_loss: 0.000064 - val_loss: 0.000062\nEpoch 10 - train_loss: 0.000063 - val_loss: 0.000061\nEpoch 11 - train_loss: 0.000061 - val_loss: 0.000059\nEpoch 12 - train_loss: 0.000061 - val_loss: 0.000059\nEpoch 13 - train_loss: 0.000060 - val_loss: 0.000059\nEpoch 14 - train_loss: 0.000061 - val_loss: 0.000059\nEpoch 15 - train_loss: 0.000061 - val_loss: 0.000059\n</pre> Out[10]: <pre>ANFISRegressor(n_mfs=15, mf_type='gaussian', init='fcm', overlap=0.5, margin=0.1, random_state=42, optimizer='hybrid_adam', learning_rate=0.01, epochs=15, batch_size=128)\n\u251c\u2500 model_: TSKANFIS, n_inputs=2, n_rules=225, inputs=['x1', 'x2'], mfs_per_input=[15, 15]\n\u251c\u2500 optimizer_: HybridAdamTrainer(learning_rate=0.01, epochs=15, verbose=True)\n\u251c\u2500 training_history_: train=15 (last=0.0001), val=15 (last=0.0001)\n\u251c\u2500 rules_: 225 learned\n\u2514\u2500 feature_names_in_: x1, x2</pre> In\u00a0[11]: Copied! <pre>results = model.evaluate(X_test, y_test)\n</pre> results = model.evaluate(X_test, y_test) <pre>ANFISRegressor evaluation:\n  mse: 0.000059\n  rmse: 0.007690\n  mae: 0.003598\n  median_absolute_error: 0.001513\n  mean_bias_error: 0.000021\n  max_error: 0.142773\n  std_error: 0.007690\n  explained_variance: 0.969698\n  r2: 0.969698\n  mape: 172.732437\n  smape: 111.957422\n  pearson: 0.985130\n</pre> <p>After training, the model is used to generate predictions for the entire input space. The predicted output values are reshaped to match the original grid format of the dataset, enabling a direct visual comparison with the target surface. A 3D surface plot is then created using Matplotlib to visualize the ANFIS model\u2019s response over the input domain.</p> In\u00a0[12]: Copied! <pre>z_predict = model.predict(X)\nz_predict = z_predict.reshape(z.shape)\nx = X[:, 0].reshape(x.shape)\ny = X[:, 1].reshape(x.shape)\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nsurf = ax.plot_surface(x, y, z_predict, cmap='turbo')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Remove grid lines\nax.grid(False)\n\n# Add a colorbar and shrink it\nfig.colorbar(surf, ax=ax, pad=0.1, shrink=0.5) # Adjust shrink value as needed\n\nplt.show()\n</pre> z_predict = model.predict(X) z_predict = z_predict.reshape(z.shape) x = X[:, 0].reshape(x.shape) y = X[:, 1].reshape(x.shape)  fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d') surf = ax.plot_surface(x, y, z_predict, cmap='turbo') ax.set_xlabel('X') ax.set_ylabel('Y') ax.set_zlabel('Z')  # Remove grid lines ax.grid(False)  # Add a colorbar and shrink it fig.colorbar(surf, ax=ax, pad=0.1, shrink=0.5) # Adjust shrink value as needed  plt.show()"},{"location":"examples/function_approximation/#function-approximation","title":"Function Approximation\u00b6","text":""},{"location":"examples/function_approximation/#1-dataset-generation","title":"1. Dataset Generation\u00b6","text":""},{"location":"examples/function_approximation/#2-build-train-and-evaluate-anfis","title":"2. Build, train, and evaluate ANFIS\u00b6","text":""},{"location":"examples/regression_basic/","title":"Regression","text":"<p>We import NumPy and helper utilities from ANFIS-Toolbox, then generate a simple noisy sine dataset for regression:</p> <ul> <li>Inputs <code>X</code> are evenly spaced in [-\u03c0, \u03c0].</li> <li>Targets <code>y</code> follow <code>sin(x)</code> with Gaussian noise. This small problem is ideal to showcase ANFIS function approximation.</li> </ul> In\u00a0[2]: Copied! <pre>import numpy as np\n\nnp.random.seed(42)  # For reproducibility\n\nn = 200\nX = np.linspace(-np.pi, np.pi, n).reshape(-1, 1)\ny = np.sin(X[:, 0]) + 0.2 * np.random.randn(n)\ny = y.reshape(-1, 1)\n</pre> import numpy as np  np.random.seed(42)  # For reproducibility  n = 200 X = np.linspace(-np.pi, np.pi, n).reshape(-1, 1) y = np.sin(X[:, 0]) + 0.2 * np.random.randn(n) y = y.reshape(-1, 1) In\u00a0[7]: Copied! <pre>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor()\nmodel.fit(X, y)\nresults = model.evaluate(X, y)\n</pre> from anfis_toolbox import ANFISRegressor  model = ANFISRegressor() model.fit(X, y) results = model.evaluate(X, y) <pre>ANFISRegressor evaluation:\n  mse: 0.033236\n  rmse: 0.182308\n  mae: 0.145525\n  median_absolute_error: 0.115488\n  mean_bias_error: -0.000188\n  max_error: 0.542722\n  std_error: 0.182308\n  explained_variance: 0.940229\n  r2: 0.940229\n  mape: 61.740511\n  smape: 37.811554\n  pearson: 0.969654\n</pre> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\nx_flat = X[:, 0]\ny_true = y[:, 0]\ny_pred = model.predict(X)\n\nplt.figure(figsize=(6, 3))\nplt.scatter(x_flat, y_true, s=20, alpha=0.25, label=\"Training samples\")\nplt.plot(x_flat, y_pred, linewidth=2, label=\"ANFIS prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"ANFIS regression on noisy sine wave\")\nplt.grid(alpha=0.2)\nplt.legend()\nplt.show()\n</pre> import matplotlib.pyplot as plt  x_flat = X[:, 0] y_true = y[:, 0] y_pred = model.predict(X)  plt.figure(figsize=(6, 3)) plt.scatter(x_flat, y_true, s=20, alpha=0.25, label=\"Training samples\") plt.plot(x_flat, y_pred, linewidth=2, label=\"ANFIS prediction\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.title(\"ANFIS regression on noisy sine wave\") plt.grid(alpha=0.2) plt.legend() plt.show()"},{"location":"examples/regression_basic/#regression","title":"Regression\u00b6","text":""},{"location":"examples/regression_basic/#1-synthetic-dataset","title":"1. Synthetic dataset\u00b6","text":""},{"location":"examples/regression_basic/#2-build-train-and-evaluate-anfis","title":"2. Build, train, and evaluate ANFIS\u00b6","text":"<p>Instantiate <code>ANFISRegressor</code> with Gaussian membership functions and the hybrid trainer:</p> <ul> <li>membership functions are inferred directly from the data;</li> <li><code>.fit</code> tunes both antecedent and consequent parameters;</li> <li>we call <code>.evaluate</code> on the fitted low-level model for a compact metric report.</li> </ul>"},{"location":"examples/regression_basic/#3-visualize-regression-predictions","title":"3. Visualize regression predictions\u00b6","text":"<p>Compare the noisy samples against the ANFIS regression curve to gauge fit quality.</p>"},{"location":"examples/time_series/","title":"Time Series","text":"In\u00a0[46]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a time vector\nt = np.linspace(0, 200, 500)\n\n# Create a sinusoidal series with small random noise\nseries = np.sin(0.2 * t) + np.random.normal(0, 10, len(t)) * 0.0\n\n# Visualize the time series\nplt.plot(t, series)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.title(\"Synthetic Time Series\")\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Generate a time vector t = np.linspace(0, 200, 500)  # Create a sinusoidal series with small random noise series = np.sin(0.2 * t) + np.random.normal(0, 10, len(t)) * 0.0  # Visualize the time series plt.plot(t, series) plt.xlabel(\"Time\") plt.ylabel(\"Value\") plt.title(\"Synthetic Time Series\") plt.show() <p>This produces a smooth oscillating pattern that simulates a noisy periodic signal. Such synthetic datasets are useful for testing time series forecasting models under controlled conditions.</p> In\u00a0[47]: Copied! <pre># Create sliding windows of size 6 (5 inputs + 1 target)\nwindow = np.lib.stride_tricks.sliding_window_view(series, 6)\n\n# Split into input features (X) and target values (y)\nX = window[:, :5]\ny = window[:, -1]\n</pre> # Create sliding windows of size 6 (5 inputs + 1 target) window = np.lib.stride_tricks.sliding_window_view(series, 6)  # Split into input features (X) and target values (y) X = window[:, :5] y = window[:, -1] <p>Here:</p> <ul> <li>$X \\in \\mathbb{R}^{(T-5) \\times 5}$ contains the 5-lagged input sequences</li> <li>$y \\in \\mathbb{R}^{(T-5)}$ contains the scalar target values corresponding to each window</li> </ul> <p>This representation allows the model to learn a mapping:</p> <p>$$ f: \\mathbb{R}^5 \\rightarrow \\mathbb{R}, \\quad \\text{where} \\quad \\hat{y}_t = f(\\mathbf{x}_t) $$</p> In\u00a0[48]: Copied! <pre># holdout temporal\ntrain_size = int(0.7 * len(Xs))\nval_size = int(0.15 * len(Xs))\nX_train, X_val, X_test = Xs[:train_size], Xs[train_size:train_size+val_size], Xs[train_size+val_size:]\ny_train, y_val, y_test = ys[:train_size], ys[train_size:train_size+val_size], ys[train_size+val_size:]\n</pre> # holdout temporal train_size = int(0.7 * len(Xs)) val_size = int(0.15 * len(Xs)) X_train, X_val, X_test = Xs[:train_size], Xs[train_size:train_size+val_size], Xs[train_size+val_size:] y_train, y_val, y_test = ys[:train_size], ys[train_size:train_size+val_size], ys[train_size+val_size:]  <p>Formally, if we denote the dataset as:</p> <p>$$ \\mathcal{D} = { (\\mathbf{x}*t, y_t) }*{t=1}^{N} $$</p> <p>then the temporal holdout partitions it into three disjoint subsets:</p> <p>$$ \\begin{aligned} \\mathcal{D}*{\\text{train}} &amp;= { (\\mathbf{x}*t, y_t) }*{t=1}^{N*{\\text{train}}} \\ \\mathcal{D}*{\\text{val}} &amp;= { (\\mathbf{x}*t, y_t) }*{t=N*{\\text{train}}+1}^{N_{\\text{train}}+N_{\\text{val}}} \\ \\mathcal{D}*{\\text{test}} &amp;= { (\\mathbf{x}*t, y_t) }*{t=N*{\\text{train}}+N_{\\text{val}}+1}^{N} \\end{aligned} $$</p> <p>where:</p> <p>$$ N_{\\text{train}} = 0.7N, \\quad N_{\\text{val}} = 0.15N, \\quad N_{\\text{test}} = 0.15N $$</p> <p>This ensures that the model is trained on past data and evaluated on unseen future values, preserving the temporal causality of the series.</p> In\u00a0[62]: Copied! <pre>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor(\n    n_mfs=2,\n    mf_type=\"gaussian\",\n    optimizer=\"hybrid_adam\",\n    epochs=10,\n    batch_size=128,\n    learning_rate=1e-2,\n    init=\"grid\",\n    )\n\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=True)\n</pre> from anfis_toolbox import ANFISRegressor  model = ANFISRegressor(     n_mfs=2,     mf_type=\"gaussian\",     optimizer=\"hybrid_adam\",     epochs=10,     batch_size=128,     learning_rate=1e-2,     init=\"grid\",     )  model.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=True) <pre>Epoch 1 - train_loss: 0.014626 - val_loss: 0.021406\nEpoch 2 - train_loss: 0.014608 - val_loss: 0.021504\nEpoch 3 - train_loss: 0.014589 - val_loss: 0.021603\nEpoch 4 - train_loss: 0.014571 - val_loss: 0.021702\nEpoch 5 - train_loss: 0.014552 - val_loss: 0.021803\nEpoch 6 - train_loss: 0.014533 - val_loss: 0.021903\nEpoch 7 - train_loss: 0.014513 - val_loss: 0.022004\nEpoch 8 - train_loss: 0.014493 - val_loss: 0.022105\nEpoch 9 - train_loss: 0.014473 - val_loss: 0.022205\nEpoch 10 - train_loss: 0.014452 - val_loss: 0.022304\n</pre> Out[62]: <pre>ANFISRegressor(n_mfs=2, mf_type='gaussian', init='grid', overlap=0.5, margin=0.1, optimizer='hybrid_adam', learning_rate=0.01, epochs=10, batch_size=128)\n\u251c\u2500 model_: TSKANFIS, n_inputs=5, n_rules=32, inputs=['x1', 'x2', 'x3', 'x4', 'x5'], mfs_per_input=[2, 2, 2, 2, 2]\n\u251c\u2500 optimizer_: HybridAdamTrainer(learning_rate=0.01, epochs=10, verbose=True)\n\u251c\u2500 training_history_: train=10 (last=0.0145), val=10 (last=0.0223)\n\u251c\u2500 rules_: 32 learned\n\u2514\u2500 feature_names_in_: x1, x2, x3, x4, x5</pre> <p>The ANFIS model implements a first-order Takagi\u2013Sugeno\u2013Kang (TSK) fuzzy inference system. For a rule ( i ) of the form:</p> <p>$$ R_i: \\text{If } x_1 \\text{ is } A_{i1} \\text{ and } x_2 \\text{ is } A_{i2} \\text{ and } \\dots \\text{ and } x_5 \\text{ is } A_{i5}, \\text{ then } y_i = p_{i1}x_1 + p_{i2}x_2 + \\dots + p_{i5}x_5 + r_i $$</p> <p>the overall output of the system is a weighted average:</p> <p>$$ \\hat{y} = \\frac{\\sum_{i=1}^{M} w_i y_i}{\\sum_{i=1}^{M} w_i} \\quad \\text{where} \\quad w_i = \\prod_{j=1}^{5} \\mu_{A_{ij}}(x_j) $$</p> <p>Here:</p> <ul> <li>$\\mu_{A_{ij}}(x_j)$ is the Gaussian membership degree of input $x_j$ in fuzzy set $A_{ij}$,</li> <li>$w_i$ is the firing strength of rule $i$,</li> <li>and $M = 2^5 = 32$ rules were automatically generated by the grid partitioning of inputs.</li> </ul> In\u00a0[63]: Copied! <pre>results = model.evaluate(X_test, y_test)\n</pre> results = model.evaluate(X_test, y_test) <pre>ANFISRegressor evaluation:\n  mse: 0.022364\n  rmse: 0.149545\n  mae: 0.112699\n  median_absolute_error: 0.089701\n  mean_bias_error: -0.021021\n  max_error: 0.386602\n  std_error: 0.148061\n  explained_variance: 0.958387\n  r2: 0.957548\n  mape: 24.371782\n  smape: 25.751406\n  pearson: 0.983337\n</pre> <p>The evaluation results indicate excellent predictive performance:</p> <ul> <li>( R^2 = 0.9575 ) and Explained Variance = 0.9584 show that the model explains over 95% of the variability in the data.</li> <li>The high Pearson correlation (0.9833) confirms strong linear agreement between predictions and true values.</li> <li>Errors (MAE \u2248 0.11, RMSE \u2248 0.15) are low relative to the scale of the target variable.</li> <li>The small negative MBE (\u22120.021) suggests a slight underestimation bias.</li> <li>MAPE and sMAPE around 25% indicate moderate percentage errors, acceptable for nonlinear regression with noisy data.</li> </ul> <p>Overall, the ANFIS achieved a robust generalization performance on the test set.</p> In\u00a0[67]: Copied! <pre># Generate predictions\ny_pred = model.predict(X_test)\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# --- (1) Time series comparison ---\naxes[0].plot(y_test, color=\"green\", alpha=0.5, label=\"True values\")\naxes[0].plot(y_pred, color=\"red\", alpha=0.7, label=\"Predicted values\")\naxes[0].set_title(\"Time Series Comparison\")\naxes[0].set_xlabel(\"Time step\")\naxes[0].set_ylabel(\"Value\")\naxes[0].legend()\naxes[0].grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- (2) Scatter plot with 45\u00b0 reference line ---\naxes[1].scatter(y_test, y_pred, alpha=0.5, color=\"royalblue\", edgecolor=\"white\")\nmin_val = min(y_test.min(), y_pred.min())\nmax_val = max(y_test.max(), y_pred.max())\naxes[1].plot([min_val, max_val], [min_val, max_val],\n             color=\"black\", linestyle=\"--\", linewidth=1.2, label=\"Ideal (y = \u0177)\")\naxes[1].set_title(\"Predicted vs. True Values\")\naxes[1].set_xlabel(\"True Values (y)\")\naxes[1].set_ylabel(\"Predicted Values (\u0177)\")\naxes[1].legend()\naxes[1].grid(True, linestyle=\"--\", alpha=0.4)\naxes[1].set_aspect(\"equal\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate predictions y_pred = model.predict(X_test)  # Create subplots fig, axes = plt.subplots(1, 2, figsize=(12, 4))  # --- (1) Time series comparison --- axes[0].plot(y_test, color=\"green\", alpha=0.5, label=\"True values\") axes[0].plot(y_pred, color=\"red\", alpha=0.7, label=\"Predicted values\") axes[0].set_title(\"Time Series Comparison\") axes[0].set_xlabel(\"Time step\") axes[0].set_ylabel(\"Value\") axes[0].legend() axes[0].grid(True, linestyle=\"--\", alpha=0.4)  # --- (2) Scatter plot with 45\u00b0 reference line --- axes[1].scatter(y_test, y_pred, alpha=0.5, color=\"royalblue\", edgecolor=\"white\") min_val = min(y_test.min(), y_pred.min()) max_val = max(y_test.max(), y_pred.max()) axes[1].plot([min_val, max_val], [min_val, max_val],              color=\"black\", linestyle=\"--\", linewidth=1.2, label=\"Ideal (y = \u0177)\") axes[1].set_title(\"Predicted vs. True Values\") axes[1].set_xlabel(\"True Values (y)\") axes[1].set_ylabel(\"Predicted Values (\u0177)\") axes[1].legend() axes[1].grid(True, linestyle=\"--\", alpha=0.4) axes[1].set_aspect(\"equal\")  plt.tight_layout() plt.show()"},{"location":"examples/time_series/#time-series","title":"Time Series\u00b6","text":""},{"location":"examples/time_series/#1-generating-synthetic-time-series-data","title":"1. Generating Synthetic Time Series Data\u00b6","text":"<p>To create a simple univariate time series for experimentation, we generated synthetic data using a sinusoidal signal with added Gaussian noise. The following code snippet illustrates this process:</p>"},{"location":"examples/time_series/#2-creating-supervised-samples-using-rolling-windows","title":"2. Creating Supervised Samples Using Rolling Windows\u00b6","text":"<p>To convert the time series into a supervised learning format, we applied a rolling window (or sliding window) of fixed length. Each input sample consists of the previous ( k = 5 ) observations, and the target corresponds to the next scalar value in the sequence.</p> <p>Formally, given a univariate time series:</p> <p>$$ { x_t }_{t=1}^{T} $$</p> <p>we construct the input\u2013output pairs as:</p> <p>$$ \\mathbf{x}*t = [x*{t-5}, , x_{t-4}, , x_{t-3}, , x_{t-2}, , x_{t-1}] \\quad \\text{and} \\quad y_t = x_t $$</p> <p>for $t = 6, 7, \\ldots, T$.</p> <p>The transformation is implemented as follows:</p>"},{"location":"examples/time_series/#3-temporal-holdout-split","title":"3. Temporal Holdout Split\u00b6","text":"<p>Since time series data are inherently sequential, the dataset must be split without shuffling to preserve temporal dependencies. We use a temporal holdout approach, dividing the dataset into training, validation, and test subsets based on chronological order.</p> <p>In this setup:</p> <ul> <li>70% of the samples are used for training,</li> <li>15% for validation,</li> <li>and the remaining 15% for testing.</li> </ul>"},{"location":"examples/time_series/#4-training-the-anfis-model","title":"4. Training the ANFIS Model\u00b6","text":"<p>To model the nonlinear dynamics of the time series, we employed an Adaptive Neuro-Fuzzy Inference System (ANFIS). ANFIS combines the interpretability of fuzzy logic with the learning capabilities of neural networks. The model was implemented using the <code>ANFISRegressor</code> class from the <code>anfis_toolbox</code> package.</p>"},{"location":"examples/time_series/#5-model-evaluation","title":"5. Model Evaluation\u00b6","text":"<p>After training, the ANFIS model was evaluated on the test set using several regression metrics to assess both accuracy and bias.</p>"},{"location":"examples/time_series/#6-visualizing-model-predictions","title":"6. Visualizing Model Predictions\u00b6","text":"<p>To jointly visualize temporal alignment and prediction accuracy, we combined both plots into a single figure with two columns.</p>"},{"location":"hooks/test_hook/","title":"Test hook","text":"In\u00a0[\u00a0]: Copied! <pre>def on_post_page(output, page, config):\n    path = str(page.file.src_uri)\n    if not path.endswith(\".ipynb\"):\n        return output\n\n    output = output.replace(\n        \"\"\"&lt;div class=\"highlight-ipynb hl-python\"&gt;\"\"\",\n        \"\"\"&lt;div class=\"language-python highlight\"&gt;\"\"\"\n        )\n\n    return output\n</pre> def on_post_page(output, page, config):     path = str(page.file.src_uri)     if not path.endswith(\".ipynb\"):         return output      output = output.replace(         \"\"\"\"\"\",         \"\"\"\"\"\"         )      return output"},{"location":"membership_functions/bell/","title":"Bell-shaped","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import BellMF\n\nbellmf = BellMF(a=2, b=4, c=5)\n\nx = np.linspace(0, 10, 100)\ny = bellmf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import BellMF  bellmf = BellMF(a=2, b=4, c=5)  x = np.linspace(0, 10, 100) y = bellmf(x)  plt.plot(x, y) plt.show() <p>The image displays a series of symmetrical bell-shaped curves. You can observe how increasing the width ($a$) makes the curve broader, while increasing the slope ($b$) makes the sides of the curve steeper.</p>"},{"location":"membership_functions/bell/#bell-shaped","title":"Bell-shaped\u00b6","text":"<p>The Generalized Bell membership function (BellMF), also known as the Bell-shaped curve, is a versatile function used in fuzzy logic to define fuzzy sets. Like other membership functions, it assigns a degree of membership to an element, but it offers greater flexibility than the GaussianMF by using an additional parameter to control its shape. Its form is a smooth, symmetrical bell curve.</p> <p>The function is defined by three parameters:</p> <ul> <li>center ($c$): This parameter determines the center of the curve, representing the point in the domain with a maximum membership value of 1.</li> <li>width ($a$): This parameter controls the width or spread of the curve. A larger value of $a$ results in a wider curve, while a smaller value produces a narrower curve.</li> <li>slope ($b$): This parameter, which must be a positive value, determines the slope of the curve's sides. It directly impacts the steepness of the curve's transition from 0 to 1. A larger $b$ value creates a steeper curve, making the fuzzy set sharper and less \"fuzzy.\"</li> </ul> <p>The mathematical formula for the Generalized Bell membership function is given by:</p> <p>$$\\mu(x) = \\frac{1}{1 + \\left|\\frac{x-c}{a}\\right|^{2b}}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$c$ is the center of the curve.</li> <li>$a$ is the width of the curve.</li> <li>$b$ is the slope of the curve.</li> </ul>"},{"location":"membership_functions/bell/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the GbellMF are essential for training and optimizing fuzzy systems. They show how the membership value changes with respect to small changes in each of the three parameters, which is vital for algorithms like backpropagation used in fuzzy-neural networks.</p> Derivative with respect to the center ($c$) <p>The partial derivative of the function with respect to its center ($c$) is:</p> <p>$$\\frac{\\partial f}{\\partial c} = \\frac{2b(x-c)}{a^2} \\left(1 + \\left|\\frac{x-c}{a}\\right|^{2b}\\right)^{-2}$$</p> <p>This derivative indicates how the membership value changes when the curve is shifted along the x-axis.</p> Derivative with respect to the width ($a$) <p>The partial derivative with respect to the width ($a$) is:</p> <p>$$\\frac{\\partial f}{\\partial a} = \\frac{2b(x-c)^2}{a^3} \\left(1 + \\left|\\frac{x-c}{a}\\right|^{2b}\\right)^{-2}$$</p> <p>This derivative helps in adjusting the spread of the fuzzy set to encompass a broader or narrower range of values.</p> Derivative with respect to the slope ($b$) <p>The partial derivative with respect to the slope ($b$) is:</p> <p>$$\\frac{\\partial f}{\\partial b} = -\\frac{2}{b} \\frac{(x-c)^2}{a^2} \\left(\\frac{1}{1 + \\left|\\frac{x-c}{a}\\right|^{2b}}\\right)^2 \\ln\\left|\\frac{x-c}{a}\\right|$$</p> <p>This derivative is used to modify the steepness of the curve, allowing for fine-tuning of the transition from non-membership to full membership.</p>"},{"location":"membership_functions/bell/#python-example","title":"Python Example\u00b6","text":"<p>The following code demonstrates how to generate a Generalized Bell membership function using the <code>numpy</code> and <code>matplotlib</code> libraries in Python.</p>"},{"location":"membership_functions/bell/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of a Generalized Bell membership function, showing how its shape is influenced by the width ($a$) and slope ($b$) parameters, while the center ($c$) remains fixed.</p>"},{"location":"membership_functions/gaussian-combination/","title":"Gaussian Combination","text":"<p>The Gaussian combination membership function (Gaussian2MF) is a versatile fuzzy membership function that combines two Gaussian curves with an optional flat region in between. This function is particularly useful for representing fuzzy sets with asymmetric shapes or plateau regions, making it suitable for applications where the membership degree needs to be constant over a range of values.</p> <p>The function is characterized by four main parameters:</p> <ul> <li>sigma1 ($\\sigma_1$): Standard deviation of the left Gaussian tail (must be positive).</li> <li>c1 ($c_1$): Center of the left Gaussian tail.</li> <li>sigma2 ($\\sigma_2$): Standard deviation of the right Gaussian tail (must be positive).</li> <li>c2 ($c_2$): Center of the right Gaussian tail. Must satisfy $c_1 \\leq c_2$.</li> </ul> <p>The mathematical formula for the Gaussian combination membership function is defined piecewise:</p> <p>$$\\mu(x) =  \\begin{array}{ll}  e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}} &amp; x &lt; c_1 \\\\[6pt]  1 &amp; c_1 \\leq x \\leq c_2 \\\\[6pt]  e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}} &amp; x &gt; c_2  \\end{array}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$\\sigma_1, c_1$ define the left Gaussian tail.</li> <li>$\\sigma_2, c_2$ define the right Gaussian tail.</li> </ul> <p>When $c_1 = c_2$, the function becomes an asymmetric Gaussian centered at $c_1$ with different spreads on each side.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import Gaussian2MF\n\n\ngaussian2 = Gaussian2MF(sigma1=.5, c1=2, sigma2=1, c2=5)\n\nx = np.linspace(0, 10, 100)\ny = gaussian2(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import Gaussian2MF   gaussian2 = Gaussian2MF(sigma1=.5, c1=2, sigma2=1, c2=5)  x = np.linspace(0, 10, 100) y = gaussian2(x)  plt.plot(x, y) plt.show() <p>The visualization above demonstrates various configurations of the Gaussian2MF. This flexibility makes Gaussian2MF suitable for modeling complex fuzzy concepts with asymmetric uncertainty or plateau regions where membership should remain constant.</p>"},{"location":"membership_functions/gaussian-combination/#gaussian-combination","title":"Gaussian Combination\u00b6","text":""},{"location":"membership_functions/gaussian-combination/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the Gaussian combination membership function are essential for optimization in adaptive fuzzy systems. Since the function is piecewise, derivatives are computed separately for each region.</p> Derivative with respect to $c_1$ <p>For $x &lt; c_1$: $$\\frac{\\partial \\mu}{\\partial c_1} = \\frac{x-c_1}{\\sigma_1^2} e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}}$$</p> <p>For $c_1 \\leq x \\leq c_2$: The derivative is 0 (flat region).</p> <p>For $x &gt; c_2$: The derivative is 0.</p> Derivative with respect to $\\sigma_1$ <p>For $x &lt; c_1$: $$\\frac{\\partial \\mu}{\\partial \\sigma_1} = \\frac{(x-c_1)^2}{\\sigma_1^3} e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $c_2$ <p>For $x &lt; c_1$: The derivative is 0.</p> <p>For $c_1 \\leq x \\leq c_2$: The derivative is 0.</p> <p>For $x &gt; c_2$: $$\\frac{\\partial \\mu}{\\partial c_2} = \\frac{x-c_2}{\\sigma_2^2} e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}}$$</p> Derivative with respect to $\\sigma_2$ <p>For $x &gt; c_2$: $$\\frac{\\partial \\mu}{\\partial \\sigma_2} = \\frac{(x-c_2)^2}{\\sigma_2^3} e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}}$$</p> <p>For other regions: The derivative is 0.</p> <p>These derivatives enable gradient-based optimization of the membership function parameters.</p>"},{"location":"membership_functions/gaussian-combination/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/gaussian-combination/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the Gaussian2MF shape changes with different parameter combinations. We'll explore variations in the centers (c\u2081, c\u2082) and standard deviations (\u03c3\u2081, \u03c3\u2082).</p>"},{"location":"membership_functions/gaussian/","title":"Gaussian","text":"<p>The Gaussian membership function (GaussianMF) is a fundamental concept in fuzzy logic, widely used to define fuzzy sets. Unlike a classical set where an element either fully belongs or does not belong, a fuzzy set allows for partial membership, and the GaussianMF provides a smooth, continuous way to represent this degree of belonging. It possesses a smooth, bell-like shape, which contributes to its intuitive nature and popularity across various applications.</p> <p>The function is characterized by two main parameters:</p> <ul> <li>mean ($\\mu$): This parameter determines the center of the curve. It represents the point in the domain where the degree of membership is maximum, specifically 1.</li> <li>sigma ($\\sigma$): This parameter controls the width or spread of the curve. It must be a positive value. A larger $\\sigma$ results in a wider, flatter curve, indicating a broader range of values with high membership. Conversely, a smaller $\\sigma$ produces a sharper, more peaked curve, suggesting a narrower range of values with a high degree of belonging.</li> </ul> <p>The mathematical formula for the Gaussian membership function is given by:</p> <p>$$\\mu(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$\\mu$ is the mean of the curve.</li> <li>$\\sigma$ is the standard deviation (width) of the curve.</li> </ul> <p>The partial derivatives of the Gaussian membership function are crucial for optimization algorithms, especially in adaptive or machine learning-based fuzzy systems. They show how the membership value changes in response to small adjustments to the parameters, which is essential for training models to better fit data.</p> Derivative with respect to $\\mu$ <p>The partial derivative of the function with respect to the mean ($\\mu$) is:</p> <p>$$\\frac{\\partial f}{\\partial \\mu} = \\frac{x-\\mu}{\\sigma^2} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>This derivative indicates how the membership value is affected when the center of the bell curve is shifted. It is used to adjust the position of the function to better align with the data.</p> Derivative with respect to $\\sigma$** <p>The partial derivative with respect to the standard deviation ($\\sigma$) is:</p> <p>$$\\frac{\\partial f}{\\partial \\sigma} = \\frac{(x-\\mu)^2}{\\sigma^3} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>This derivative shows how the membership value changes as the width of the curve is adjusted. It is used to refine the spread of the function, making it sharper or wider as needed to represent the uncertainty in the data more accurately.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import GaussianMF\n\ngaussian = GaussianMF(5, 1)\n\nx = np.linspace(0, 10, 100)\ny = gaussian(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import GaussianMF  gaussian = GaussianMF(5, 1)  x = np.linspace(0, 10, 100) y = gaussian(x)  plt.plot(x, y) plt.show() <p>Below is a visual representation of a Gaussian membership function, showing how its shape is influenced by the mean $\\mu$ and sigma ($\\sigma$) parameters.</p> <p>The image displays a classic bell-shaped curve, illustrating how the membership value (on the y-axis) smoothly changes for different input values (on the x-axis). The peak of the curve is located at the mean, and the spread of the curve is controlled by sigma.</p>"},{"location":"membership_functions/gaussian/#gaussian","title":"Gaussian\u00b6","text":""},{"location":"membership_functions/gaussian/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"membership_functions/gaussian/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/gaussian/#visualization","title":"Visualization\u00b6","text":""},{"location":"membership_functions/pi/","title":"Pi-shaped","text":"<p>The Pi-shaped Membership Function is defined piecewise with smooth transitions:</p> <p>$$\\mu(x) = \\begin{array}{ll}  S(x; a, b) &amp; a \\leq x \\leq b \\\\[6pt] 1 &amp; b \\leq x \\leq c \\\\[6pt] Z(x; c, d) &amp; c \\leq x \\leq d \\\\[6pt] 0 &amp; \\text{otherwise} \\end{array}$$</p> <p>Where:</p> <ul> <li>S(x; a, b) is the rising S-shaped function: smooth transition from 0 to 1</li> <li>Z(x; c, d) is the falling Z-shaped function: smooth transition from 1 to 0</li> </ul> <p>The smooth transitions use cubic smoothstep functions:</p> <p>S-function (rising edge): $$S(x; a, b) = 3t^2 - 2t^3 \\quad \\text{where} \\quad t = \\frac{x - a}{b - a}$$</p> <p>Z-function (falling edge): $$Z(x; c, d) = 1 - S(x; c, d) = 1 - (3t^2 - 2t^3) \\quad \\text{where} \\quad t = \\frac{x - c}{d - c}$$</p> <p>The function is characterized by four parameters:</p> <ul> <li>a: Left foot - where the function starts rising from 0</li> <li>b: Left shoulder - where the function reaches the plateau (\u03bc = 1)</li> <li>c: Right shoulder - where the function starts falling from the plateau</li> <li>d: Right foot - where the function reaches 0</li> </ul> <p>For a valid Pi-shaped function, parameters must satisfy: a &lt; b \u2264 c &lt; d</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import PiMF\n\npimf = PiMF(a=1, b=3, c=7, d=9)\n\nx = np.linspace(0, 10, 200)\ny = pimf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import PiMF  pimf = PiMF(a=1, b=3, c=7, d=9)  x = np.linspace(0, 10, 200) y = pimf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/pi/#pi-shaped","title":"Pi-shaped\u00b6","text":"<p>The Pi-shaped Membership Function creates a smooth bell-like curve with a flat plateau at the top. It combines S-shaped rising and Z-shaped falling edges with a trapezoidal-like plateau, making it ideal for representing concepts with gradual transitions and stable regions.</p>"},{"location":"membership_functions/pi/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The PiMF has analytical gradients computed by region:</p> S-Function Region (a \u2264 x \u2264 b) For the rising edge: \u03bc(x) = S(x; a, b) = 3t\u00b2 - 2t\u00b3 where t = (x-a)/(b-a)  <ul> <li>\u2202\u03bc/\u2202a = dS/dt \u00b7 dt/da = [6t(1-t)] \u00b7 [(x-b)/(b-a)\u00b2]</li> <li>\u2202\u03bc/\u2202b = dS/dt \u00b7 dt/db = [6t(1-t)] \u00b7 [-(x-a)/(b-a)\u00b2]</li> <li>\u2202\u03bc/\u2202c = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202d = 0 (no effect in this region)</li> </ul> Plateau Region (b \u2264 x \u2264 c) \u03bc(x) = 1 (constant function)  <ul> <li>\u2202\u03bc/\u2202a = 0 (constant function)</li> <li>\u2202\u03bc/\u2202b = 0 (constant function)</li> <li>\u2202\u03bc/\u2202c = 0 (constant function)</li> <li>\u2202\u03bc/\u2202d = 0 (constant function)</li> </ul> Z-Function Region (c \u2264 x \u2264 d) For the falling edge: \u03bc(x) = Z(x; c, d) = 1 - S(x; c, d) where t = (x-c)/(d-c)  <ul> <li>\u2202\u03bc/\u2202a = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202b = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202c = dZ/dt \u00b7 dt/dc = [-6t(1-t)] \u00b7 [(x-d)/(d-c)\u00b2]</li> <li>\u2202\u03bc/\u2202d = dZ/dt \u00b7 dt/dd = [-6t(1-t)] \u00b7 [-(x-c)/(d-c)\u00b2]</li> </ul>"},{"location":"membership_functions/pi/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a Pi-shaped membership function and visualize it:</p>"},{"location":"membership_functions/pi/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different Pi-shaped membership functions with varying parameter combinations. Each subplot demonstrates how the plateau width and transition smoothness affect the overall shape.</p>"},{"location":"membership_functions/sigmoidal-difference/","title":"Difference of Sigmoidal","text":"<p>The Difference of Sigmoidal Membership Functions is defined as:</p> <p>$$\\mu(x) = s_1(x) - s_2(x)$$</p> <p>Where each sigmoid function is:</p> <p>$$s_1(x) = \\frac{1}{1 + e^{-a_1(x - c_1)}}$$ $$s_2(x) = \\frac{1}{1 + e^{-a_2(x - c_2)}}$$</p> <p>The function is characterized by four parameters (two for each sigmoid):</p> <ul> <li><p>a\u2081: Slope parameter for the first sigmoid (s\u2081)</p> <ul> <li>Positive values: standard sigmoid (0 \u2192 1 as x increases)</li> <li>Negative values: inverted sigmoid (1 \u2192 0 as x increases)</li> <li>Larger |a\u2081|: steeper transition for s\u2081</li> </ul> </li> <li><p>c\u2081: Center parameter for the first sigmoid (s\u2081)</p> <ul> <li>Controls the inflection point where s\u2081(c\u2081) = 0.5</li> <li>Shifts s\u2081 left/right along the x-axis</li> </ul> </li> <li><p>a\u2082: Slope parameter for the second sigmoid (s\u2082)</p> <ul> <li>Same interpretation as a\u2081 but for s\u2082</li> </ul> </li> <li><p>c\u2082: Center parameter for the second sigmoid (s\u2082)</p> <ul> <li>Same interpretation as c\u2081 but for s\u2082</li> </ul> </li> <li><p>a\u2081 \u2260 0 and a\u2082 \u2260 0: Cannot be zero (would result in constant functions)</p> </li> <li><p>All parameters can be any real numbers otherwise</p> </li> </ul> In\u00a0[5]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import DiffSigmoidalMF\n\ndiff_sigmoid = DiffSigmoidalMF(a1=2, c1=4, a2=5, c2=8)\n\nx = np.linspace(0, 10, 100)\ny = diff_sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import DiffSigmoidalMF  diff_sigmoid = DiffSigmoidalMF(a1=2, c1=4, a2=5, c2=8)  x = np.linspace(0, 10, 100) y = diff_sigmoid(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal-difference/#difference-of-sigmoidal","title":"Difference of Sigmoidal\u00b6","text":"<p>The Difference of Sigmoidal Membership Functions implements \u03bc(x) = s\u2081(x) - s\u2082(x), where each s is a logistic curve with its own slope and center parameters. This creates complex membership shapes by combining two sigmoid functions.</p>"},{"location":"membership_functions/sigmoidal-difference/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. Since \u03bc(x) = s\u2081(x) - s\u2082(x), the derivatives follow from the chain rule:</p> Derivative w.r.t. Parameters of First Sigmoid (s\u2081) <p>For s\u2081(x) = 1/(1 + exp(-a\u2081(x - c\u2081))):</p> <ul> <li>\u2202\u03bc/\u2202a\u2081 = \u2202s\u2081/\u2202a\u2081 = s\u2081(x) \u00b7 (1 - s\u2081(x)) \u00b7 (x - c\u2081)</li> <li>\u2202\u03bc/\u2202c\u2081 = \u2202s\u2081/\u2202c\u2081 = -a\u2081 \u00b7 s\u2081(x) \u00b7 (1 - s\u2081(x))</li> </ul> Derivative w.r.t. Parameters of Second Sigmoid (s\u2082) <p>For s\u2082(x) = 1/(1 + exp(-a\u2082(x - c\u2082))), and since \u03bc(x) = s\u2081(x) - s\u2082(x):</p> <ul> <li>\u2202\u03bc/\u2202a\u2082 = -\u2202s\u2082/\u2202a\u2082 = -s\u2082(x) \u00b7 (1 - s\u2082(x)) \u00b7 (x - c\u2082)</li> <li>\u2202\u03bc/\u2202c\u2082 = -\u2202s\u2082/\u2202c\u2082 = -(-a\u2082 \u00b7 s\u2082(x) \u00b7 (1 - s\u2082(x))) = a\u2082 \u00b7 s\u2082(x) \u00b7 (1 - s\u2082(x))</li> </ul> Derivative w.r.t. Input (Optional) <p>For chaining in neural networks:</p> <p>$$\\frac{d\\mu}{dx} = \\frac{ds_1}{dx} - \\frac{ds_2}{dx}$$</p> <p>Where: $$\\frac{ds_1}{dx} = a_1 \\cdot s_1(x) \\cdot (1 - s_1(x))$$ $$\\frac{ds_2}{dx} = a_2 \\cdot s_2(x) \\cdot (1 - s_2(x))$$</p> Gradient Computation Details <p>The gradients are computed using the fundamental sigmoid derivative property: $$\\frac{d}{dx}\\left(\\frac{1}{1+e^{-z}}\\right) = \\frac{1}{1+e^{-z}} \\cdot \\left(1 - \\frac{1}{1+e^{-z}}\\right) = s(x) \\cdot (1 - s(x))$$</p> <p>This property is used extensively in neural network backpropagation and makes the DiffSigmoidalMF computationally efficient for optimization.</p>"},{"location":"membership_functions/sigmoidal-difference/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a difference of sigmoidal membership functions and visualize its components:</p>"},{"location":"membership_functions/sigmoidal-difference/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different difference of sigmoidal membership functions with varying parameter combinations. Each subplot demonstrates how the combination of two sigmoids creates complex membership shapes.</p>"},{"location":"membership_functions/sigmoidal-product/","title":"Product of Sigmoidal","text":"<p>The function is defined as the product of two sigmoidal functions:</p> <p>$$\\mu(x) = f_1(x) \\cdot f_2(x)$$ $$f_1(x) = \\frac{1}{1 + e^{-a_1(x-c_1)}}$$ $$f_2(x) = \\frac{1}{1 + e^{-a_2(x-c_2)}}$$</p> <p>The function is controlled by four parameters, two for each component sigmoid:</p> <ul> <li><p>$a_1$: Slope of the first sigmoid function.</p> </li> <li><p>$c_1$: Center (inflection point) of the first sigmoid function.</p> </li> <li><p>$a_2$: Slope of the second sigmoid function.</p> </li> <li><p>$c_2$: Center (inflection point) of the second sigmoid function.</p> </li> <li><p>To form a proper bell-shaped curve, the slopes $a_1$ and $a_2$ must have opposite signs (e.g., if $a_1 &gt; 0$, then $a_2 &lt; 0$).</p> </li> <li><p>The centers $c_1$ and $c_2$ determine the width and position of the curve's peak.</p> </li> </ul> In\u00a0[12]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import ProdSigmoidalMF\n\nmf = ProdSigmoidalMF(a1=2, c1=3, a2=-2, c2=4)\n\nx = np.linspace(0, 10, 400)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import ProdSigmoidalMF  mf = ProdSigmoidalMF(a1=2, c1=3, a2=-2, c2=4)  x = np.linspace(0, 10, 400) y = mf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal-product/#product-of-sigmoidal","title":"Product of Sigmoidal\u00b6","text":"<p>The Product of Sigmoidal Membership Function creates a smooth, asymmetrical, bell-shaped curve by multiplying two distinct sigmoidal functions. It's used to model fuzzy sets that require a gradual but non-uniform transition, offering more flexibility than symmetric functions.</p>"},{"location":"membership_functions/sigmoidal-product/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives with respect to the parameters $a_1, c_1, a_2,$ and $c_2$ are:</p> <ol> <li><p>With respect to $a_1$: $$\\frac{\\partial \\mu}{\\partial a_1} = (x-c_1) \\cdot (1 - f_1(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $c_1$: $$\\frac{\\partial \\mu}{\\partial c_1} = -a_1 \\cdot (1 - f_1(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $a_2$: $$\\frac{\\partial \\mu}{\\partial a_2} = (x-c_2) \\cdot (1 - f_2(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $c_2$: $$\\frac{\\partial \\mu}{\\partial c_2} = -a_2 \\cdot (1 - f_2(x)) \\cdot \\mu(x)$$</p> </li> </ol>"},{"location":"membership_functions/sigmoidal-product/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/sigmoidal-product/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the ProdSigmoidalMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/sigmoidal/","title":"Sigmoidal","text":"<p>The Sigmoidal Membership Function is defined by the logistic function:</p> <p>$$\\mu(x) = \\frac{1}{1 + e^{-a(x - c)}}$$</p> <p>The function is characterized by two parameters:</p> <ul> <li><p>a: Slope parameter - controls the steepness of the S-curve</p> <ul> <li>Positive values: standard sigmoid (0 \u2192 1 as x increases)</li> <li>Negative values: inverted sigmoid (1 \u2192 0 as x increases)</li> <li>Larger |a|: steeper transition (more abrupt change)</li> <li>Smaller |a|: gentler transition (more gradual change)</li> </ul> </li> <li><p>c: Center parameter - controls the inflection point where \u03bc(c) = 0.5</p> <ul> <li>\u03bc(c) = 0.5 (50% membership)</li> <li>Shifts the curve left/right along the x-axis</li> </ul> </li> <li><p>a \u2260 0: Cannot be zero (would result in constant function \u03bc(x) = 0.5)</p> </li> <li><p>a and c can be any real numbers otherwise</p> </li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import SigmoidalMF\n\nsigmoid = SigmoidalMF(a=2, c=0)\n\nx = np.linspace(-5, 5, 200)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import SigmoidalMF  sigmoid = SigmoidalMF(a=2, c=0)  x = np.linspace(-5, 5, 200) y = sigmoid(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal/#sigmoidal","title":"Sigmoidal\u00b6","text":"<p>The Sigmoidal Membership Function implements a smooth S-shaped curve that transitions gradually from 0 to 1. It is widely used in fuzzy logic systems and neural networks for modeling smooth transitions and gradual changes.</p>"},{"location":"membership_functions/sigmoidal/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The sigmoid function has elegant derivative properties:</p> Derivative of the Sigmoid Function For \u03bc(x) = 1/(1 + e^{-a(x-c)}), the derivative with respect to x is:  <p>$$\\frac{d\\mu}{dx} = \\mu(x) \\cdot (1 - \\mu(x)) \\cdot a$$</p> <p>This is a fundamental property of the sigmoid function and is used extensively in neural networks.</p> Partial Derivatives w.r.t. Parameters <p>For \u03bc(x) = 1/(1 + exp(-a(x-c))):</p> <ul> <li>\u2202\u03bc/\u2202a = \u03bc(x) \u00b7 (1 - \u03bc(x)) \u00b7 (x - c)</li> <li>\u2202\u03bc/\u2202c = -a \u00b7 \u03bc(x) \u00b7 (1 - \u03bc(x))</li> </ul>"},{"location":"membership_functions/sigmoidal/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a sigmoidal membership function and visualize it:</p>"},{"location":"membership_functions/sigmoidal/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different sigmoidal membership functions with varying parameter combinations. Each subplot demonstrates how the slope (a) and center (c) parameters affect the shape of the S-curve.</p>"},{"location":"membership_functions/sshaped-linear/","title":"Linear S-shaped","text":"<p>The mathematical formula for the linear S-shaped membership function is given by:</p> <ul> <li>$\u03bc(x) = 0$, for $x \\le a$</li> <li>$\u03bc(x) = (x - a) / (b - a)$, for $a &lt; x &lt; b$</li> <li>$\u03bc(x) = 1$, for $x \\ge b$</li> </ul> <p>Where:</p> <ul> <li>$\u03bc(x)$ is the degree of membership for element $x$ in the fuzzy set.</li> <li>$a$ and $b$ are the parameters that define the start and end of the linear ramp.</li> </ul> <p>The function is characterized by two main parameters:</p> <ul> <li><code>a</code> (start point): The \"left foot\" of the curve. This is the point where the membership transition begins from 0.</li> <li><code>b</code> (end point): The \"right shoulder\" of the curve. This is the point where the membership reaches and stays at 1. The parameter <code>b</code> must always be greater than <code>a</code>.</li> </ul> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import LinSShapedMF\n\nmf = LinSShapedMF(a=3, b=7)\n\nx = np.linspace(0, 10, 100)\ny = mf.forward(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import LinSShapedMF  mf = LinSShapedMF(a=3, b=7)  x = np.linspace(0, 10, 100) y = mf.forward(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sshaped-linear/#linear-s-shaped","title":"Linear S-shaped\u00b6","text":"<p>The linear S-shaped membership function (<code>LinSShapedMF</code>) is a fundamental concept in fuzzy logic, used to define fuzzy sets. Unlike a classical set where an element either fully belongs or doesn't, a fuzzy set allows for partial membership, and the <code>LinSShapedMF</code> provides a smooth, continuous way to represent this degree of belonging.</p> <p>It's a piecewise linear function that transitions from 0 to 1 over a defined interval.</p>"},{"location":"membership_functions/sshaped-linear/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives of the membership function are crucial for optimization algorithms, such as backpropagation, which allow the system to adapt. They indicate how the membership value changes in response to small adjustments in the parameters <code>a</code> and <code>b</code>.</p> Derivative with respect to `a` ($\\frac{\\partial \\mu}{\\partial a}$) <p>The partial derivative with respect to <code>a</code> indicates how the membership value is affected when the starting point of the ramp is adjusted.</p> <p>$\\frac{\\partial \\mu}{\\partial a} = -\\frac{1}{b-a}$ (for the ramp region)</p> Derivative with respect to `b` ($\\frac{\\partial \\mu}{\\partial b}$) <p>The partial derivative with respect to <code>b</code> shows how the membership value changes when the end point of the ramp is adjusted.</p> <p>$\\frac{\\partial \\mu}{\\partial b} = \\frac{x-a}{(b-a)^2}$ (for the ramp region)</p>"},{"location":"membership_functions/sshaped-linear/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of the S-shaped membership function, showing how its shape is influenced by the parameters <code>a</code> and <code>b</code>.</p>"},{"location":"membership_functions/sshaped/","title":"S-shaped","text":"<p>The S-shaped function's curve is defined by two main parameters:</p> <ul> <li>$a$: The point where the function begins to rise from a membership degree of 0.0.</li> <li>$b$: The point where the function reaches a membership degree of 1.0.</li> </ul> <p>The transition between these two points is described by the following equation:</p> <p>$$ S(x; a, b) = \\begin{array}{ll} 0 &amp; x \\le a \\\\[6pt] 2 \\left( \\frac{x-a}{b-a} \\right)^2 &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt] 1 - 2 \\left( \\frac{x-b}{b-a} \\right)^2 &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt] 1 &amp; x \\ge b \\end{array}{ll} $$</p> <p>This formulation ensures a smooth and continuous transition between the different segments of the curve, which is fundamental for representing uncertainties and imprecision in fuzzy logic systems.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom anfis_toolbox.membership import SShapedMF\n\n\nmf = SShapedMF(a=2, b=8)\n\nx = np.linspace(0, 10, 200)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from anfis_toolbox.membership import SShapedMF   mf = SShapedMF(a=2, b=8)  x = np.linspace(0, 10, 200) y = mf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sshaped/#s-shaped","title":"S-shaped\u00b6","text":"<p>The S-shaped Membership Function (<code>SShapedMF</code>) is a type of membership function used in fuzzy logic. It models the gradual transition from a zero degree of membership (0.0) to a full degree (1.0). This form is ideal for representing concepts like \"hot\" or \"fast,\" where membership starts low and progressively increases to a certain point. The transition is defined by a cubic polynomial, resulting in a smooth, continuous curve without angular points.</p>"},{"location":"membership_functions/sshaped/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>To optimize the shape of the S-shaped membership function for a specific application, it's often necessary to calculate the partial derivatives with respect to its parameters, $a$ and $b$. These derivatives are crucial for optimization algorithms like gradient descent.</p> Partial Derivative with Respect to $a$ <p>The partial derivative of the S-shaped function with respect to the parameter $a$ is calculated as follows:</p> <p>$$ \\frac{\\partial S}{\\partial a} = \\begin{array}{ll}   0 &amp; x \\le a \\\\[6pt]   \\frac{-4(x-a)}{(b-a)^2} + \\frac{4(x-a)^2}{(b-a)^3} &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt]   \\frac{4(x-b)^2}{(b-a)^3} &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt]   0 &amp; x \\ge b \\end{array} $$</p> <p>This derivative shows how the membership value changes as the starting point of the curve, $a$, is adjusted.</p> Partial Derivative with Respect to $b$ <p>The partial derivative of the S-shaped function with respect to the parameter $b$ is calculated as follows:</p> <p>$$ \\frac{\\partial S}{\\partial b} = \\begin{array}{ll}   0 &amp; x \\le a \\\\[6pt]   \\frac{-4(x-a)^2}{(b-a)^3} &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt]   \\frac{-4(x-b)}{(b-a)^2} + \\frac{4(x-b)^2}{(b-a)^3} &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt]   0 &amp; x \\ge b \\end{array} $$</p> <p>This derivative indicates how the membership value changes as the ending point of the curve, $b$, is adjusted.</p> <p>These partial derivatives are essential tools for tuning the S-shaped membership function to better fit data or to meet specific system requirements. They enable gradient-based optimization by providing the direction and magnitude of the steepest ascent/descent for the parameters.</p>"},{"location":"membership_functions/sshaped/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/sshaped/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of the S-shaped membership function, showing how its shape is influenced by the parameters <code>a</code> and <code>b</code>.</p>"},{"location":"membership_functions/trapezoidal/","title":"Trapezoidal","text":"<p>The Trapezoidal Membership Function is defined by the piecewise linear equation:</p> <p>$$\\mu(x) =  \\begin{array}{ll} 0 &amp; x \\leq a \\\\ \\frac{x - a}{b - a} &amp; a &lt; x &lt; b \\\\ 1 &amp; b \\leq x \\leq c \\\\ \\frac{d - x}{d - c} &amp; c &lt; x &lt; d \\\\ 0 &amp; x \\geq d  \\end{array}$$</p> <p>The function is characterized by four parameters:</p> <ul> <li>a: Left base point (lower support bound) - where \u03bc(x) starts increasing from 0</li> <li>b: Left peak point (start of plateau) - where \u03bc(x) reaches 1</li> <li>c: Right peak point (end of plateau) - where \u03bc(x) starts decreasing from 1</li> <li>d: Right base point (upper support bound) - where \u03bc(x) returns to 0</li> </ul> <p>Parameter Constraints For a valid trapezoidal function, parameters must satisfy: a \u2264 b \u2264 c \u2264 d</p> <p>Geometric Interpretation</p> <ul> <li>The region [a, b] forms the left slope (rising edge)</li> <li>The region [b, c] forms the plateau (full membership region)</li> <li>The region [c, d] forms the right slope (falling edge)</li> <li>Outside [a, d], membership is zero</li> </ul> <p>This shape is particularly useful when you need:</p> <ul> <li>A stable region of full membership (plateau)</li> <li>Gradual transitions at the boundaries</li> <li>Robustness to small variations in input values</li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import TrapezoidalMF\n\ntrapezoidal = TrapezoidalMF(a=2, b=4, c=6, d=8)\n\n\nx = np.linspace(0, 10, 200)\ny = trapezoidal(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import TrapezoidalMF  trapezoidal = TrapezoidalMF(a=2, b=4, c=6, d=8)   x = np.linspace(0, 10, 200) y = trapezoidal(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/trapezoidal/#trapezoidal","title":"Trapezoidal\u00b6","text":"<p>The Trapezoidal Membership Function is a piecewise linear function that creates a trapezoid-shaped membership curve. It is widely used in fuzzy logic systems when you need a plateau region of full membership, providing robustness to noise and uncertainty.</p>"},{"location":"membership_functions/trapezoidal/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The derivatives are computed analytically for each region:</p> Left Slope Region (a &lt; x &lt; b) $$\\mu(x) = \\frac{x - a}{b - a}$$  <ul> <li>\u2202\u03bc/\u2202a = -1/(b-a)</li> <li>\u2202\u03bc/\u2202b = -(x-a)/(b-a)\u00b2</li> <li>\u2202\u03bc/\u2202c = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202d = 0 (no effect in this region)</li> </ul> Plateau Region (b \u2264 x \u2264 c) $$\\mu(x) = 1$$  <ul> <li>\u2202\u03bc/\u2202a = 0 (constant function)</li> <li>\u2202\u03bc/\u2202b = 0 (constant function)</li> <li>\u2202\u03bc/\u2202c = 0 (constant function)</li> <li>\u2202\u03bc/\u2202d = 0 (constant function)</li> </ul> Right Slope Region (c &lt; x &lt; d) $$\\mu(x) = \\frac{d - x}{d - c}$$  <ul> <li>\u2202\u03bc/\u2202a = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202b = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202c = (x-d)/(d-c)\u00b2</li> <li>\u2202\u03bc/\u2202d = (x-c)/(d-c)\u00b2</li> </ul>"},{"location":"membership_functions/trapezoidal/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a trapezoidal membership function and visualize it:</p>"},{"location":"membership_functions/trapezoidal/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different trapezoidal membership functions with varying parameter combinations. Each subplot demonstrates how the shape changes with different plateau widths and slope characteristics.</p>"},{"location":"membership_functions/triangular/","title":"Triangular","text":"<p>The Triangular membership function (TriangularMF) is one of the most fundamental and widely used membership functions in fuzzy logic. It represents fuzzy sets with a simple triangular shape, making it intuitive and computationally efficient. The function is defined by three key points that form the triangle: the left base point, the peak, and the right base point.</p> <p>The function is characterized by three main parameters:</p> <ul> <li>a: Left base point of the triangle (\u03bc(x) = 0 for x \u2264 a).</li> <li>b: Peak point of the triangle (\u03bc(b) = 1, the maximum membership value).</li> <li>c: Right base point of the triangle (\u03bc(x) = 0 for x \u2265 c).</li> </ul> <p>These parameters must satisfy the constraint: a \u2264 b \u2264 c.</p> <p>The mathematical formula for the triangular membership function is defined piecewise:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 0, &amp; x \\leq a, \\\\[4pt] \\dfrac{x-a}{b-a}, &amp; a &lt; x \\leq b, \\\\[4pt] \\dfrac{c-x}{c-b}, &amp; b &lt; x &lt; c, \\\\[4pt] 0, &amp; x \\geq c. \\end{array} $$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$a$ is the left base point.</li> <li>$b$ is the peak point.</li> <li>$c$ is the right base point.</li> </ul> <p>The triangular membership function is particularly useful for representing concepts like \"approximately equal to b\" or \"around b\", where the membership decreases linearly as we move away from the peak value.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import TriangularMF\n\ntriangular = TriangularMF(a=2, b=5, c=8)\n\nx = np.linspace(0, 10, 100)\ny = triangular(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import TriangularMF  triangular = TriangularMF(a=2, b=5, c=8)  x = np.linspace(0, 10, 100) y = triangular(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/triangular/#triangular","title":"Triangular\u00b6","text":""},{"location":"membership_functions/triangular/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the triangular membership function are essential for optimization in adaptive fuzzy systems. Since the function is piecewise linear, the derivatives are computed separately for each region.</p> Derivative with respect to $a$ <p>For $a &lt; x &lt; b$: $$\\frac{\\partial \\mu}{\\partial a} = \\frac{x-b}{(b-a)^2}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $b$ <p>For $a &lt; x &lt; b$: $$\\frac{\\partial \\mu}{\\partial b} = \\frac{-(x-a)}{(b-a)^2}$$</p> <p>For $b &lt; x &lt; c$: $$\\frac{\\partial \\mu}{\\partial b} = \\frac{x-c}{(c-b)^2}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $c$ <p>For $b &lt; x &lt; c$: $$\\frac{\\partial \\mu}{\\partial c} = \\frac{x-b}{(c-b)^2}$$</p> <p>For other regions: The derivative is 0.</p> <p>These derivatives enable gradient-based optimization of the triangular membership function parameters, allowing the triangle to adapt its shape during training to better fit the data.</p>"},{"location":"membership_functions/triangular/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/triangular/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the TriangularMF shape changes with different parameter combinations. We'll explore variations in the base points (a, c) and peak position (b).</p>"},{"location":"membership_functions/zshaped-linear/","title":"Linear Z-shaped","text":"<p>The formula for the <code>LinZShapedMF</code> is a piecewise linear function:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 1, &amp; x \\le a \\\\[6pt] \\frac{b - x}{b - a}, &amp; a &lt; x &lt; b \\\\[6pt] 0, &amp; x \\ge b \\end{array} $$</p> <p>Where $\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</p> <p>The function is defined by two parameters that delimit the linear transition region:</p> <ul> <li><code>a</code> (Left Shoulder): The point where the membership value begins to transition from 1.0. For input values less than or equal to <code>a</code>, the membership is always 1.0.</li> <li><code>b</code> (Right Foot): The point where the membership value reaches and stays at zero. For input values greater than or equal to <code>b</code>, the membership is always 0.0.</li> </ul> <p>It is crucial that the parameter <code>a</code> is less than <code>b</code> for the linear transition to occur correctly.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import LinZShapedMF\n\nmf = LinZShapedMF(a=15, b=35)\n\nx = np.linspace(0, 50, 100)\ny = mf.forward(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import LinZShapedMF  mf = LinZShapedMF(a=15, b=35)  x = np.linspace(0, 50, 100) y = mf.forward(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/zshaped-linear/#linear-z-shaped","title":"Linear Z-shaped\u00b6","text":"<p>The Linear Z-shaped Membership Function (<code>LinZShapedMF</code>) is a type of membership function in fuzzy logic that represents a smooth linear transition from a full degree of membership (1.0) to zero. Its shape is ideal for modeling concepts like \"cold\" or \"low,\" where membership is high up to a certain point and then decreases linearly to zero.</p>"},{"location":"membership_functions/zshaped-linear/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives are essential for optimizing the parameters <code>a</code> and <code>b</code> in adaptive fuzzy systems.</p> Derivative with respect to `a` <p>$$\\frac{\\partial \\mu}{\\partial a} = \\frac{b - x}{(b - a)^2}$$</p> Derivative with respect to `b` <p>$$\\frac{\\partial \\mu}{\\partial b} = -\\frac{x - a}{(b - a)^2}$$</p>"},{"location":"membership_functions/zshaped-linear/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/zshaped-linear/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the LinZShapedMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/zshaped/","title":"Z-shaped","text":"<p>The function is defined by two key parameters that delimit the transition region:</p> <ul> <li><code>a</code> (Left Shoulder): The point where the transition from a full degree of membership (1.0) begins. For input values less than or equal to <code>a</code>, the membership is always 1.0.</li> <li><code>b</code> (Right Foot): The point where the transition ends and the degree of membership becomes zero. For input values greater than or equal to <code>b</code>, the membership is always 0.0.</li> </ul> <p>It is crucial that <code>a</code> is less than <code>b</code> for the transition to occur correctly.</p> <p>The formula for the <code>ZShapedMF</code> is based on the smoothstep function, a third-degree polynomial. The function is defined in parts:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 1, &amp; x \\leq a, \\\\[6pt] 1 - \\bigl(3t^{2} - 2t^{3}\\bigr),  &amp; a &lt; x &lt; b \\\\[6pt] 0, &amp; x \\geq b. \\end{array} $$</p> <p>Where:</p> <p>$\\mu(x)$ is the degree of membership of element $x$ and $t = \\dfrac{x-a}{\\,b-a\\,}$.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom anfis_toolbox.membership import ZShapedMF\n\nmf = ZShapedMF(a=15, b=25)\n\nx = np.linspace(0, 40, 100)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from anfis_toolbox.membership import ZShapedMF  mf = ZShapedMF(a=15, b=25)  x = np.linspace(0, 40, 100) y = mf(x)  plt.plot(x, y) plt.show() <p>Below is a comprehensive visualization showing how the ZShapedMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/zshaped/#z-shaped","title":"Z-shaped\u00b6","text":"<p>The Z-shaped Membership Function (<code>ZShapedMF</code>) is a fundamental type of membership function in fuzzy logic. It provides a smooth and continuous transition from a full degree of membership (1.0) to zero. This form is ideal for modeling concepts like \"cold\" or \"slow,\" where membership is high up to a certain point and then decreases gradually. The transition is defined using a cubic polynomial, resulting in a smooth curve without angular points.</p>"},{"location":"membership_functions/zshaped/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives are crucial for optimizing the parameters <code>a</code> and <code>b</code> in adaptive fuzzy systems. They show how the function's output changes in response to small changes in these parameters.</p> Derivative with respect to `a` ($\\frac{\\partial \\mu}{\\partial a}$) <p>This derivative indicates how the degree of membership is affected by adjusting the starting point of the transition.</p> <p>$$\\frac{\\partial \\mu}{\\partial a} = \\frac{\\partial \\mu}{\\partial t} \\frac{\\partial t}{\\partial a} = - (6t(t-1)) \\cdot \\frac{x-b}{(b-a)^2}$$</p> Derivative with respect to `b` ($\\frac{\\partial \\mu}{\\partial b}$) <p>This derivative indicates how the degree of membership is affected by adjusting the endpoint of the transition.</p> <p>$$\\frac{\\partial \\mu}{\\partial b} = \\frac{\\partial \\mu}{\\partial t} \\frac{\\partial t}{\\partial b} = - (6t(t-1)) \\cdot - \\frac{x-a}{(b-a)^2}$$</p>"},{"location":"membership_functions/zshaped/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/zshaped/#visualization","title":"Visualization\u00b6","text":""}]}