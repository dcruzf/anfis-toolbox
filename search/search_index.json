{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"ANFIS Toolbox <p> The most user-friendly Python library for Adaptive Neuro-Fuzzy Inference Systems (ANFIS) </p> <p>ANFIS Toolbox is a comprehensive Python library for creating, training, and deploying Adaptive Neuro-Fuzzy Inference Systems (ANFIS). It provides an intuitive API that makes fuzzy neural networks accessible to both beginners and experts.</p> \ud83d\udd17 GitHub | \ud83d\udce6 PyPI"},{"location":"#key-features","title":"Key Features","text":"\u2728 Easy to Use         Get started with just 3 lines of code               \ud83c\udfd7\ufe0f Flexible Architecture         13 membership functions, hybrid learning               \ud83d\udcca Built-in Visualization         Automatic plots for training and results               \u2705 Robust Validation         Cross-validation, metrics, model comparison               \ud83d\udcda Rich Documentation         Comprehensive examples and tutorials               \ud83d\udd27 Production Ready         Model persistence and configuration management"},{"location":"#why-anfis-toolbox","title":"Why ANFIS Toolbox?","text":""},{"location":"#simplicity-first","title":"\ud83d\ude80 Simplicity First","text":"<p>Most fuzzy logic libraries require extensive boilerplate code. ANFIS Toolbox gets you running in seconds:</p> <pre><code># Traditional approach (10+ lines)\ninput_mfs = {\n    'x1': [GaussianMF(-1, 1), GaussianMF(1, 1)],\n    'x2': [GaussianMF(-1, 1), GaussianMF(1, 1)]\n}\nmodel = ANFIS(input_mfs)\n# ... manual setup ...\n\n# ANFIS Toolbox approach (1 line)\nmodel = QuickANFIS.for_regression(X)\n</code></pre>"},{"location":"#validation-made-easy-built-in","title":"\u2705 Validation Made Easy (Built-in)","text":"<p>Comprehensive model evaluation with minimal code:</p> <pre><code>from anfis_toolbox import ANFISValidator\n\nvalidator = ANFISValidator(model)\n\n# Cross-validation\ncv_results = validator.cross_validate(X, y, cv=5)\nprint(f\"CV R\u00b2: {cv_results['r2_mean']:.4f} \u00b1 {cv_results['r2_std']:.4f}\")\n\n# Learning curves\nlearning_data = validator.learning_curve(X, y)\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nfrom anfis_toolbox import QuickANFIS, quick_evaluate\n\n# 1. Prepare your data\nX = np.random.uniform(-2, 2, (100, 2))  # 2 inputs\ny = X[:, 0]**2 + X[:, 1]**2  # Target: x1\u00b2 + x2\u00b2\n\n# 2. Create and train model (one line!)\nmodel = QuickANFIS.for_regression(X, n_mfs=3)\nlosses = model.fit_hybrid(X, y, epochs=50)\n\n# 3. Evaluate and use\nmetrics = quick_evaluate(model, X, y)\npredictions = model.predict([[1.0, -0.5], [0.5, 1.2]])\n\nprint(f\"R\u00b2 Score: {metrics['r2']:.4f}\")\n</code></pre> <p>That's it! \ud83c\udf89 You just created and trained a neuro-fuzzy system!</p>"},{"location":"#installation","title":"Installation","text":"Basic InstallationFull Installation <p>Install the core package with minimal dependencies:</p> <pre><code>pip install anfis-toolbox\n</code></pre> <p>Install with all features (visualization):</p> <pre><code>pip install anfis-toolbox[all]\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"Application Description Code Example Function Approximation Learn complex mathematical functions <code>QuickANFIS.for_function_approximation([(-\u03c0, \u03c0)])</code> Regression Predict continuous values <code>QuickANFIS.for_regression(X)</code> Control Systems Design fuzzy controllers Custom MF setup for error/error-rate Time Series Forecast future values Multi-lag input configuration Pattern Recognition Classify with fuzzy boundaries Post-process regression outputs"},{"location":"#architecture","title":"Architecture","text":"<p>ANFIS Toolbox implements the complete 4-layer ANFIS architecture:</p> <pre><code>graph LR\n    A[Input Layer] --&gt; B[Membership Layer]\n    B --&gt; C[Rule Layer]\n    C --&gt; D[Normalization Layer]\n    D --&gt; E[Consequent Layer]\n    E --&gt; F[Output]</code></pre>"},{"location":"#supported-membership-functions","title":"Supported Membership Functions","text":"<ul> <li>Gaussian (<code>GaussianMF</code>) - Smooth bell curves</li> <li>Gaussian2 (<code>Gaussian2MF</code>) - Two-sided Gaussian with flat region</li> <li>Triangular (<code>TriangularMF</code>) - Simple triangular shapes</li> <li>Trapezoidal (<code>TrapezoidalMF</code>) - Plateau regions</li> <li>Bell-shaped (<code>BellMF</code>) - Generalized bell curves</li> <li>Sigmoidal (<code>SigmoidalMF</code>) - S-shaped transitions</li> <li>Diff-Sigmoidal (<code>DiffSigmoidalMF</code>) - Difference of two sigmoids</li> <li>Prod-Sigmoidal (<code>ProdSigmoidalMF</code>) - Product of two sigmoids</li> <li>S-shaped (<code>SShapedMF</code>) - Smooth S-curve transitions</li> <li>Linear S-shaped (<code>LinSShapedMF</code>) - Piecewise linear S-curve</li> <li>Z-shaped (<code>ZShapedMF</code>) - Smooth Z-curve transitions</li> <li>Linear Z-shaped (<code>LinZShapedMF</code>) - Piecewise linear Z-curve</li> <li>Pi-shaped (<code>PiMF</code>) - Bell with flat top</li> </ul>"},{"location":"#training-methods","title":"Training Methods","text":"<ul> <li>Hybrid Learning (recommended) - Combines least squares + backpropagation</li> <li>Pure Backpropagation - Full gradient-based training</li> <li>Analytical Gradients - Fast and accurate derivative computation</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>\ud83d\udca1 Examples - Real-world use cases</li> <li>\ud83d\udd27 API Reference - Complete function documentation</li> <li>\ud83e\udd16 ANFIS Models - Regression and classification models</li> <li>\ud83d\udcd0 Membership Functions - All MF classes</li> <li>\ud83d\udd0d Fuzzy C-Means - Clustering for MF initialization</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udc1b Report Issues - Bug reports and feature requests</li> <li>\ud83d\udcac Discussions - Questions and community chat</li> <li>\u2b50 Star on GitHub - Show your support!</li> </ul> Ready to dive into fuzzy neural networks? Get started now \u2192"},{"location":"api/","title":"API Reference","text":"<p>Complete reference documentation for all ANFIS Toolbox classes, functions, and modules.</p>"},{"location":"api/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":""},{"location":"api/#main-model-classes","title":"Main Model Classes","text":"Class Purpose Module <code>ANFIS</code> Core ANFIS implementation for regression tasks models <code>ANFISClassifier</code> ANFIS for multi-class classification models"},{"location":"api/#builder-classes","title":"Builder Classes","text":"Class Purpose Module <code>QuickANFIS</code> Simplified API for common use cases builders <code>ANFISBuilder</code> Fluent API for custom model construction builders"},{"location":"api/#neural-network-layers","title":"Neural Network Layers","text":"Class Purpose Module <code>MembershipLayer</code> Fuzzy membership function layer layers <code>RuleLayer</code> Fuzzy rule firing strength computation layers <code>NormalizationLayer</code> Rule activation normalization layers <code>ConsequentLayer</code> Takagi-Sugeno consequent computation layers <code>ClassificationConsequentLayer</code> Classification-specific consequent layer layers"},{"location":"api/#membership-functions","title":"\ud83c\udfaf Membership Functions","text":"<p>Complete set of 13 fuzzy membership function implementations:</p> Function Type Parameters Module <code>GaussianMF</code> Gaussian <code>mean</code>, <code>sigma</code> membership-functions <code>Gaussian2MF</code> Two-sided Gaussian <code>sigma1</code>, <code>c1</code>, <code>sigma2</code>, <code>c2</code> membership-functions <code>TriangularMF</code> Triangular <code>a</code>, <code>b</code>, <code>c</code> membership-functions <code>TrapezoidalMF</code> Trapezoidal <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> membership-functions <code>BellMF</code> Bell-shaped <code>a</code>, <code>b</code>, <code>c</code> membership-functions <code>SigmoidalMF</code> Sigmoidal <code>a</code>, <code>c</code> membership-functions <code>DiffSigmoidalMF</code> Difference of sigmoids <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code> membership-functions <code>ProdSigmoidalMF</code> Product of sigmoids <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code> membership-functions <code>SShapedMF</code> S-shaped <code>a</code>, <code>b</code> membership-functions <code>LinSShapedMF</code> Linear S-shaped <code>a</code>, <code>b</code> membership-functions <code>ZShapedMF</code> Z-shaped <code>a</code>, <code>b</code> membership-functions <code>LinZShapedMF</code> Linear Z-shaped <code>a</code>, <code>b</code> membership-functions <code>PiMF</code> Pi-shaped <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> membership-functions"},{"location":"api/#training-optimization","title":"\ud83d\udd27 Training &amp; Optimization","text":""},{"location":"api/#training-algorithms","title":"Training Algorithms","text":"Trainer Method Module <code>HybridTrainer</code> Least squares + backpropagation (recommended) optim <code>SGDTrainer</code> Stochastic gradient descent optim <code>AdamTrainer</code> Adaptive moment estimation optim <code>RMSPropTrainer</code> Root mean square propagation optim <code>PSOTrainer</code> Particle swarm optimization optim"},{"location":"api/#loss-functions","title":"Loss Functions","text":"Function Purpose Module <code>mse_loss</code> Mean squared error for regression losses <code>mse_grad</code> MSE gradient computation losses <code>cross_entropy_loss</code> Cross-entropy for classification losses <code>cross_entropy_grad</code> Cross-entropy gradient computation losses"},{"location":"api/#evaluation-validation","title":"\ud83d\udcca Evaluation &amp; Validation","text":""},{"location":"api/#metrics","title":"Metrics","text":"<p>Comprehensive metrics for model evaluation:</p> Category Functions Module Regression MSE, RMSE, MAE, MAPE, SMAPE, R\u00b2, Pearson correlation, MSLE metrics Classification Cross-entropy, accuracy, softmax metrics Clustering Partition coefficient, classification entropy, Xie-Beni index metrics"},{"location":"api/#clustering","title":"\ud83d\udd0d Clustering","text":"Class Purpose Module <code>FuzzyCMeans</code> Fuzzy C-Means clustering algorithm clustering"},{"location":"api/#detailed-documentation","title":"\ud83d\udcda Detailed Documentation","text":""},{"location":"api/#by-category","title":"By Category","text":"<ul> <li>ANFIS Models - High-level model documentation</li> <li>Builders - Model construction utilities</li> <li>Membership Functions - All 13 MF implementations</li> <li>Models - Core ANFIS and ANFISClassifier classes</li> <li>Layers - Neural network layer implementations</li> <li>Clustering - FuzzyCMeans clustering</li> <li>Losses - Training loss functions and gradients</li> <li>Metrics - Performance evaluation metrics</li> <li>Optimization - Training algorithm implementations</li> </ul>"},{"location":"api/#search-and-navigation","title":"\ud83d\udd0d Search and Navigation","text":""},{"location":"api/#find-by-functionality","title":"Find by Functionality","text":"I want to... Look at... Create a simple model <code>QuickANFIS</code> in Builders Build custom architecture <code>ANFISBuilder</code> in Builders Choose membership functions Membership Functions Choose loss functions Losses Train my model <code>fit()</code> method in Models Evaluate performance Metrics Cluster data <code>FuzzyCMeans</code> in Clustering Configure training Optimization"},{"location":"api/#navigation","title":"Navigation","text":"<p>Start here for specific needs:</p> <ul> <li>\ud83d\ude80 New user? \u2192 ANFIS Models</li> <li>\ud83c\udfd7\ufe0f Building models? \u2192 Builders</li> <li>\ud83d\udcca Analyzing results? \u2192 Metrics</li> <li>\ud83d\udd0d Clustering? \u2192 Clustering</li> <li>\u2699\ufe0f Training? \u2192 Optimization</li> </ul>"},{"location":"api/builders/","title":"Builders","text":""},{"location":"api/builders/#anfis_toolbox.QuickANFIS","title":"anfis_toolbox.QuickANFIS","text":"<p>Quick setup class for common ANFIS use cases.</p>"},{"location":"api/builders/#anfis_toolbox.QuickANFIS.for_classification","title":"for_classification  <code>staticmethod</code>","text":"<pre><code>for_classification(\n    X: ndarray,\n    n_classes: int,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str = \"grid\",\n    random_state: int | None = None,\n) -&gt; ANFISClassifier\n</code></pre> <p>Create ANFISClassifier configured from data.</p> <p>Mirrors for_regression but returns a classifier with n_classes.</p>"},{"location":"api/builders/#anfis_toolbox.QuickANFIS.for_function_approximation","title":"for_function_approximation  <code>staticmethod</code>","text":"<pre><code>for_function_approximation(\n    input_ranges: list[tuple[float, float]], n_mfs: int = 5\n) -&gt; ANFIS\n</code></pre> <p>Create ANFIS model for function approximation.</p> <p>Parameters:</p> Name Type Description Default <code>input_ranges</code> <code>list[tuple[float, float]]</code> <p>List of (min, max) tuples for each input dimension</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions per input</p> <code>5</code> <p>Returns:</p> Type Description <code>ANFIS</code> <p>Configured ANFIS model</p>"},{"location":"api/builders/#anfis_toolbox.QuickANFIS.for_regression","title":"for_regression  <code>staticmethod</code>","text":"<pre><code>for_regression(\n    X: ndarray,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str = \"grid\",\n    random_state: int | None = None,\n) -&gt; ANFIS\n</code></pre> <p>Create ANFIS model automatically configured for regression data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input training data (n_samples, n_features)</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions per input</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Type of membership functions</p> <code>'gaussian'</code> <code>init</code> <code>str</code> <p>Initialization strategy per input: 'grid' (default) or 'fcm'.</p> <code>'grid'</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for deterministic FCM.</p> <code>None</code> <p>Returns:</p> Type Description <code>ANFIS</code> <p>Configured ANFIS model</p>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder","title":"anfis_toolbox.ANFISBuilder","text":"<pre><code>ANFISBuilder()\n</code></pre> <p>Builder class for creating ANFIS models with intuitive API.</p>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.add_input","title":"add_input","text":"<pre><code>add_input(\n    name: str,\n    range_min: float,\n    range_max: float,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n) -&gt; ANFISBuilder\n</code></pre> <p>Add an input variable with automatic membership function generation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the input variable</p> required <code>range_min</code> <code>float</code> <p>Minimum value of the input range</p> required <code>range_max</code> <code>float</code> <p>Maximum value of the input range</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions (default: 3)</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Type of membership functions. Supported: 'gaussian', 'gaussian2', 'triangular', 'trapezoidal', 'bell', 'sigmoidal', 'sshape', 'zshape', 'pi'</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor between adjacent MFs (0.0 to 1.0)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ANFISBuilder</code> <p>Self for method chaining</p>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.add_input_from_data","title":"add_input_from_data","text":"<pre><code>add_input_from_data(\n    name: str,\n    data: ndarray,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    init: str = \"grid\",\n    random_state: int | None = None,\n) -&gt; ANFISBuilder\n</code></pre> <p>Add an input inferring range_min/range_max from data with a margin.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Input name</p> required <code>data</code> <code>ndarray</code> <p>1D array-like samples for this input</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Membership function type (see add_input)</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor between adjacent MFs</p> <code>0.5</code> <code>margin</code> <code>float</code> <p>Fraction of (max-min) to pad on each side</p> <code>0.1</code> <code>init</code> <code>str</code> <p>Initialization strategy: \"grid\" (default) or \"fcm\". When \"fcm\", clusters from the data determine MF centers and widths (supports 'gaussian' and 'bell').</p> <code>'grid'</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for deterministic FCM initialization.</p> <code>None</code>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.build","title":"build","text":"<pre><code>build() -&gt; ANFIS\n</code></pre> <p>Build the ANFIS model with configured parameters.</p>"},{"location":"api/clustering/","title":"Clustering","text":""},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans","title":"anfis_toolbox.clustering.FuzzyCMeans","text":"<pre><code>FuzzyCMeans(\n    n_clusters: int,\n    m: float = 2.0,\n    max_iter: int = 300,\n    tol: float = 0.0001,\n    random_state: int | None = None,\n)\n</code></pre> <p>Fuzzy C-Means clustering.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters (&gt;= 2).</p> required <code>m</code> <code>float</code> <p>Fuzzifier (&gt; 1). Default 2.0.</p> <code>2.0</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations.</p> <code>300</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on centers.</p> <code>0.0001</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for reproducibility.</p> <code>None</code> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def __init__(\n    self,\n    n_clusters: int,\n    m: float = 2.0,\n    max_iter: int = 300,\n    tol: float = 1e-4,\n    random_state: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize FuzzyCMeans with hyperparameters.\"\"\"\n    if n_clusters &lt; 2:\n        raise ValueError(\"n_clusters must be &gt;= 2\")\n    if m &lt;= 1:\n        raise ValueError(\"m (fuzzifier) must be &gt; 1\")\n    self.n_clusters = int(n_clusters)\n    self.m = float(m)\n    self.max_iter = int(max_iter)\n    self.tol = float(tol)\n    self.random_state = random_state\n    self.cluster_centers_ = None\n    self.membership_ = None\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.classification_entropy","title":"classification_entropy","text":"<pre><code>classification_entropy() -&gt; float\n</code></pre> <p>Classification Entropy (CE). Lower is better (crisper).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def classification_entropy(self) -&gt; float:\n    \"\"\"Classification Entropy (CE). Lower is better (crisper).\"\"\"\n    if self.membership_ is None:\n        raise RuntimeError(\"Fit the model before calling classification_entropy().\")\n    return _ce(self.membership_)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.fit","title":"fit","text":"<pre><code>fit(X: ndarray) -&gt; FuzzyCMeans\n</code></pre> <p>Fit the FCM model.</p> <p>Sets cluster_centers_ (k,d) and membership_ (n,k).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; FuzzyCMeans:\n    \"\"\"Fit the FCM model.\n\n    Sets cluster_centers_ (k,d) and membership_ (n,k).\n    \"\"\"\n    X = self._check_X(X)\n    n, _ = X.shape\n    if n &lt; self.n_clusters:\n        raise ValueError(\"n_samples must be &gt;= n_clusters\")\n    U = self._init_membership(n)\n    m = self.m\n\n    def update_centers(Um: np.ndarray) -&gt; np.ndarray:\n        num = Um.T @ X  # (k,d)\n        den = np.maximum(Um.sum(axis=0)[:, None], 1e-12)\n        return num / den\n\n    Um = U**m\n    C = update_centers(Um)\n    for _ in range(self.max_iter):\n        d2 = np.maximum(self._pairwise_sq_dists(X, C), 1e-12)  # (n,k)\n        inv = d2 ** (-1.0 / (m - 1.0))\n        U_new = inv / np.sum(inv, axis=1, keepdims=True)\n        Um_new = U_new**m\n        C_new = update_centers(Um_new)\n        if np.max(np.linalg.norm(C_new - C, axis=1)) &lt; self.tol:\n            U, C = U_new, C_new\n            break\n        U, C = U_new, C_new\n    self.membership_ = U\n    self.cluster_centers_ = C\n    return self\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Fit and return hard labels via argmax of membership.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def fit_predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Fit and return hard labels via argmax of membership.\"\"\"\n    self.fit(X)\n    return self.predict(X)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.partition_coefficient","title":"partition_coefficient","text":"<pre><code>partition_coefficient() -&gt; float\n</code></pre> <p>Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def partition_coefficient(self) -&gt; float:\n    \"\"\"Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.\"\"\"\n    if self.membership_ is None:\n        raise RuntimeError(\"Fit the model before calling partition_coefficient().\")\n    return _pc(self.membership_)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return hard labels via argmax of predict_proba.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return hard labels via argmax of predict_proba.\"\"\"\n    U = self.predict_proba(X)\n    return np.argmax(U, axis=1)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return membership degrees for samples to clusters (rows sum to 1).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return membership degrees for samples to clusters (rows sum to 1).\"\"\"\n    if self.cluster_centers_ is None:\n        raise RuntimeError(\"Call fit() before predict_proba().\")\n    X = self._check_X(X)\n    C = self.cluster_centers_\n    m = self.m\n    d2 = np.maximum(self._pairwise_sq_dists(X, C), 1e-12)\n    inv = d2 ** (-1.0 / (m - 1.0))\n    return inv / np.sum(inv, axis=1, keepdims=True)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Alias for predict_proba.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Alias for predict_proba.\"\"\"\n    return self.predict_proba(X)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.xie_beni_index","title":"xie_beni_index","text":"<pre><code>xie_beni_index(X: ndarray) -&gt; float\n</code></pre> <p>Xie-Beni index (XB). Lower is better.</p> <p>XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def xie_beni_index(self, X: np.ndarray) -&gt; float:\n    \"\"\"Xie-Beni index (XB). Lower is better.\n\n    XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)\n    \"\"\"\n    if self.membership_ is None or self.cluster_centers_ is None:\n        raise RuntimeError(\"Fit the model before calling xie_beni_index().\")\n    X = self._check_X(X)\n    return _xb(X, self.membership_, self.cluster_centers_, m=self.m)\n</code></pre>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer","title":"anfis_toolbox.layers.MembershipLayer","text":"<pre><code>MembershipLayer(input_mfs: dict)\n</code></pre> <p>Membership layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This is the first layer of ANFIS that applies membership functions to input variables. Each input variable has multiple membership functions that transform crisp input values into fuzzy membership degrees.</p> <p>This layer serves as the fuzzification stage, converting crisp inputs into fuzzy sets that can be processed by subsequent ANFIS layers.</p> <p>Attributes:</p> Name Type Description <code>input_mfs</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.</p> <code>input_names</code> <code>list</code> <p>List of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_mfs</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.              Format: {input_name: [MembershipFunction, ...]}</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, input_mfs: dict):\n    \"\"\"Initializes the membership layer with input membership functions.\n\n    Parameters:\n        input_mfs (dict): Dictionary mapping input names to lists of membership functions.\n                         Format: {input_name: [MembershipFunction, ...]}\n    \"\"\"\n    self.input_mfs = input_mfs\n    self.input_names = list(input_mfs.keys())\n    self.n_inputs = len(input_mfs)\n    self.mf_per_input = [len(mfs) for mfs in input_mfs.values()]\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.membership_functions","title":"membership_functions  <code>property</code>","text":"<pre><code>membership_functions: dict\n</code></pre> <p>Alias for input_mfs to provide a standardized interface.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.</p>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.backward","title":"backward","text":"<pre><code>backward(gradients: dict)\n</code></pre> <p>Performs backward pass to compute gradients for membership functions.</p> <p>Parameters:</p> Name Type Description Default <code>gradients</code> <code>dict</code> <p>Dictionary mapping input names to gradient arrays.              Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Gradients are accumulated in the membership functions.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, gradients: dict):\n    \"\"\"Performs backward pass to compute gradients for membership functions.\n\n    Parameters:\n        gradients (dict): Dictionary mapping input names to gradient arrays.\n                         Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n\n    Returns:\n        None: Gradients are accumulated in the membership functions.\n    \"\"\"\n    # Propagate gradients to each membership function\n    for name in self.input_names:\n        mfs = self.input_mfs[name]\n        grad_array = gradients[name]  # (batch_size, n_mfs)\n\n        for mf_idx, mf in enumerate(mfs):\n            # Extract gradient for this specific membership function\n            mf_gradient = grad_array[:, mf_idx]  # (batch_size,)\n            # Propagate gradient to the membership function\n            mf.backward(mf_gradient)\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; dict\n</code></pre> <p>Performs forward pass to compute membership degrees for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data with shape (batch_size, n_inputs).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to membership degree arrays.  Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; dict:\n    \"\"\"Performs forward pass to compute membership degrees for all inputs.\n\n    Parameters:\n        x (np.ndarray): Input data with shape (batch_size, n_inputs).\n\n    Returns:\n        dict: Dictionary mapping input names to membership degree arrays.\n             Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n    \"\"\"\n    _batch_size = x.shape[0]\n    membership_outputs = {}\n\n    # Compute membership degrees for each input variable\n    for i, name in enumerate(self.input_names):\n        mfs = self.input_mfs[name]\n        # Apply each membership function to the i-th input\n        mu_values = []\n        for mf in mfs:\n            mu = mf(x[:, i])  # (batch_size,)\n            mu_values.append(mu)\n\n        # Stack membership values for all MFs of this input\n        membership_outputs[name] = np.stack(mu_values, axis=-1)  # (batch_size, n_mfs)\n\n    # Cache values for backward pass\n    self.last = {\"x\": x, \"membership_outputs\": membership_outputs}\n\n    return membership_outputs\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets all membership functions to their initial state.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets all membership functions to their initial state.\n\n    Returns:\n        None\n    \"\"\"\n    for name in self.input_names:\n        for mf in self.input_mfs[name]:\n            mf.reset()\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer","title":"anfis_toolbox.layers.RuleLayer","text":"<pre><code>RuleLayer(input_names: list, mf_per_input: list)\n</code></pre> <p>Rule layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer computes the rule strengths (firing strengths) by applying the T-norm (typically product) operation to the membership degrees of all input variables for each rule.</p> <p>This is the second layer of ANFIS that takes membership degrees from the MembershipLayer and computes rule activations.</p> <p>Attributes:</p> Name Type Description <code>input_names</code> <code>list</code> <p>List of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input.</p> <code>rules</code> <code>list</code> <p>List of all possible rule combinations.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_names</code> <code>list</code> <p>List of input variable names.</p> required <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input variable.</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, input_names: list, mf_per_input: list):\n    \"\"\"Initializes the rule layer with input configuration.\n\n    Parameters:\n        input_names (list): List of input variable names.\n        mf_per_input (list): Number of membership functions per input variable.\n    \"\"\"\n    self.input_names = input_names\n    self.n_inputs = len(input_names)\n    self.mf_per_input = mf_per_input\n    # Generate all possible rule combinations (Cartesian product)\n    self.rules = list(product(*[range(n) for n in self.mf_per_input]))\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer.backward","title":"backward","text":"<pre><code>backward(dL_dw: ndarray) -&gt; dict\n</code></pre> <p>Performs backward pass to compute gradients for membership functions.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dw</code> <code>ndarray</code> <p>Gradient of loss with respect to rule strengths.                Shape: (batch_size, n_rules)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to gradient arrays for membership functions.  Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dw: np.ndarray) -&gt; dict:\n    \"\"\"Performs backward pass to compute gradients for membership functions.\n\n    Parameters:\n        dL_dw (np.ndarray): Gradient of loss with respect to rule strengths.\n                           Shape: (batch_size, n_rules)\n\n    Returns:\n        dict: Dictionary mapping input names to gradient arrays for membership functions.\n             Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n    \"\"\"\n    batch_size = dL_dw.shape[0]\n    mu = self.last[\"mu\"]  # (batch_size, n_inputs, n_mfs)\n\n    # Initialize gradient accumulators for each input's membership functions\n    gradients = {}\n    for i, name in enumerate(self.input_names):\n        n_mfs = self.mf_per_input[i]\n        gradients[name] = np.zeros((batch_size, n_mfs))\n\n    # Compute gradients for each rule\n    for rule_idx, rule in enumerate(self.rules):\n        for input_idx, mf_idx in enumerate(rule):\n            name = self.input_names[input_idx]\n\n            # Compute partial derivative: d(rule_strength)/d(mu_ij)\n            # This is the product of all other membership degrees in the rule\n            other_factors = []\n            for j, j_mf in enumerate(rule):\n                if j == input_idx:\n                    continue  # Skip the current input\n                other_factors.append(mu[:, j, j_mf])\n\n            # Product of other factors (or 1 if no other factors)\n            partial = np.prod(other_factors, axis=0) if other_factors else np.ones(batch_size)\n\n            # Apply chain rule: dL/dmu = dL/dw * dw/dmu\n            gradients[name][:, mf_idx] += dL_dw[:, rule_idx] * partial\n\n    return gradients\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer.forward","title":"forward","text":"<pre><code>forward(membership_outputs: dict) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to compute rule strengths.</p> <p>Parameters:</p> Name Type Description Default <code>membership_outputs</code> <code>dict</code> <p>Dictionary mapping input names to membership degree arrays.                      Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Rule strengths with shape (batch_size, n_rules).</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, membership_outputs: dict) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to compute rule strengths.\n\n    Parameters:\n        membership_outputs (dict): Dictionary mapping input names to membership degree arrays.\n                                 Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n\n    Returns:\n        np.ndarray: Rule strengths with shape (batch_size, n_rules).\n    \"\"\"\n    # Convert membership outputs to array format for easier processing\n    mu_list = []\n    for name in self.input_names:\n        mu_list.append(membership_outputs[name])  # (batch_size, n_mfs)\n    mu = np.stack(mu_list, axis=1)  # (batch_size, n_inputs, n_mfs)\n\n    _batch_size = mu.shape[0]\n\n    # Compute rule activations (firing strengths)\n    rule_activations = []\n    for rule in self.rules:\n        rule_mu = []\n        # Get membership degree for each input in this rule\n        for input_idx, mf_idx in enumerate(rule):\n            rule_mu.append(mu[:, input_idx, mf_idx])  # (batch_size,)\n        # Apply T-norm (product) to get rule strength\n        rule_strength = np.prod(rule_mu, axis=0)  # (batch_size,)\n        rule_activations.append(rule_strength)\n\n    rule_activations = np.stack(rule_activations, axis=1)  # (batch_size, n_rules)\n\n    # Cache values for backward pass\n    self.last = {\"membership_outputs\": membership_outputs, \"mu\": mu, \"rule_activations\": rule_activations}\n\n    return rule_activations\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer","title":"anfis_toolbox.layers.NormalizationLayer","text":"<pre><code>NormalizationLayer()\n</code></pre> <p>Normalization layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer normalizes the rule strengths (firing strengths) to ensure they sum to 1.0 for each sample in the batch. This is a crucial step in ANFIS as it converts rule strengths to normalized rule weights.</p> <p>The normalization formula is: norm_w_i = w_i / sum(w_j for all j)</p> <p>Attributes:</p> Name Type Description <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the normalization layer.\"\"\"\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer.backward","title":"backward","text":"<pre><code>backward(dL_dnorm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs backward pass to compute gradients for original rule weights.</p> <p>The gradient computation uses the quotient rule for derivatives: If norm_w_i = w_i / sum_w, then: - d(norm_w_i)/d(w_i) = (sum_w - w_i) / sum_w\u00b2 - d(norm_w_i)/d(w_j) = -w_j / sum_w\u00b2 for j \u2260 i</p> <p>Parameters:</p> Name Type Description Default <code>dL_dnorm_w</code> <code>ndarray</code> <p>Gradient of loss with respect to normalized weights.                     Shape: (batch_size, n_rules)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Gradient of loss with respect to original weights.        Shape: (batch_size, n_rules)</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dnorm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs backward pass to compute gradients for original rule weights.\n\n    The gradient computation uses the quotient rule for derivatives:\n    If norm_w_i = w_i / sum_w, then:\n    - d(norm_w_i)/d(w_i) = (sum_w - w_i) / sum_w\u00b2\n    - d(norm_w_i)/d(w_j) = -w_j / sum_w\u00b2 for j \u2260 i\n\n    Parameters:\n        dL_dnorm_w (np.ndarray): Gradient of loss with respect to normalized weights.\n                                Shape: (batch_size, n_rules)\n\n    Returns:\n        np.ndarray: Gradient of loss with respect to original weights.\n                   Shape: (batch_size, n_rules)\n    \"\"\"\n    w = self.last[\"w\"]  # (batch_size, n_rules)\n    sum_w = self.last[\"sum_w\"]  # (batch_size, 1)\n\n    # Jacobian-vector product without building the full Jacobian:\n    # (J^T g)_j = (sum_w * g_j - (g \u00b7 w)) / sum_w^2\n    g = dL_dnorm_w  # (batch_size, n_rules)\n    s = sum_w  # (batch_size, 1)\n    gw_dot = np.sum(g * w, axis=1, keepdims=True)  # (batch_size, 1)\n    dL_dw = (s * g - gw_dot) / (s**2)  # (batch_size, n_rules)\n\n    return dL_dw\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer.forward","title":"forward","text":"<pre><code>forward(w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to normalize rule weights.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>Rule strengths with shape (batch_size, n_rules).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized rule weights with shape (batch_size, n_rules).        Each row sums to 1.0.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to normalize rule weights.\n\n    Parameters:\n        w (np.ndarray): Rule strengths with shape (batch_size, n_rules).\n\n    Returns:\n        np.ndarray: Normalized rule weights with shape (batch_size, n_rules).\n                   Each row sums to 1.0.\n    \"\"\"\n    # Add small epsilon to avoid division by zero\n    sum_w = np.sum(w, axis=1, keepdims=True) + 1e-8\n    norm_w = w / sum_w\n\n    # Cache values for backward pass\n    self.last = {\"w\": w, \"sum_w\": sum_w, \"norm_w\": norm_w}\n    return norm_w\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer","title":"anfis_toolbox.layers.ConsequentLayer","text":"<pre><code>ConsequentLayer(n_rules: int, n_inputs: int)\n</code></pre> <p>Consequent layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer implements the consequent part of fuzzy rules in ANFIS. Each rule has a linear consequent function of the form: f_i(x) = p_i * x_1 + q_i * x_2 + ... + r_i (TSK model)</p> <p>The final output is computed as a weighted sum: y = \u03a3(w_i * f_i(x)) where w_i are normalized rule weights</p> <p>Attributes:</p> Name Type Description <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>parameters</code> <code>ndarray</code> <p>Linear parameters for each rule with shape (n_rules, n_inputs + 1).                     Each row contains [p_i, q_i, ..., r_i] for rule i.</p> <code>gradients</code> <code>ndarray</code> <p>Accumulated gradients for parameters.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules.</p> required <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, n_rules: int, n_inputs: int):\n    \"\"\"Initializes the consequent layer with random linear parameters.\n\n    Parameters:\n        n_rules (int): Number of fuzzy rules.\n        n_inputs (int): Number of input variables.\n    \"\"\"\n    # Each rule has (n_inputs + 1) parameters: p_i, q_i, ..., r_i (including bias)\n    self.n_rules = n_rules\n    self.n_inputs = n_inputs\n    self.parameters = np.random.randn(n_rules, n_inputs + 1)\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Performs backward pass to compute gradients for parameters and inputs.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of loss with respect to layer output.                Shape: (batch_size, 1)</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(dL_dnorm_w, dL_dx) where: - dL_dnorm_w: Gradient w.r.t. normalized weights, shape (batch_size, n_rules) - dL_dx: Gradient w.r.t. input x, shape (batch_size, n_inputs)</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Performs backward pass to compute gradients for parameters and inputs.\n\n    Parameters:\n        dL_dy (np.ndarray): Gradient of loss with respect to layer output.\n                           Shape: (batch_size, 1)\n\n    Returns:\n        tuple: (dL_dnorm_w, dL_dx) where:\n            - dL_dnorm_w: Gradient w.r.t. normalized weights, shape (batch_size, n_rules)\n            - dL_dx: Gradient w.r.t. input x, shape (batch_size, n_inputs)\n    \"\"\"\n    X_aug = self.last[\"X_aug\"]  # (batch_size, n_inputs + 1)\n    norm_w = self.last[\"norm_w\"]  # (batch_size, n_rules)\n    f = self.last[\"f\"]  # (batch_size, n_rules)\n\n    batch_size = X_aug.shape[0]\n\n    # Compute gradients for consequent parameters\n    self.gradients = np.zeros_like(self.parameters)\n\n    for i in range(self.n_rules):\n        # Gradient of y_hat w.r.t. parameters of rule i: norm_w_i * x_aug\n        for b in range(batch_size):\n            self.gradients[i] += dL_dy[b, 0] * norm_w[b, i] * X_aug[b]\n\n    # Compute gradient of loss w.r.t. normalized weights\n    # dy/dnorm_w_i = f_i(x), so dL/dnorm_w_i = dL/dy * f_i(x)\n    dL_dnorm_w = dL_dy * f  # (batch_size, n_rules)\n\n    # Compute gradient of loss w.r.t. input x (for backpropagation to previous layers)\n    dL_dx = np.zeros((batch_size, self.n_inputs))\n\n    for b in range(batch_size):\n        for i in range(self.n_rules):\n            # dy/dx = norm_w_i * parameters_i[:-1] (excluding bias term)\n            dL_dx[b] += dL_dy[b, 0] * norm_w[b, i] * self.parameters[i, :-1]\n\n    return dL_dnorm_w, dL_dx\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray, norm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to compute the final ANFIS output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data with shape (batch_size, n_inputs).</p> required <code>norm_w</code> <code>ndarray</code> <p>Normalized rule weights with shape (batch_size, n_rules).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Final ANFIS output with shape (batch_size, 1).</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray, norm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to compute the final ANFIS output.\n\n    Parameters:\n        x (np.ndarray): Input data with shape (batch_size, n_inputs).\n        norm_w (np.ndarray): Normalized rule weights with shape (batch_size, n_rules).\n\n    Returns:\n        np.ndarray: Final ANFIS output with shape (batch_size, 1).\n    \"\"\"\n    batch_size = x.shape[0]\n\n    # Augment input with bias term (column of ones)\n    X_aug = np.hstack([x, np.ones((batch_size, 1))])  # (batch_size, n_inputs + 1)\n\n    # Compute consequent function f_i(x) for each rule\n    # f[b, i] = p_i * x[b, 0] + q_i * x[b, 1] + ... + r_i\n    f = X_aug @ self.parameters.T  # (batch_size, n_rules)\n\n    # Compute final output as weighted sum: y = \u03a3(w_i * f_i(x))\n    y_hat = np.sum(norm_w * f, axis=1, keepdims=True)  # (batch_size, 1)\n\n    # Cache values for backward pass\n    self.last = {\"X_aug\": X_aug, \"norm_w\": norm_w, \"f\": f}\n\n    return y_hat\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets gradients and cached values.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets gradients and cached values.\n\n    Returns:\n        None\n    \"\"\"\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer","title":"anfis_toolbox.layers.ClassificationConsequentLayer","text":"<pre><code>ClassificationConsequentLayer(\n    n_rules: int,\n    n_inputs: int,\n    n_classes: int,\n    random_state: int | None = None,\n)\n</code></pre> <p>Consequent layer that produces per-class logits for classification.</p> <p>Each rule i has a vector of class logits with a linear function of inputs: f_i(x) = W_i x + b_i, where W_i has shape (n_classes, n_inputs) and b_i (n_classes,). We store parameters as a single array of shape (n_rules, n_classes, n_inputs + 1).</p> <p>Parameters:</p> Name Type Description Default <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules in the layer.</p> required <code>n_inputs</code> <code>int</code> <p>Number of input features.</p> required <code>n_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>random_state</code> <code>int | None</code> <p>Random seed for parameter initialization.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_rules</code> <code>int</code> <p>Stores the number of fuzzy rules.</p> <code>n_inputs</code> <code>int</code> <p>Stores the number of input features.</p> <code>n_classes</code> <code>int</code> <p>Stores the number of output classes.</p> <code>parameters</code> <code>ndarray</code> <p>Randomly initialized parameters for each rule, class, and input (including bias).</p> <code>gradients</code> <code>ndarray</code> <p>Gradient values initialized to zeros, matching the shape of parameters.</p> <code>last</code> <code>dict</code> <p>Dictionary for storing intermediate results or state.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, n_rules: int, n_inputs: int, n_classes: int, random_state: int | None = None):\n    \"\"\"Initializes the layer with the specified number of rules, inputs, and classes.\n\n    Args:\n        n_rules (int): Number of fuzzy rules in the layer.\n        n_inputs (int): Number of input features.\n        n_classes (int): Number of output classes.\n        random_state (int | None): Random seed for parameter initialization.\n\n    Attributes:\n        n_rules (int): Stores the number of fuzzy rules.\n        n_inputs (int): Stores the number of input features.\n        n_classes (int): Stores the number of output classes.\n\n\n        parameters (np.ndarray): Randomly initialized parameters for each rule, class, and input (including bias).\n        gradients (np.ndarray): Gradient values initialized to zeros, matching the shape of parameters.\n        last (dict): Dictionary for storing intermediate results or state.\n    \"\"\"\n    self.n_rules = n_rules\n    self.n_inputs = n_inputs\n    self.n_classes = n_classes\n    if random_state is None:\n        self.parameters = np.random.randn(n_rules, n_classes, n_inputs + 1)\n    else:\n        rng = np.random.default_rng(random_state)\n        self.parameters = rng.normal(size=(n_rules, n_classes, n_inputs + 1))\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.backward","title":"backward","text":"<pre><code>backward(dL_dlogits: ndarray)\n</code></pre> <p>Computes the backward pass for the classification consequent layer.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dlogits: np.ndarray):\n    \"\"\"Computes the backward pass for the classification consequent layer.\"\"\"\n    X_aug = self.last[\"X_aug\"]  # (b, d+1)\n    norm_w = self.last[\"norm_w\"]  # (b, r)\n    f = self.last[\"f\"]  # (b, r, k)\n\n    # Gradients w.r.t. per-rule parameters\n    self.gradients = np.zeros_like(self.parameters)\n    # dL/df_{brk} = dL/dlogits_{bk} * norm_w_{br}\n    dL_df = dL_dlogits[:, None, :] * norm_w[:, :, None]  # (b, r, k)\n    # Accumulate over batch: grad[r,k,d] = sum_b dL_df[b,r,k] * X_aug[b,d]\n    self.gradients = np.einsum(\"brk,bd-&gt;rkd\", dL_df, X_aug)\n\n    # dL/dnorm_w: sum_k dL/dlogits_{bk} * f_{brk}\n    dL_dnorm_w = np.einsum(\"bk,brk-&gt;br\", dL_dlogits, f)\n\n    # dL/dx: sum_r sum_k dL/dlogits_{bk} * norm_w_{br} * W_{r,k,:}\n    W = self.parameters[:, :, :-1]  # (r,k,d)\n    dL_dx = np.einsum(\"bk,br,rkd-&gt;bd\", dL_dlogits, norm_w, W)\n    return dL_dnorm_w, dL_dx\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray, norm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes the forward pass for the classification consequent layer.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray, norm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the forward pass for the classification consequent layer.\"\"\"\n    batch = x.shape[0]\n    X_aug = np.hstack([x, np.ones((batch, 1))])  # (b, d+1)\n    # Compute per-rule class logits: (b, r, k)\n    f = np.einsum(\"bd,rkd-&gt;brk\", X_aug, self.parameters)\n    # Weighted sum over rules -&gt; logits (b, k)\n    logits = np.einsum(\"br,brk-&gt;bk\", norm_w, f)\n    self.last = {\"X_aug\": X_aug, \"norm_w\": norm_w, \"f\": f}\n    return logits\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the gradients and cached values.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets the gradients and cached values.\"\"\"\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/losses/","title":"Losses API","text":"<p>This module provides loss functions and their gradients used during ANFIS model training.</p>"},{"location":"api/losses/#anfis_toolbox.losses","title":"anfis_toolbox.losses","text":"<p>Loss functions and their gradients for ANFIS Toolbox.</p> <p>This module centralizes the loss definitions used during training to make it explicit which objective is being optimized. Trainers can import from here so the chosen loss is clear in one place.</p>"},{"location":"api/losses/#anfis_toolbox.losses.cross_entropy_grad","title":"cross_entropy_grad","text":"<pre><code>cross_entropy_grad(\n    y_true: ndarray, logits: ndarray\n) -&gt; np.ndarray\n</code></pre> <p>Gradient of cross-entropy w.r.t logits.</p> <p>Accepts integer labels (n,) or one-hot (n,k). Returns gradient with the same shape as logits: (n,k).</p>"},{"location":"api/losses/#anfis_toolbox.losses.cross_entropy_loss","title":"cross_entropy_loss","text":"<pre><code>cross_entropy_loss(\n    y_true: ndarray, logits: ndarray\n) -&gt; float\n</code></pre> <p>Cross-entropy loss from labels (int or one-hot) and logits.</p> <p>This delegates to metrics.cross_entropy for the scalar value.</p>"},{"location":"api/losses/#anfis_toolbox.losses.mse_grad","title":"mse_grad","text":"<pre><code>mse_grad(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Gradient of MSE w.r.t. predictions.</p> <p>d/dy_pred MSE = 2 * (y_pred - y_true) / n</p>"},{"location":"api/losses/#anfis_toolbox.losses.mse_loss","title":"mse_loss","text":"<pre><code>mse_loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Mean squared error (MSE) loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>Array-like true targets of shape (n, d) or (n,).</p> required <code>y_pred</code> <code>ndarray</code> <p>Array-like predictions of same shape as y_true.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar MSE value.</p>"},{"location":"api/losses/#regression-losses","title":"Regression Losses","text":"<p>Functions for regression tasks (continuous output prediction).</p> <ul> <li><code>mse_loss()</code> - Mean squared error loss</li> <li><code>mse_grad()</code> - Gradient of MSE loss</li> </ul>"},{"location":"api/losses/#classification-losses","title":"Classification Losses","text":"<p>Functions for classification tasks (discrete output prediction).</p> <ul> <li><code>cross_entropy_loss()</code> - Cross-entropy loss</li> <li><code>cross_entropy_grad()</code> - Gradient of cross-entropy loss</li> </ul>"},{"location":"api/membership-functions/","title":"Membership Functions","text":""},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF","title":"anfis_toolbox.membership.GaussianMF","text":"<pre><code>GaussianMF(mean: float = 0.0, sigma: float = 1.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Gaussian Membership Function.</p> <p>Implements a Gaussian (bell-shaped) membership function using the formula: \u03bc(x) = exp(-((x - mean)\u00b2 / (2 * sigma\u00b2)))</p> <p>This function is commonly used in fuzzy logic systems due to its smooth and differentiable properties.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>Mean of the Gaussian (center). Defaults to 0.0.</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation (width). Defaults to 1.0.</p> <code>1.0</code> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, mean: float = 0.0, sigma: float = 1.0):\n    \"\"\"Initialize with mean and standard deviation.\n\n    Args:\n        mean: Mean of the Gaussian (center). Defaults to 0.0.\n        sigma: Standard deviation (width). Defaults to 1.0.\n    \"\"\"\n    super().__init__()\n    self.parameters = {\"mean\": mean, \"sigma\": sigma}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients w.r.t. parameters given upstream gradient.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss with respect to the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients w.r.t. parameters given upstream gradient.\n\n    Args:\n        dL_dy: Gradient of the loss with respect to the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    mean = self.parameters[\"mean\"]\n    sigma = self.parameters[\"sigma\"]\n\n    x = self.last_input\n    y = self.last_output\n\n    z = (x - mean) / sigma\n\n    # Derivatives of the Gaussian function\n    dy_dmean = -y * z / sigma\n    dy_dsigma = y * (z**2) / sigma\n\n    # Gradient with respect to mean\n    dL_dmean = np.sum(dL_dy * dy_dmean)\n\n    # Gradient with respect to sigma\n    dL_dsigma = np.sum(dL_dy * dy_dsigma)\n\n    # Update gradients\n    self.gradients[\"mean\"] += dL_dmean\n    self.gradients[\"sigma\"] += dL_dsigma\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute Gaussian membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of Gaussian membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Gaussian membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of Gaussian membership values.\n    \"\"\"\n    mean = self.parameters[\"mean\"]\n    sigma = self.parameters[\"sigma\"]\n    self.last_input = x\n    self.last_output = np.exp(-((x - mean) ** 2) / (2 * sigma**2))\n    return self.last_output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF","title":"anfis_toolbox.membership.Gaussian2MF","text":"<pre><code>Gaussian2MF(\n    sigma1: float = 1.0,\n    c1: float = 0.0,\n    sigma2: float = 1.0,\n    c2: float = 0.0,\n)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Gaussian combination Membership Function (two-sided Gaussian).</p> <p>This membership function uses Gaussian tails on both sides with an optional flat region in the middle.</p> <p>Parameters:</p> Name Type Description Default <code>sigma1</code> <code>float</code> <p>Standard deviation of the left Gaussian tail (must be &gt; 0).</p> <code>1.0</code> <code>c1</code> <code>float</code> <p>Center of the left Gaussian tail.</p> <code>0.0</code> <code>sigma2</code> <code>float</code> <p>Standard deviation of the right Gaussian tail (must be &gt; 0).</p> <code>1.0</code> <code>c2</code> <code>float</code> <p>Center of the right Gaussian tail. Must satisfy c1 &lt;= c2.</p> <code>0.0</code> <p>Definition (with c1 &lt;= c2):     - For x &lt; c1: \u03bc(x) = exp(-((x - c1)^2) / (2*sigma1^2))     - For c1 &lt;= x &lt;= c2: \u03bc(x) = 1     - For x &gt; c2: \u03bc(x) = exp(-((x - c2)^2) / (2*sigma2^2))</p> <p>Special case (c1 == c2): asymmetric Gaussian centered at c1 with sigma1 on the left side and sigma2 on the right side (no flat region).</p> <p>Parameters:</p> Name Type Description Default <code>sigma1</code> <code>float</code> <p>Standard deviation of the first Gaussian. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>c1</code> <code>float</code> <p>Center of the first Gaussian. Defaults to 0.0.</p> <code>0.0</code> <code>sigma2</code> <code>float</code> <p>Standard deviation of the second Gaussian. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>c2</code> <code>float</code> <p>Center of the second Gaussian. Must satisfy c1 &lt;= c2. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sigma1 or sigma2 are not positive.</p> <code>ValueError</code> <p>If c1 &gt; c2.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the parameters 'sigma1', 'c1', 'sigma2', 'c2'.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing the gradients for each parameter, initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, sigma1: float = 1.0, c1: float = 0.0, sigma2: float = 1.0, c2: float = 0.0):\n    \"\"\"Initialize the membership function with two Gaussian components.\n\n    Args:\n        sigma1 (float, optional): Standard deviation of the first Gaussian. Must be positive. Defaults to 1.0.\n        c1 (float, optional): Center of the first Gaussian. Defaults to 0.0.\n        sigma2 (float, optional): Standard deviation of the second Gaussian. Must be positive. Defaults to 1.0.\n        c2 (float, optional): Center of the second Gaussian. Must satisfy c1 &lt;= c2. Defaults to 0.0.\n\n    Raises:\n        ValueError: If sigma1 or sigma2 are not positive.\n        ValueError: If c1 &gt; c2.\n\n    Attributes:\n        parameters (dict): Dictionary containing the parameters 'sigma1', 'c1', 'sigma2', 'c2'.\n        gradients (dict): Dictionary containing the gradients for each parameter, initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if sigma1 &lt;= 0:\n        raise ValueError(f\"Parameter 'sigma1' must be positive, got sigma1={sigma1}\")\n    if sigma2 &lt;= 0:\n        raise ValueError(f\"Parameter 'sigma2' must be positive, got sigma2={sigma2}\")\n    if c1 &gt; c2:\n        raise ValueError(f\"Parameters must satisfy c1 &lt;= c2, got c1={c1}, c2={c2}\")\n\n    self.parameters = {\"sigma1\": float(sigma1), \"c1\": float(c1), \"sigma2\": float(sigma2), \"c2\": float(c2)}\n    self.gradients = {\"sigma1\": 0.0, \"c1\": 0.0, \"sigma2\": 0.0, \"c2\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate parameter gradients for the two-sided Gaussian.</p> <p>The flat middle region contributes no gradients.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Upstream gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate parameter gradients for the two-sided Gaussian.\n\n    The flat middle region contributes no gradients.\n\n    Args:\n        dL_dy: Upstream gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    s1 = self.parameters[\"sigma1\"]\n    c1 = self.parameters[\"c1\"]\n    s2 = self.parameters[\"sigma2\"]\n    c2 = self.parameters[\"c2\"]\n\n    # Regions\n    left_mask = x &lt; c1\n    mid_mask = (x &gt;= c1) &amp; (x &lt;= c2)\n    right_mask = x &gt; c2\n\n    # Left Gaussian tail contributions (treat like a GaussianMF on that region)\n    if np.any(left_mask):\n        xl = x[left_mask]\n        yl = np.exp(-((xl - c1) ** 2) / (2.0 * s1 * s1))\n        z1 = (xl - c1) / s1\n        # Match GaussianMF derivative conventions\n        dmu_dc1 = yl * z1 / s1\n        dmu_dsigma1 = yl * (z1**2) / s1\n\n        dL_dc1 = np.sum(dL_dy[left_mask] * dmu_dc1)\n        dL_dsigma1 = np.sum(dL_dy[left_mask] * dmu_dsigma1)\n\n        self.gradients[\"c1\"] += float(dL_dc1)\n        self.gradients[\"sigma1\"] += float(dL_dsigma1)\n\n    # Mid region (flat) contributes no gradients\n    _ = mid_mask  # placeholder to document intentional no-op\n\n    # Right Gaussian tail contributions\n    if np.any(right_mask):\n        xr = x[right_mask]\n        yr = np.exp(-((xr - c2) ** 2) / (2.0 * s2 * s2))\n        z2 = (xr - c2) / s2\n        dmu_dc2 = yr * z2 / s2\n        dmu_dsigma2 = yr * (z2**2) / s2\n\n        dL_dc2 = np.sum(dL_dy[right_mask] * dmu_dc2)\n        dL_dsigma2 = np.sum(dL_dy[right_mask] * dmu_dsigma2)\n\n        self.gradients[\"c2\"] += float(dL_dc2)\n        self.gradients[\"sigma2\"] += float(dL_dsigma2)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute two-sided Gaussian membership values.</p> <p>The input space is divided by c1 and c2 into: - x &lt; c1: left Gaussian tail with sigma1 centered at c1 - c1 &lt;= x &lt;= c2: flat region (1.0) - x &gt; c2: right Gaussian tail with sigma2 centered at c2</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership degrees for each input value.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute two-sided Gaussian membership values.\n\n    The input space is divided by c1 and c2 into:\n    - x &lt; c1: left Gaussian tail with sigma1 centered at c1\n    - c1 &lt;= x &lt;= c2: flat region (1.0)\n    - x &gt; c2: right Gaussian tail with sigma2 centered at c2\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Membership degrees for each input value.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n\n    s1 = self.parameters[\"sigma1\"]\n    c1 = self.parameters[\"c1\"]\n    s2 = self.parameters[\"sigma2\"]\n    c2 = self.parameters[\"c2\"]\n\n    y = np.zeros_like(x, dtype=float)\n\n    # Regions\n    left_mask = x &lt; c1\n    mid_mask = (x &gt;= c1) &amp; (x &lt;= c2)\n    right_mask = x &gt; c2\n\n    if np.any(left_mask):\n        xl = x[left_mask]\n        y[left_mask] = np.exp(-((xl - c1) ** 2) / (2.0 * s1 * s1))\n\n    if np.any(mid_mask):\n        y[mid_mask] = 1.0\n\n    if np.any(right_mask):\n        xr = x[right_mask]\n        y[right_mask] = np.exp(-((xr - c2) ** 2) / (2.0 * s2 * s2))\n\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF","title":"anfis_toolbox.membership.BellMF","text":"<pre><code>BellMF(a: float = 1.0, b: float = 2.0, c: float = 0.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Bell-shaped (Generalized Bell) Membership Function.</p> <p>Implements a bell-shaped membership function using the formula: \u03bc(x) = 1 / (1 + |((x - c) / a)|^(2b))</p> <p>This function is a generalization of the Gaussian function and provides more flexibility in controlling the shape through the 'b' parameter. It's particularly useful when you need asymmetric membership functions or want to fine-tune the slope characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Width parameter (positive). Controls the width of the curve.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Slope parameter (positive). Controls the steepness of the curve.</p> <code>2.0</code> <code>c</code> <code>float</code> <p>Center parameter. Controls the center position of the curve.</p> <code>0.0</code> Note <p>Parameters 'a' and 'b' must be positive for a valid bell function.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Width parameter (must be positive). Defaults to 1.0.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Slope parameter (must be positive). Defaults to 2.0.</p> <code>2.0</code> <code>c</code> <code>float</code> <p>Center parameter. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' or 'b' are not positive.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float = 1.0, b: float = 2.0, c: float = 0.0):\n    \"\"\"Initialize with width, slope, and center parameters.\n\n    Args:\n        a: Width parameter (must be positive). Defaults to 1.0.\n        b: Slope parameter (must be positive). Defaults to 2.0.\n        c: Center parameter. Defaults to 0.0.\n\n    Raises:\n        ValueError: If 'a' or 'b' are not positive.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if a &lt;= 0:\n        raise ValueError(f\"Parameter 'a' must be positive, got a={a}\")\n\n    if b &lt;= 0:\n        raise ValueError(f\"Parameter 'b' must be positive, got b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients given upstream gradient.</p> <p>Analytical gradients: - \u2202\u03bc/\u2202a: width - \u2202\u03bc/\u2202b: steepness - \u2202\u03bc/\u2202c: center</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients given upstream gradient.\n\n    Analytical gradients:\n    - \u2202\u03bc/\u2202a: width\n    - \u2202\u03bc/\u2202b: steepness\n    - \u2202\u03bc/\u2202c: center\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n\n    x = self.last_input\n    y = self.last_output  # This is \u03bc(x)\n\n    # Intermediate calculations\n    normalized = (x - c) / a\n    abs_normalized = np.abs(normalized)\n\n    # Avoid division by zero and numerical issues\n    # Only compute gradients where abs_normalized &gt; epsilon\n    epsilon = 1e-12\n    valid_mask = abs_normalized &gt; epsilon\n\n    if not np.any(valid_mask):\n        # If all values are at the peak (x \u2248 c), gradients are zero\n        return\n\n    # Initialize gradients\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n\n    # Only compute where we have valid values\n    x_valid = x[valid_mask]\n    y_valid = y[valid_mask]\n    dL_dy_valid = dL_dy[valid_mask]\n    normalized_valid = (x_valid - c) / a\n    abs_normalized_valid = np.abs(normalized_valid)\n\n    # Power term: |normalized|^(2b)\n    power_term_valid = np.power(abs_normalized_valid, 2 * b)\n\n    # For the bell function \u03bc = 1/(1 + z) where z = |normalized|^(2b)\n    # \u2202\u03bc/\u2202z = -1/(1 + z)\u00b2 = -\u03bc\u00b2\n    dmu_dz = -y_valid * y_valid\n\n    # Chain rule: \u2202L/\u2202param = \u2202L/\u2202\u03bc \u00d7 \u2202\u03bc/\u2202z \u00d7 \u2202z/\u2202param\n\n    # \u2202z/\u2202a = \u2202(|normalized|^(2b))/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 \u2202|normalized|/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 \u2202normalized/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (-(x-c)/a\u00b2)\n    # = -2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (x-c)/a\u00b2\n\n    sign_normalized = np.sign(normalized_valid)\n    dz_da = -2 * b * np.power(abs_normalized_valid, 2 * b - 1) * sign_normalized * (x_valid - c) / (a * a)\n    dL_da += np.sum(dL_dy_valid * dmu_dz * dz_da)\n\n    # \u2202z/\u2202b = \u2202(|normalized|^(2b))/\u2202b\n    # = |normalized|^(2b) \u00d7 ln(|normalized|) \u00d7 2\n    # But ln(|normalized|) can be problematic near zero, so we use a safe version\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        ln_abs_normalized = np.log(abs_normalized_valid)\n        ln_abs_normalized = np.where(np.isfinite(ln_abs_normalized), ln_abs_normalized, 0.0)\n\n    dz_db = 2 * power_term_valid * ln_abs_normalized\n    dL_db += np.sum(dL_dy_valid * dmu_dz * dz_db)\n\n    # \u2202z/\u2202c = \u2202(|normalized|^(2b))/\u2202c\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 \u2202normalized/\u2202c\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (-1/a)\n    # = -2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) / a\n\n    dz_dc = -2 * b * np.power(abs_normalized_valid, 2 * b - 1) * sign_normalized / a\n    dL_dc += np.sum(dL_dy_valid * dmu_dz * dz_dc)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute bell membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of bell membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute bell membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of bell membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n\n    self.last_input = x\n\n    # Compute the bell function: \u03bc(x) = 1 / (1 + |((x - c) / a)|^(2b))\n    # To avoid numerical issues, we use the absolute value and handle edge cases\n\n    # Compute (x - c) / a\n    normalized = (x - c) / a\n\n    # Compute |normalized|^(2b)\n    # Use np.abs to handle negative values properly\n    abs_normalized = np.abs(normalized)\n\n    # Handle the case where abs_normalized is very close to zero\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        power_term = np.power(abs_normalized, 2 * b)\n        # Replace any inf or nan with a very large number to make output close to 0\n        power_term = np.where(np.isfinite(power_term), power_term, 1e10)\n\n    # Compute the final result\n    output = 1.0 / (1.0 + power_term)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF","title":"anfis_toolbox.membership.SigmoidalMF","text":"<pre><code>SigmoidalMF(a: float = 1.0, c: float = 0.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Sigmoidal Membership Function.</p> <p>Implements a sigmoidal (S-shaped) membership function using the formula: \u03bc(x) = 1 / (1 + exp(-a(x - c)))</p> <p>This function provides a smooth S-shaped curve that transitions from 0 to 1. It's particularly useful for modeling gradual transitions and is commonly used in neural networks and fuzzy systems.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Slope parameter. Controls the steepness of the sigmoid.        - Positive values: standard sigmoid (0 \u2192 1 as x increases)        - Negative values: inverted sigmoid (1 \u2192 0 as x increases)        - Larger |a|: steeper transition</p> <code>1.0</code> <code>c</code> <code>float</code> <p>Center parameter. Controls the inflection point where \u03bc\u00a9 = 0.5.</p> <code>0.0</code> Note <p>Parameter 'a' cannot be zero (would result in constant function).</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Slope parameter (cannot be zero). Defaults to 1.0.</p> <code>1.0</code> <code>c</code> <code>float</code> <p>Center parameter (inflection point). Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is zero.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float = 1.0, c: float = 0.0):\n    \"\"\"Initialize the sigmoidal membership function.\n\n    Args:\n        a: Slope parameter (cannot be zero). Defaults to 1.0.\n        c: Center parameter (inflection point). Defaults to 0.0.\n\n    Raises:\n        ValueError: If 'a' is zero.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if a == 0:\n        raise ValueError(f\"Parameter 'a' cannot be zero, got a={a}\")\n\n    self.parameters = {\"a\": float(a), \"c\": float(c)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients given upstream gradient.</p> <p>For \u03bc(x) = 1/(1 + exp(-a(x-c))): - \u2202\u03bc/\u2202a = \u03bc(x)(1-\u03bc(x))(x-c) - \u2202\u03bc/\u2202c = -a\u03bc(x)(1-\u03bc(x))</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients given upstream gradient.\n\n    For \u03bc(x) = 1/(1 + exp(-a(x-c))):\n    - \u2202\u03bc/\u2202a = \u03bc(x)(1-\u03bc(x))(x-c)\n    - \u2202\u03bc/\u2202c = -a\u03bc(x)(1-\u03bc(x))\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    a = self.parameters[\"a\"]\n    c = self.parameters[\"c\"]\n\n    x = self.last_input\n    y = self.last_output  # This is \u03bc(x)\n\n    # For sigmoid: \u2202\u03bc/\u2202z = \u03bc(1-\u03bc) where z = -a(x-c)\n    # This is a fundamental property of the sigmoid function\n    dmu_dz = y * (1.0 - y)\n\n    # Chain rule: \u2202L/\u2202param = \u2202L/\u2202\u03bc \u00d7 \u2202\u03bc/\u2202z \u00d7 \u2202z/\u2202param\n\n    # For z = a(x-c):\n    # \u2202z/\u2202a = (x-c)\n    # \u2202z/\u2202c = -a\n\n    # Gradient w.r.t. 'a'\n    dz_da = x - c\n    dL_da = np.sum(dL_dy * dmu_dz * dz_da)\n\n    # Gradient w.r.t. 'c'\n    dz_dc = -a\n    dL_dc = np.sum(dL_dy * dmu_dz * dz_dc)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute sigmoidal membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of sigmoidal membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute sigmoidal membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of sigmoidal membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    c = self.parameters[\"c\"]\n\n    self.last_input = x\n\n    # Compute the sigmoid function: \u03bc(x) = 1 / (1 + exp(-a(x - c)))\n    # To avoid numerical overflow, we use a stable implementation\n\n    # Compute a(x - c) (note: not -a(x - c))\n    z = a * (x - c)\n\n    # Use stable sigmoid implementation to avoid overflow\n    # Standard sigmoid: \u03c3(z) = 1 / (1 + exp(-z))\n    # For numerical stability:\n    # If z &gt;= 0: \u03c3(z) = 1 / (1 + exp(-z))\n    # If z &lt; 0: \u03c3(z) = exp(z) / (1 + exp(z))\n\n    output = np.zeros_like(x, dtype=float)\n\n    # Case 1: z &gt;= 0 (standard case)\n    mask_pos = z &gt;= 0\n    if np.any(mask_pos):\n        output[mask_pos] = 1.0 / (1.0 + np.exp(-z[mask_pos]))\n\n    # Case 2: z &lt; 0 (to avoid exp overflow)\n    mask_neg = z &lt; 0\n    if np.any(mask_neg):\n        exp_z = np.exp(z[mask_neg])\n        output[mask_neg] = exp_z / (1.0 + exp_z)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF","title":"anfis_toolbox.membership.DiffSigmoidalMF","text":"<pre><code>DiffSigmoidalMF(a1: float, c1: float, a2: float, c2: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Difference of two sigmoidal functions.</p> <p>Implements y = s1(x) - s2(x), where each s is a logistic curve with its own slope and center parameters.</p> <p>Parameters:</p> Name Type Description Default <code>a1</code> <code>float</code> <p>The first 'a' parameter for the membership function.</p> required <code>c1</code> <code>float</code> <p>The first 'c' parameter for the membership function.</p> required <code>a2</code> <code>float</code> <p>The second 'a' parameter for the membership function.</p> required <code>c2</code> <code>float</code> <p>The second 'c' parameter for the membership function.</p> required <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the membership function parameters.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for each parameter, initialized to 0.0.</p> <code>last_input</code> <code>dict</code> <p>Stores the last input value (initially None).</p> <code>last_output</code> <code>dict</code> <p>Stores the last output value (initially None).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a1: float, c1: float, a2: float, c2: float):\n    \"\"\"Initializes the membership function with two sets of parameters.\n\n    Args:\n        a1 (float): The first 'a' parameter for the membership function.\n        c1 (float): The first 'c' parameter for the membership function.\n        a2 (float): The second 'a' parameter for the membership function.\n        c2 (float): The second 'c' parameter for the membership function.\n\n    Attributes:\n        parameters (dict): Dictionary containing the membership function parameters.\n        gradients (dict): Dictionary containing gradients for each parameter, initialized to 0.0.\n        last_input: Stores the last input value (initially None).\n        last_output: Stores the last output value (initially None).\n    \"\"\"\n    super().__init__()\n    self.parameters = {\n        \"a1\": float(a1),\n        \"c1\": float(c1),\n        \"a2\": float(a2),\n        \"c2\": float(c2),\n    }\n    self.gradients = dict.fromkeys(self.parameters, 0.0)\n    self.last_input = None\n    self.last_output = None\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients w.r.t. parameters and optionally input.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>np.ndarray | None: Gradient of the loss w.r.t. the input, if available.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients w.r.t. parameters and optionally input.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        np.ndarray | None: Gradient of the loss w.r.t. the input, if available.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n    s1, s2 = self._s1, self._s2\n\n    # Sigmoid derivatives\n    ds1_da1 = (x - c1) * s1 * (1 - s1)\n    ds1_dc1 = -a1 * s1 * (1 - s1)\n    ds2_da2 = (x - c2) * s2 * (1 - s2)\n    ds2_dc2 = -a2 * s2 * (1 - s2)\n\n    # Parameter gradients\n    self.gradients[\"a1\"] += float(np.sum(dL_dy * ds1_da1))\n    self.gradients[\"c1\"] += float(np.sum(dL_dy * ds1_dc1))\n    self.gradients[\"a2\"] += float(np.sum(dL_dy * -ds2_da2))\n    self.gradients[\"c2\"] += float(np.sum(dL_dy * -ds2_dc2))\n\n    # Gradient w.r.t. input (optional, for chaining)\n    dmu_dx = a1 * s1 * (1 - s1) - a2 * s2 * (1 - s2)\n    return dL_dy * dmu_dx\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute y = s1(x) - s2(x).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values for the input.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute y = s1(x) - s2(x).\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Membership values for the input.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n\n    s1 = 1.0 / (1.0 + np.exp(-a1 * (x - c1)))\n    s2 = 1.0 / (1.0 + np.exp(-a2 * (x - c2)))\n    y = s1 - s2\n\n    self.last_output = y\n    self._s1, self._s2 = s1, s2  # store for backward\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF","title":"anfis_toolbox.membership.ProdSigmoidalMF","text":"<pre><code>ProdSigmoidalMF(a1: float, c1: float, a2: float, c2: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Product of two sigmoidal functions.</p> <p>Implements \u03bc(x) = s1(x) * s2(x) with separate parameters for each sigmoid.</p> <p>Parameters:</p> Name Type Description Default <code>a1</code> <code>float</code> <p>The first parameter for the membership function.</p> required <code>c1</code> <code>float</code> <p>The second parameter for the membership function.</p> required <code>a2</code> <code>float</code> <p>The third parameter for the membership function.</p> required <code>c2</code> <code>float</code> <p>The fourth parameter for the membership function.</p> required <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the membership function parameters.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for each parameter, initialized to 0.0.</p> <code>last_input</code> <code>dict</code> <p>Stores the last input value (initialized to None).</p> <code>last_output</code> <code>dict</code> <p>Stores the last output value (initialized to None).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a1: float, c1: float, a2: float, c2: float):\n    \"\"\"Initializes the membership function with specified parameters.\n\n    Args:\n        a1 (float): The first parameter for the membership function.\n        c1 (float): The second parameter for the membership function.\n        a2 (float): The third parameter for the membership function.\n        c2 (float): The fourth parameter for the membership function.\n\n    Attributes:\n        parameters (dict): Dictionary containing the membership function parameters.\n        gradients (dict): Dictionary containing gradients for each parameter, initialized to 0.0.\n        last_input: Stores the last input value (initialized to None).\n        last_output: Stores the last output value (initialized to None).\n    \"\"\"\n    super().__init__()\n    self.parameters = {\n        \"a1\": float(a1),\n        \"c1\": float(c1),\n        \"a2\": float(a2),\n        \"c2\": float(c2),\n    }\n    self.gradients = dict.fromkeys(self.parameters, 0.0)\n    self.last_input = None\n    self.last_output = None\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients and optionally return input gradient.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>np.ndarray | None: Gradient of the loss w.r.t. the input, if available.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients and optionally return input gradient.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        np.ndarray | None: Gradient of the loss w.r.t. the input, if available.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n    s1, s2 = self._s1, self._s2\n\n    # derivatives of sigmoids w.r.t. parameters\n    ds1_da1 = (x - c1) * s1 * (1 - s1)\n    ds1_dc1 = -a1 * s1 * (1 - s1)\n    ds2_da2 = (x - c2) * s2 * (1 - s2)\n    ds2_dc2 = -a2 * s2 * (1 - s2)\n\n    # parameter gradients using product rule\n    self.gradients[\"a1\"] += float(np.sum(dL_dy * ds1_da1 * s2))\n    self.gradients[\"c1\"] += float(np.sum(dL_dy * ds1_dc1 * s2))\n    self.gradients[\"a2\"] += float(np.sum(dL_dy * s1 * ds2_da2))\n    self.gradients[\"c2\"] += float(np.sum(dL_dy * s1 * ds2_dc2))\n\n    # gradient w.r.t. input (optional)\n    dmu_dx = a1 * s1 * (1 - s1) * s2 + a2 * s2 * (1 - s2) * s1\n    return dL_dy * dmu_dx\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes the membership value(s) for input x using the product of two sigmoidal functions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array to the membership function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array after applying the membership function.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the membership value(s) for input x using the product of two sigmoidal functions.\n\n    Args:\n        x (np.ndarray): Input array to the membership function.\n\n    Returns:\n        np.ndarray: Output array after applying the membership function.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n\n    s1 = 1.0 / (1.0 + np.exp(-a1 * (x - c1)))\n    s2 = 1.0 / (1.0 + np.exp(-a2 * (x - c2)))\n    y = s1 * s2\n\n    self.last_output = y\n    self._s1, self._s2 = s1, s2  # store for backward\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF","title":"anfis_toolbox.membership.SShapedMF","text":"<pre><code>SShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>S-shaped Membership Function.</p> <p>Smoothly transitions from 0 to 1 between two parameters a and b using the smoothstep polynomial S(t) = 3t\u00b2 - 2t\u00b3. Commonly used in fuzzy logic for gradual onset of membership.</p> <p>Definition with a &lt; b: - \u03bc(x) = 0, for x \u2264 a - \u03bc(x) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a), for a &lt; x &lt; b - \u03bc(x) = 1, for x \u2265 b</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot (start of transition from 0).</p> required <code>b</code> <code>float</code> <p>Right shoulder (end of transition at 1).</p> required Note <p>Requires a &lt; b.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter, must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter, must be greater than 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter, must be less than 'b'.\n        b (float): The second parameter, must be greater than 'a'.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a and b using analytical derivatives.</p> <p>Uses S(t) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a) on the transition region.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a and b using analytical derivatives.\n\n    Uses S(t) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a) on the transition region.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    # Only transition region contributes to parameter gradients\n    mask = (x &gt;= a) &amp; (x &lt;= b)\n    if not (np.any(mask) and b != a):\n        return\n\n    x_t = x[mask]\n    dL_dy_t = dL_dy[mask]\n    t = (x_t - a) / (b - a)\n\n    # dS/dt = 6*t*(1-t)\n    dS_dt = _dsmoothstep_dt(t)\n\n    # dt/da and dt/db\n    dt_da = (x_t - b) / (b - a) ** 2\n    dt_db = -(x_t - a) / (b - a) ** 2\n\n    dS_da = dS_dt * dt_da\n    dS_db = dS_dt * dt_db\n\n    self.gradients[\"a\"] += float(np.sum(dL_dy_t * dS_da))\n    self.gradients[\"b\"] += float(np.sum(dL_dy_t * dS_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute S-shaped membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute S-shaped membership values.\"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # Right side (x \u2265 b): \u03bc = 1\n    mask_right = x &gt;= b\n    y[mask_right] = 1.0\n\n    # Transition region (a &lt; x &lt; b): \u03bc = smoothstep(t)\n    mask_trans = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_trans):\n        x_t = x[mask_trans]\n        t = (x_t - a) / (b - a)\n        y[mask_trans] = _smoothstep(t)\n\n    # Left side (x \u2264 a) remains 0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF","title":"anfis_toolbox.membership.LinSShapedMF","text":"<pre><code>LinSShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Linear S-shaped saturation Membership Function.</p> Piecewise linear ramp from 0 to 1 between parameters a and b <ul> <li>\u03bc(x) = 0, for x \u2264 a</li> <li>\u03bc(x) = (x - a) / (b - a), for a &lt; x &lt; b</li> <li>\u03bc(x) = 1, for x \u2265 b</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot (start of transition from 0).</p> required <code>b</code> <code>float</code> <p>Right shoulder (end of transition at 1). Requires a &lt; b.</p> required <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter, must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter, must be greater than 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter, must be less than 'b'.\n        b (float): The second parameter, must be greater than 'a'.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for 'a' and 'b' in the ramp region.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for 'a' and 'b' in the ramp region.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    d = b - a\n    if d == 0:\n        return\n    # Only ramp region contributes to parameter gradients\n    mask = (x &gt; a) &amp; (x &lt; b)\n    if not np.any(mask):\n        return\n    xm = x[mask]\n    g = dL_dy[mask]\n    # \u03bc = (x-a)/d with d = b-a\n    # \u2202\u03bc/\u2202a = -(1/d) + (x-a)/d^2\n    dmu_da = -(1.0 / d) + (xm - a) / (d * d)\n    # \u2202\u03bc/\u2202b = -(x-a)/d^2\n    dmu_db = -((xm - a) / (d * d))\n    self.gradients[\"a\"] += float(np.sum(g * dmu_da))\n    self.gradients[\"b\"] += float(np.sum(g * dmu_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute linear S-shaped membership values for x.</p> <p>The rules based on a and b: - x &gt;= b: 1.0 (right saturated) - a &lt; x &lt; b: linear ramp from 0 to 1 - x &lt;= a: 0.0 (left)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array with membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute linear S-shaped membership values for x.\n\n    The rules based on a and b:\n    - x &gt;= b: 1.0 (right saturated)\n    - a &lt; x &lt; b: linear ramp from 0 to 1\n    - x &lt;= a: 0.0 (left)\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Output array with membership values.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    y = np.zeros_like(x, dtype=float)\n    # right saturated region\n    mask_right = x &gt;= b\n    y[mask_right] = 1.0\n    # linear ramp\n    mask_mid = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_mid):\n        y[mask_mid] = (x[mask_mid] - a) / (b - a)\n    # left stays 0\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF","title":"anfis_toolbox.membership.ZShapedMF","text":"<pre><code>ZShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Z-shaped Membership Function.</p> <p>Smoothly transitions from 1 to 0 between two parameters a and b using the smoothstep polynomial S(t) = 3t\u00b2 - 2t\u00b3 (Z = 1 - S). Commonly used in fuzzy logic as the complement of the S-shaped function.</p> <p>Definition with a &lt; b: - \u03bc(x) = 1, for x \u2264 a - \u03bc(x) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a), for a &lt; x &lt; b - \u03bc(x) = 0, for x \u2265 b</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left shoulder (start of transition).</p> required <code>b</code> <code>float</code> <p>Right foot (end of transition).</p> required Note <p>Requires a &lt; b. In the degenerate case a == b, the function becomes an instantaneous drop at x=a.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Lower bound parameter.</p> required <code>b</code> <code>float</code> <p>Upper bound parameter.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a is not less than b.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters a and b.\n\n    Args:\n        a: Lower bound parameter.\n        b: Upper bound parameter.\n\n    Raises:\n        ValueError: If a is not less than b.\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a and b using analytical derivatives.</p> <p>Uses Z(t) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a) on the transition region.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a and b using analytical derivatives.\n\n    Uses Z(t) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a) on the transition region.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    # Only transition region contributes to parameter gradients\n    mask = (x &gt;= a) &amp; (x &lt;= b)\n    if not (np.any(mask) and b != a):\n        return\n\n    x_t = x[mask]\n    dL_dy_t = dL_dy[mask]\n    t = (x_t - a) / (b - a)\n\n    # dZ/dt = -dS/dt = 6*t*(t-1)\n    dZ_dt = -_dsmoothstep_dt(t)\n\n    # dt/da and dt/db\n    dt_da = (x_t - b) / (b - a) ** 2\n    dt_db = -(x_t - a) / (b - a) ** 2\n\n    dZ_da = dZ_dt * dt_da\n    dZ_db = dZ_dt * dt_db\n\n    self.gradients[\"a\"] += float(np.sum(dL_dy_t * dZ_da))\n    self.gradients[\"b\"] += float(np.sum(dL_dy_t * dZ_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute Z-shaped membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Z-shaped membership values.\"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # Left side (x \u2264 a): \u03bc = 1\n    mask_left = x &lt;= a\n    y[mask_left] = 1.0\n\n    # Transition region (a &lt; x &lt; b): \u03bc = 1 - smoothstep(t)\n    mask_trans = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_trans):\n        x_t = x[mask_trans]\n        t = (x_t - a) / (b - a)\n        y[mask_trans] = 1.0 - _smoothstep(t)\n\n    # Right side (x \u2265 b) remains 0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF","title":"anfis_toolbox.membership.LinZShapedMF","text":"<pre><code>LinZShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Linear Z-shaped saturation Membership Function.</p> Piecewise linear ramp from 1 to 0 between parameters a and b <ul> <li>\u03bc(x) = 1, for x \u2264 a</li> <li>\u03bc(x) = (b - x) / (b - a), for a &lt; x &lt; b</li> <li>\u03bc(x) = 0, for x \u2265 b</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left shoulder (end of saturation at 1).</p> required <code>b</code> <code>float</code> <p>Right foot (end of transition to 0). Requires a &lt; b.</p> required <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter of the membership function. Must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter of the membership function.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter of the membership function. Must be less than 'b'.\n        b (float): The second parameter of the membership function.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for 'a' and 'b'.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for 'a' and 'b'.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    d = b - a\n    if d == 0:\n        return\n    mask = (x &gt; a) &amp; (x &lt; b)\n    if not np.any(mask):\n        return\n    xm = x[mask]\n    g = dL_dy[mask]\n    # \u03bc = (b-x)/(b-a)\n    # \u2202\u03bc/\u2202a = (b-x)/(d^2)\n    # \u2202\u03bc/\u2202b = (x-a)/(d^2)\n    dmu_da = (b - xm) / (d * d)\n    dmu_db = (xm - a) / (d * d)\n    self.gradients[\"a\"] += float(np.sum(g * dmu_da))\n    self.gradients[\"b\"] += float(np.sum(g * dmu_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute linear Z-shaped membership values for x.</p> <p>Rules: - x &lt;= a: 1.0 (left saturated) - a &lt; x &lt; b: linear ramp from 1 to 0 - x &gt;= b: 0.0 (right)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output membership values for each input.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute linear Z-shaped membership values for x.\n\n    Rules:\n    - x &lt;= a: 1.0 (left saturated)\n    - a &lt; x &lt; b: linear ramp from 1 to 0\n    - x &gt;= b: 0.0 (right)\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Output membership values for each input.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    y = np.zeros_like(x, dtype=float)\n\n    # left saturated region\n    mask_left = x &lt;= a\n    y[mask_left] = 1.0\n    # linear ramp\n    mask_mid = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_mid):\n        y[mask_mid] = (b - x[mask_mid]) / (b - a)\n    # right stays 0\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF","title":"anfis_toolbox.membership.PiMF","text":"<pre><code>PiMF(a: float, b: float, c: float, d: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Pi-shaped membership function.</p> <p>The Pi-shaped membership function is characterized by a trapezoidal-like shape with smooth S-shaped transitions on both sides. It is defined by four parameters that control the shape and position:</p> <p>Mathematical definition: \u03bc(x) = S(x; a, b) for x \u2208 [a, b]      = 1 for x \u2208 [b, c]      = Z(x; c, d) for x \u2208 [c, d]      = 0 elsewhere</p> <p>Where: - S(x; a, b) is an S-shaped function from 0 to 1 - Z(x; c, d) is a Z-shaped function from 1 to 0</p> <p>The S and Z functions use smooth cubic splines for differentiability: S(x; a, b) = 2*((x-a)/(b-a))^3 for x \u2208 [a, (a+b)/2]            = 1 - 2*((b-x)/(b-a))^3 for x \u2208 [(a+b)/2, b]</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot of the function (where function starts rising from 0)</p> required <code>b</code> <code>float</code> <p>Left shoulder of the function (where function reaches 1)</p> required <code>c</code> <code>float</code> <p>Right shoulder of the function (where function starts falling from 1)</p> required <code>d</code> <code>float</code> <p>Right foot of the function (where function reaches 0)</p> required Note <p>Parameters must satisfy: a &lt; b \u2264 c &lt; d</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot parameter.</p> required <code>b</code> <code>float</code> <p>Left shoulder parameter.</p> required <code>c</code> <code>float</code> <p>Right shoulder parameter.</p> required <code>d</code> <code>float</code> <p>Right foot parameter.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters don't satisfy a &lt; b \u2264 c &lt; d.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float, d: float):\n    \"\"\"Initialize the Pi-shaped membership function.\n\n    Args:\n        a: Left foot parameter.\n        b: Left shoulder parameter.\n        c: Right shoulder parameter.\n        d: Right foot parameter.\n\n    Raises:\n        ValueError: If parameters don't satisfy a &lt; b \u2264 c &lt; d.\n    \"\"\"\n    super().__init__()\n\n    # Parameter validation\n    if not (a &lt; b &lt;= c &lt; d):\n        raise ValueError(f\"Parameters must satisfy a &lt; b \u2264 c &lt; d, got a={a}, b={b}, c={c}, d={d}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"d\": float(d)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0, \"c\": 0.0, \"d\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients for backpropagation.</p> <p>Analytical gradients are computed by region: - S-function: gradients w.r.t. a, b - Z-function: gradients w.r.t. c, d - Flat region: no gradients</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of loss w.r.t. function output.</p> required Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients for backpropagation.\n\n    Analytical gradients are computed by region:\n    - S-function: gradients w.r.t. a, b\n    - Z-function: gradients w.r.t. c, d\n    - Flat region: no gradients\n\n    Args:\n        dL_dy: Gradient of loss w.r.t. function output.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b, c, d = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"], self.parameters[\"d\"]\n\n    # Initialize gradients\n    grad_a = grad_b = grad_c = grad_d = 0.0\n\n    # S-function gradients [a, b]\n    mask_s = (x &gt;= a) &amp; (x &lt;= b)\n    if np.any(mask_s) and b != a:\n        x_s = x[mask_s]\n        dL_dy_s = dL_dy[mask_s]\n        t = (x_s - a) / (b - a)\n\n        # Calculate parameter derivatives\n        dt_da = (x_s - b) / (b - a) ** 2  # Correct derivative\n        dt_db = -(x_s - a) / (b - a) ** 2\n\n        # For smoothstep S(t) = 3*t\u00b2 - 2*t\u00b3, derivative is dS/dt = 6*t - 6*t\u00b2 = 6*t*(1-t)\n        dS_dt = _dsmoothstep_dt(t)\n\n        # Apply chain rule: dS/da = dS/dt * dt/da\n        dS_da = dS_dt * dt_da\n        dS_db = dS_dt * dt_db\n\n        grad_a += np.sum(dL_dy_s * dS_da)\n        grad_b += np.sum(dL_dy_s * dS_db)\n\n    # Z-function gradients [c, d]\n    mask_z = (x &gt;= c) &amp; (x &lt;= d)\n    if np.any(mask_z) and d != c:\n        x_z = x[mask_z]\n        dL_dy_z = dL_dy[mask_z]\n        t = (x_z - c) / (d - c)\n\n        # Calculate parameter derivatives\n        dt_dc = (x_z - d) / (d - c) ** 2  # Correct derivative\n        dt_dd = -(x_z - c) / (d - c) ** 2\n\n        # For Z(t) = 1 - S(t) = 1 - (3*t\u00b2 - 2*t\u00b3), derivative is dZ/dt = -dS/dt = -6*t*(1-t) = 6*t*(t-1)\n        dZ_dt = -_dsmoothstep_dt(t)\n\n        # Apply chain rule: dZ/dc = dZ/dt * dt/dc\n        dZ_dc = dZ_dt * dt_dc\n        dZ_dd = dZ_dt * dt_dd\n\n        grad_c += np.sum(dL_dy_z * dZ_dc)\n        grad_d += np.sum(dL_dy_z * dZ_dd)\n\n    # Accumulate gradients\n    self.gradients[\"a\"] += grad_a\n    self.gradients[\"b\"] += grad_b\n    self.gradients[\"c\"] += grad_c\n    self.gradients[\"d\"] += grad_d\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the Pi-shaped membership function.</p> <p>Combines S and Z functions for smooth transitions: - Rising edge: S-function from a to b - Flat top: constant 1 from b to c - Falling edge: Z-function from c to d - Outside: 0</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values \u03bc(x) \u2208 [0, 1].</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the Pi-shaped membership function.\n\n    Combines S and Z functions for smooth transitions:\n    - Rising edge: S-function from a to b\n    - Flat top: constant 1 from b to c\n    - Falling edge: Z-function from c to d\n    - Outside: 0\n\n    Args:\n        x: Input values.\n\n    Returns:\n        np.ndarray: Membership values \u03bc(x) \u2208 [0, 1].\n    \"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b, c, d = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"], self.parameters[\"d\"]\n\n    # Initialize output\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # S-function for rising edge [a, b]\n    mask_s = (x &gt;= a) &amp; (x &lt;= b)\n    if np.any(mask_s):\n        x_s = x[mask_s]\n        # Avoid division by zero\n        if b != a:\n            t = (x_s - a) / (b - a)  # Normalize to [0, 1]\n\n            # Smooth S-function using smoothstep: S(t) = 3*t\u00b2 - 2*t\u00b3\n            # This is continuous and differentiable across the entire [0,1] interval\n            y_s = _smoothstep(t)\n\n            y[mask_s] = y_s\n        else:\n            # Degenerate case: instant transition\n            y[mask_s] = 1.0\n\n    # Flat region [b, c]: \u03bc(x) = 1\n    mask_flat = (x &gt;= b) &amp; (x &lt;= c)\n    y[mask_flat] = 1.0\n\n    # Z-function for falling edge [c, d]\n    mask_z = (x &gt;= c) &amp; (x &lt;= d)\n    if np.any(mask_z):\n        x_z = x[mask_z]\n        # Avoid division by zero\n        if d != c:\n            t = (x_z - c) / (d - c)  # Normalize to [0, 1]\n\n            # Smooth Z-function (inverted smoothstep): Z(t) = 1 - S(t) = 1 - (3*t\u00b2 - 2*t\u00b3)\n            # This is continuous and differentiable, going from 1 to 0\n            y_z = 1 - _smoothstep(t)\n\n            y[mask_z] = y_z\n        else:\n            # Degenerate case: instant transition\n            y[mask_z] = 0.0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF","title":"anfis_toolbox.membership.TriangularMF","text":"<pre><code>TriangularMF(a: float, b: float, c: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Triangular Membership Function.</p> <p>Implements a triangular membership function using piecewise linear segments: \u03bc(x) = { 0,           x \u2264 a or x \u2265 c        { (x-a)/(b-a), a &lt; x &lt; b        { (c-x)/(c-b), b \u2264 x &lt; c</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point of the triangle.</p> required <code>b</code> <code>float</code> <p>Peak point of the triangle (\u03bc(b) = 1).</p> required <code>c</code> <code>float</code> <p>Right base point of the triangle.</p> required Note <p>Must satisfy: a \u2264 b \u2264 c</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point (must satisfy a \u2264 b).</p> required <code>b</code> <code>float</code> <p>Peak point (must satisfy a \u2264 b \u2264 c).</p> required <code>c</code> <code>float</code> <p>Right base point (must satisfy b \u2264 c).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters do not satisfy a \u2264 b \u2264 c or if a == c (zero width).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float):\n    \"\"\"Initialize the triangular membership function.\n\n    Args:\n        a: Left base point (must satisfy a \u2264 b).\n        b: Peak point (must satisfy a \u2264 b \u2264 c).\n        c: Right base point (must satisfy b \u2264 c).\n\n    Raises:\n        ValueError: If parameters do not satisfy a \u2264 b \u2264 c or if a == c (zero width).\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt;= b &lt;= c):\n        raise ValueError(f\"Triangular MF parameters must satisfy a \u2264 b \u2264 c, got a={a}, b={b}, c={c}\")\n    if a == c:\n        raise ValueError(\"Parameters 'a' and 'c' cannot be equal (zero width triangle)\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c)}\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a, b, c given upstream gradient.</p> <p>Computes analytical derivatives for the rising (a, b) and falling (b, c) regions and sums them over the batch.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. \u03bc(x); same shape or broadcastable to output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a, b, c given upstream gradient.\n\n    Computes analytical derivatives for the rising (a, b) and falling (b, c)\n    regions and sums them over the batch.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. \u03bc(x); same shape or broadcastable to output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    a, b, c = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"]\n    x = self.last_input\n\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n\n    # Left slope: a &lt; x &lt; b\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        if np.any(left_mask):\n            x_left = x[left_mask]\n            dL_dy_left = dL_dy[left_mask]\n\n            # \u2202\u03bc/\u2202a = (x - b) / (b - a)^2\n            dmu_da_left = (x_left - b) / ((b - a) ** 2)\n            dL_da += np.sum(dL_dy_left * dmu_da_left)\n\n            # \u2202\u03bc/\u2202b = -(x - a) / (b - a)^2\n            dmu_db_left = -(x_left - a) / ((b - a) ** 2)\n            dL_db += np.sum(dL_dy_left * dmu_db_left)\n\n    # Right slope: b &lt; x &lt; c\n    if c &gt; b:\n        right_mask = (x &gt; b) &amp; (x &lt; c)\n        if np.any(right_mask):\n            x_right = x[right_mask]\n            dL_dy_right = dL_dy[right_mask]\n\n            # \u2202\u03bc/\u2202b = (x - c) / (c - b)^2\n            dmu_db_right = (x_right - c) / ((c - b) ** 2)\n            dL_db += np.sum(dL_dy_right * dmu_db_right)\n\n            # \u2202\u03bc/\u2202c = (x - b) / (c - b)^2\n            dmu_dc_right = (x_right - b) / ((c - b) ** 2)\n            dL_dc += np.sum(dL_dy_right * dmu_dc_right)\n\n    # Update gradients\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute triangular membership values \u03bc(x).</p> <p>Uses piecewise linear segments defined by (a, b, c): - 0 outside [a, c] - rising slope in (a, b) - peak 1 at x == b - falling slope in (b, c)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values in [0, 1] with the same shape as x.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute triangular membership values \u03bc(x).\n\n    Uses piecewise linear segments defined by (a, b, c):\n    - 0 outside [a, c]\n    - rising slope in (a, b)\n    - peak 1 at x == b\n    - falling slope in (b, c)\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Membership values in [0, 1] with the same shape as x.\n    \"\"\"\n    a, b, c = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"]\n    self.last_input = x\n\n    output = np.zeros_like(x, dtype=float)\n\n    # Left slope\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        output[left_mask] = (x[left_mask] - a) / (b - a)\n\n    # Peak\n    peak_mask = x == b\n    output[peak_mask] = 1.0\n\n    # Right slope\n    if c &gt; b:\n        right_mask = (x &gt; b) &amp; (x &lt; c)\n        output[right_mask] = (c - x[right_mask]) / (c - b)\n\n    # Clip for numerical stability\n    output = np.clip(output, 0.0, 1.0)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF","title":"anfis_toolbox.membership.TrapezoidalMF","text":"<pre><code>TrapezoidalMF(a: float, b: float, c: float, d: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Trapezoidal Membership Function.</p> <p>Implements a trapezoidal membership function using piecewise linear segments: \u03bc(x) = { 0,           x \u2264 a or x \u2265 d        { (x-a)/(b-a), a &lt; x &lt; b        { 1,           b \u2264 x \u2264 c        { (d-x)/(d-c), c &lt; x &lt; d</p> <p>This function is commonly used in fuzzy logic systems when you need a plateau region of full membership, providing robustness to noise and uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point of the trapezoid (lower support bound).</p> required <code>b</code> <code>float</code> <p>Left peak point (start of plateau where \u03bc(x) = 1).</p> required <code>c</code> <code>float</code> <p>Right peak point (end of plateau where \u03bc(x) = 1).</p> required <code>d</code> <code>float</code> <p>Right base point of the trapezoid (upper support bound).</p> required Note <p>Parameters must satisfy: a \u2264 b \u2264 c \u2264 d for a valid trapezoidal function.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point (\u03bc(a) = 0).</p> required <code>b</code> <code>float</code> <p>Left peak point (\u03bc(b) = 1, start of plateau).</p> required <code>c</code> <code>float</code> <p>Right peak point (\u03bc\u00a9 = 1, end of plateau).</p> required <code>d</code> <code>float</code> <p>Right base point (\u03bc(d) = 0).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters don't satisfy a \u2264 b \u2264 c \u2264 d.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float, d: float):\n    \"\"\"Initialize the trapezoidal membership function.\n\n    Args:\n        a: Left base point (\u03bc(a) = 0).\n        b: Left peak point (\u03bc(b) = 1, start of plateau).\n        c: Right peak point (\u03bc(c) = 1, end of plateau).\n        d: Right base point (\u03bc(d) = 0).\n\n    Raises:\n        ValueError: If parameters don't satisfy a \u2264 b \u2264 c \u2264 d.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if not (a &lt;= b &lt;= c &lt;= d):\n        raise ValueError(f\"Trapezoidal MF parameters must satisfy a \u2264 b \u2264 c \u2264 d, got a={a}, b={b}, c={c}, d={d}\")\n\n    if a == d:\n        raise ValueError(\"Parameters 'a' and 'd' cannot be equal (zero width trapezoid)\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"d\": float(d)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients for parameters based on upstream loss gradient.</p> <p>Analytical gradients for the piecewise linear function: - \u2202\u03bc/\u2202a: left slope - \u2202\u03bc/\u2202b: left slope and plateau transition - \u2202\u03bc/\u2202c: right slope and plateau transition - \u2202\u03bc/\u2202d: right slope</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients for parameters based on upstream loss gradient.\n\n    Analytical gradients for the piecewise linear function:\n    - \u2202\u03bc/\u2202a: left slope\n    - \u2202\u03bc/\u2202b: left slope and plateau transition\n    - \u2202\u03bc/\u2202c: right slope and plateau transition\n    - \u2202\u03bc/\u2202d: right slope\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n    d = self.parameters[\"d\"]\n\n    x = self.last_input\n\n    # Initialize gradients\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n    dL_dd = 0.0\n\n    # Left slope region: a &lt; x &lt; b, \u03bc(x) = (x-a)/(b-a)\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        if np.any(left_mask):\n            x_left = x[left_mask]\n            dL_dy_left = dL_dy[left_mask]\n\n            # \u2202\u03bc/\u2202a = -1/(b-a) for left slope\n            dmu_da_left = -1.0 / (b - a)\n            dL_da += np.sum(dL_dy_left * dmu_da_left)\n\n            # \u2202\u03bc/\u2202b = -(x-a)/(b-a)\u00b2 for left slope\n            dmu_db_left = -(x_left - a) / ((b - a) ** 2)\n            dL_db += np.sum(dL_dy_left * dmu_db_left)\n\n    # Plateau region: b \u2264 x \u2264 c, \u03bc(x) = 1\n    # No gradients for plateau region (constant function)\n\n    # Right slope region: c &lt; x &lt; d, \u03bc(x) = (d-x)/(d-c)\n    if d &gt; c:\n        right_mask = (x &gt; c) &amp; (x &lt; d)\n        if np.any(right_mask):\n            x_right = x[right_mask]\n            dL_dy_right = dL_dy[right_mask]\n\n            # \u2202\u03bc/\u2202c = (x-d)/(d-c)\u00b2 for right slope\n            dmu_dc_right = (x_right - d) / ((d - c) ** 2)\n            dL_dc += np.sum(dL_dy_right * dmu_dc_right)\n\n            # \u2202\u03bc/\u2202d = (x-c)/(d-c)\u00b2 for right slope (derivative of (d-x)/(d-c) w.r.t. d)\n            dmu_dd_right = (x_right - c) / ((d - c) ** 2)\n            dL_dd += np.sum(dL_dy_right * dmu_dd_right)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n    self.gradients[\"d\"] += dL_dd\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute trapezoidal membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array containing the trapezoidal membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute trapezoidal membership values.\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Array containing the trapezoidal membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n    d = self.parameters[\"d\"]\n\n    self.last_input = x\n\n    # Initialize output with zeros\n    output = np.zeros_like(x)\n\n    # Left slope: (x - a) / (b - a) for a &lt; x &lt; b\n    if b &gt; a:  # Avoid division by zero\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        output[left_mask] = (x[left_mask] - a) / (b - a)\n\n    # Plateau: \u03bc(x) = 1 for b \u2264 x \u2264 c\n    plateau_mask = (x &gt;= b) &amp; (x &lt;= c)\n    output[plateau_mask] = 1.0\n\n    # Right slope: (d - x) / (d - c) for c &lt; x &lt; d\n    if d &gt; c:  # Avoid division by zero\n        right_mask = (x &gt; c) &amp; (x &lt; d)\n        output[right_mask] = (d - x[right_mask]) / (d - c)\n\n    # Values outside [a, d] are already zero\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/metrics/","title":"Metrics API","text":"<p>This module provides comprehensive metrics for evaluating ANFIS models across regression, classification, and clustering tasks.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics","title":"anfis_toolbox.metrics","text":"<p>Common metrics utilities for ANFIS Toolbox.</p> <p>This module provides lightweight, dependency-free metrics that are useful for training and evaluating ANFIS models.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.accuracy","title":"accuracy","text":"<pre><code>accuracy(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute accuracy from integer/one-hot labels and logits/probabilities.</p> <p>y_pred can be class indices (n,), logits (n,k), or probabilities (n,k). y_true can be class indices (n,) or one-hot (n,k).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.classification_entropy","title":"classification_entropy","text":"<pre><code>classification_entropy(\n    U: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Classification Entropy (CE). Lower is better (crisper).</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Membership matrix of shape (n_samples, n_clusters).</p> required <code>epsilon</code> <code>float</code> <p>Small constant to avoid log(0).</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>CE value as float.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(\n    y_true, logits: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute mean cross-entropy from integer labels or one-hot vs logits.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of shape (n_samples,) of integer class labels, or     one-hot array of shape (n_samples, n_classes).</p> required <code>logits</code> <code>ndarray</code> <p>Array-like raw scores, shape (n_samples, n_classes).</p> required <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>Mean cross-entropy (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.log_loss","title":"log_loss","text":"<pre><code>log_loss(\n    y_true, y_prob: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute mean log loss from integer/one-hot labels and probabilities.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_absolute_error","title":"mean_absolute_error","text":"<pre><code>mean_absolute_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean absolute error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values, shape (...,)</p> required <code>y_pred</code> <p>Array-like of predicted values, same shape as y_true</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of absolute differences over all elements as a float.</p> Notes <ul> <li>Inputs are coerced to NumPy arrays with dtype=float.</li> <li>Broadcasting follows NumPy semantics. If shapes are not compatible   for element-wise subtraction, a ValueError will be raised by NumPy.</li> </ul>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_absolute_percentage_error","title":"mean_absolute_percentage_error","text":"<pre><code>mean_absolute_percentage_error(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the mean absolute percentage error (MAPE) in percent.</p> <p>MAPE = mean( abs((y_true - y_pred) / max(abs(y_true), epsilon)) ) * 100</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values.</p> required <code>y_pred</code> <p>Array-like of predicted values, broadcastable to y_true.</p> required <code>epsilon</code> <code>float</code> <p>Small constant to avoid division by zero when y_true == 0.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>MAPE value as a percentage (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_squared_error","title":"mean_squared_error","text":"<pre><code>mean_squared_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values, shape (...,)</p> required <code>y_pred</code> <p>Array-like of predicted values, same shape as y_true</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of squared differences over all elements as a float.</p> Notes <ul> <li>Inputs are coerced to NumPy arrays with dtype=float.</li> <li>Broadcasting follows NumPy semantics. If shapes are not compatible   for element-wise subtraction, a ValueError will be raised by NumPy.</li> </ul>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_squared_logarithmic_error","title":"mean_squared_logarithmic_error","text":"<pre><code>mean_squared_logarithmic_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean squared logarithmic error (MSLE).</p> <p>Requires non-negative inputs. Uses log1p for numerical stability: MSLE = mean( (log1p(y_true) - log1p(y_pred))^2 ).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.partition_coefficient","title":"partition_coefficient","text":"<pre><code>partition_coefficient(U: ndarray) -&gt; float\n</code></pre> <p>Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Membership matrix of shape (n_samples, n_clusters).</p> required <p>Returns:</p> Type Description <code>float</code> <p>PC value as float.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.pearson_correlation","title":"pearson_correlation","text":"<pre><code>pearson_correlation(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the Pearson correlation coefficient r.</p> <p>Returns 0.0 when the standard deviation of either input is ~0 (undefined r).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.r2_score","title":"r2_score","text":"<pre><code>r2_score(y_true, y_pred, epsilon: float = 1e-12) -&gt; float\n</code></pre> <p>Compute the coefficient of determination R^2.</p> <p>R^2 = 1 - SS_res / SS_tot, where SS_res = sum((y - y_hat)^2) and SS_tot = sum((y - mean(y))^2). If SS_tot is ~0 (constant target), returns 1.0 when predictions match the constant target (SS_res ~0), otherwise 0.0.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.root_mean_squared_error","title":"root_mean_squared_error","text":"<pre><code>root_mean_squared_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the root mean squared error (RMSE).</p> <p>This is simply the square root of mean_squared_error.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.softmax","title":"softmax","text":"<pre><code>softmax(logits: ndarray, axis: int = -1) -&gt; np.ndarray\n</code></pre> <p>Compute a numerically stable softmax along a given axis.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.symmetric_mean_absolute_percentage_error","title":"symmetric_mean_absolute_percentage_error","text":"<pre><code>symmetric_mean_absolute_percentage_error(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the symmetric mean absolute percentage error (SMAPE) in percent.</p> <p>SMAPE = mean( 200 * |y_true - y_pred| / (|y_true| + |y_pred|) ) with an epsilon added to denominator to avoid division by zero.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values.</p> required <code>y_pred</code> <p>Array-like of predicted values, broadcastable to y_true.</p> required <code>epsilon</code> <code>float</code> <p>Small constant added to denominator to avoid division by zero.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>SMAPE value as a percentage (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.xie_beni_index","title":"xie_beni_index","text":"<pre><code>xie_beni_index(\n    X: ndarray,\n    U: ndarray,\n    C: ndarray,\n    m: float = 2.0,\n    epsilon: float = 1e-12,\n) -&gt; float\n</code></pre> <p>Xie\u2013Beni index (XB). Lower is better.</p> <p>XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data array, shape (n_samples, n_features) or (n_samples,).</p> required <code>U</code> <code>ndarray</code> <p>Membership matrix, shape (n_samples, n_clusters).</p> required <code>C</code> <code>ndarray</code> <p>Cluster centers, shape (n_clusters, n_features).</p> required <code>m</code> <code>float</code> <p>Fuzzifier (&gt;1).</p> <code>2.0</code> <code>epsilon</code> <code>float</code> <p>Small constant to avoid division by zero.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>XB value as float (np.inf when centers &lt; 2).</p>"},{"location":"api/metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Functions for evaluating regression model performance:</p> <ul> <li><code>mean_squared_error()</code> - Mean squared error</li> <li><code>mean_absolute_error()</code> - Mean absolute error</li> <li><code>root_mean_squared_error()</code> - Root mean squared error</li> <li><code>mean_absolute_percentage_error()</code> - Mean absolute percentage error</li> <li><code>symmetric_mean_absolute_percentage_error()</code> - Symmetric MAPE</li> <li><code>r2_score()</code> - Coefficient of determination</li> <li><code>pearson_correlation()</code> - Pearson correlation coefficient</li> <li><code>mean_squared_logarithmic_error()</code> - Mean squared logarithmic error</li> </ul>"},{"location":"api/metrics/#classification-metrics","title":"Classification Metrics","text":"<p>Functions for evaluating classification model performance:</p> <ul> <li><code>softmax()</code> - Numerically stable softmax</li> <li><code>cross_entropy()</code> - Cross-entropy loss</li> <li><code>log_loss()</code> - Log loss</li> <li><code>accuracy()</code> - Classification accuracy</li> </ul>"},{"location":"api/metrics/#clustering-validation","title":"Clustering Validation","text":"<p>Functions for evaluating fuzzy clustering quality:</p> <ul> <li><code>partition_coefficient()</code> - Bezdek's partition coefficient</li> <li><code>classification_entropy()</code> - Classification entropy</li> <li><code>xie_beni_index()</code> - Xie-Beni validity index</li> </ul>"},{"location":"api/models/","title":"ANFIS Models","text":""},{"location":"api/models/#anfis_toolbox.model.ANFIS","title":"anfis_toolbox.model.ANFIS","text":"<pre><code>ANFIS(input_mfs: dict[str, list[MembershipFunction]])\n</code></pre> <p>Adaptive Neuro-Fuzzy Inference System (ANFIS) model.</p> <p>Implements the classic 4-layer ANFIS architecture:</p> <p>1) MembershipLayer \u2014 fuzzification of inputs 2) RuleLayer \u2014 rule strength computation (T-norm) 3) NormalizationLayer \u2014 weight normalization 4) ConsequentLayer \u2014 final output via a TSK model</p> <p>Supports forward/backward passes for training, parameter access/update, and a simple prediction API.</p> <p>Attributes:</p> Name Type Description <code>input_mfs</code> <code>dict[str, list[MembershipFunction]]</code> <p>Mapping from input name to its list of membership functions.</p> <code>membership_layer</code> <code>MembershipLayer</code> <p>Layer 1 \u2014 fuzzification.</p> <code>rule_layer</code> <code>RuleLayer</code> <p>Layer 2 \u2014 rule strength computation.</p> <code>normalization_layer</code> <code>NormalizationLayer</code> <p>Layer 3 \u2014 weight normalization.</p> <code>consequent_layer</code> <code>ConsequentLayer</code> <p>Layer 4 \u2014 final TSK output.</p> <code>input_names</code> <code>list[str]</code> <p>Ordered list of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables (features).</p> <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules (Cartesian product of MFs per input).</p> <p>Parameters:</p> Name Type Description Default <code>input_mfs</code> <code>dict[str, list[MembershipFunction]]</code> <p>Mapping from input name to a list of membership functions. Example: <code>{\"x1\": [GaussianMF(0,1), ...], \"x2\": [...]}</code>.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anfis_toolbox.membership import GaussianMF\n&gt;&gt;&gt; input_mfs = {\n...     'x1': [GaussianMF(0, 1), GaussianMF(1, 1)],\n...     'x2': [GaussianMF(0, 1), GaussianMF(1, 1)]\n... }\n&gt;&gt;&gt; model = ANFIS(input_mfs)\n</code></pre> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def __init__(self, input_mfs: dict[str, list[MembershipFunction]]):\n    \"\"\"Initialize the ANFIS model.\n\n    Args:\n        input_mfs (dict[str, list[MembershipFunction]]): Mapping from input\n            name to a list of membership functions. Example:\n            ``{\"x1\": [GaussianMF(0,1), ...], \"x2\": [...]}``.\n\n    Examples:\n        &gt;&gt;&gt; from anfis_toolbox.membership import GaussianMF\n        &gt;&gt;&gt; input_mfs = {\n        ...     'x1': [GaussianMF(0, 1), GaussianMF(1, 1)],\n        ...     'x2': [GaussianMF(0, 1), GaussianMF(1, 1)]\n        ... }\n        &gt;&gt;&gt; model = ANFIS(input_mfs)\n    \"\"\"\n    self.input_mfs = input_mfs\n    self.input_names = list(input_mfs.keys())\n    self.n_inputs = len(input_mfs)\n\n    # Calculate number of membership functions per input\n    mf_per_input = [len(mfs) for mfs in input_mfs.values()]\n\n    # Calculate total number of rules (Cartesian product)\n    self.n_rules = np.prod(mf_per_input)\n\n    # Initialize all layers\n    self.membership_layer = MembershipLayer(input_mfs)\n    self.rule_layer = RuleLayer(self.input_names, mf_per_input)\n    self.normalization_layer = NormalizationLayer()\n    self.consequent_layer = ConsequentLayer(self.n_rules, self.n_inputs)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.membership_functions","title":"membership_functions  <code>property</code>","text":"<pre><code>membership_functions: dict[str, list[MembershipFunction]]\n</code></pre> <p>Return the membership functions grouped by input.</p> <p>Returns:</p> Type Description <code>dict[str, list[MembershipFunction]]</code> <p>dict[str, list[MembershipFunction]]: Mapping from input name to</p> <code>dict[str, list[MembershipFunction]]</code> <p>its list of membership functions.</p>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Returns detailed representation of the ANFIS model.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Returns detailed representation of the ANFIS model.\"\"\"\n    return f\"ANFIS(n_inputs={self.n_inputs}, n_rules={self.n_rules})\"\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns string representation of the ANFIS model.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns string representation of the ANFIS model.\"\"\"\n    return (\n        f\"ANFIS Model:\\n\"\n        f\"  - Inputs: {self.n_inputs} ({', '.join(self.input_names)})\\n\"\n        f\"  - Rules: {self.n_rules}\\n\"\n        f\"  - Membership Functions: {[len(mfs) for mfs in self.input_mfs.values()]}\\n\"\n        f\"  - Parameters: \\\n                {sum(len(mfs) * 2 for mfs in self.input_mfs.values()) + self.n_rules * (self.n_inputs + 1)}\"\n    )\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Run a backward pass through all layers.</p> <p>Propagates gradients from the output back through all layers and stores parameter gradients for a later update step.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the model output, shape <code>(batch_size, 1)</code>.</p> required Source code in <code>anfis_toolbox/model.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Run a backward pass through all layers.\n\n    Propagates gradients from the output back through all layers and stores\n    parameter gradients for a later update step.\n\n    Args:\n        dL_dy (np.ndarray): Gradient of the loss w.r.t. the model output,\n            shape ``(batch_size, 1)``.\n    \"\"\"\n    # Backward pass through Layer 4: Consequent layer\n    dL_dnorm_w, _ = self.consequent_layer.backward(dL_dy)\n\n    # Backward pass through Layer 3: Normalization layer\n    dL_dw = self.normalization_layer.backward(dL_dnorm_w)\n\n    # Backward pass through Layer 2: Rule layer\n    gradients = self.rule_layer.backward(dL_dw)\n\n    # Backward pass through Layer 1: Membership layer\n    self.membership_layer.backward(gradients)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.fit","title":"fit","text":"<pre><code>fit(\n    x: ndarray,\n    y: ndarray,\n    epochs: int = 100,\n    learning_rate: float = 0.01,\n    verbose: bool = True,\n    trainer: None | object = None,\n) -&gt; list[float]\n</code></pre> <p>Train the ANFIS model.</p> <p>If a trainer is provided (see <code>anfis_toolbox.optim</code>), delegate training to it while preserving a scikit-learn-style <code>fit(X, y)</code> entry point. If no trainer is provided, a default <code>HybridTrainer</code> is used with the given hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Training inputs of shape <code>(n_samples, n_inputs)</code>.</p> required <code>y</code> <code>ndarray</code> <p>Training targets of shape <code>(n_samples, 1)</code> for regression.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs. Defaults to <code>100</code>.</p> <code>100</code> <code>learning_rate</code> <code>float</code> <p>Learning rate. Defaults to <code>0.01</code>.</p> <code>0.01</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress. Defaults to <code>True</code>.</p> <code>True</code> <code>trainer</code> <code>object | None</code> <p>External trainer implementing <code>fit(model, X, y)</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Per-epoch loss values.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def fit(\n    self,\n    x: np.ndarray,\n    y: np.ndarray,\n    epochs: int = 100,\n    learning_rate: float = 0.01,\n    verbose: bool = True,\n    trainer: None | object = None,\n) -&gt; list[float]:\n    \"\"\"Train the ANFIS model.\n\n    If a trainer is provided (see ``anfis_toolbox.optim``), delegate training\n    to it while preserving a scikit-learn-style ``fit(X, y)`` entry point. If\n    no trainer is provided, a default ``HybridTrainer`` is used with the given\n    hyperparameters.\n\n    Args:\n        x (np.ndarray): Training inputs of shape ``(n_samples, n_inputs)``.\n        y (np.ndarray): Training targets of shape ``(n_samples, 1)`` for\n            regression.\n        epochs (int, optional): Number of epochs. Defaults to ``100``.\n        learning_rate (float, optional): Learning rate. Defaults to ``0.01``.\n        verbose (bool, optional): Whether to log progress. Defaults to ``True``.\n        trainer (object | None, optional): External trainer implementing\n            ``fit(model, X, y)``. Defaults to ``None``.\n\n    Returns:\n        list[float]: Per-epoch loss values.\n    \"\"\"\n    if trainer is None:\n        # Lazy import to avoid unnecessary dependency at module import time\n        from .optim import HybridTrainer\n\n        trainer = HybridTrainer(learning_rate=learning_rate, epochs=epochs, verbose=verbose)\n\n    # Delegate training to the provided or default trainer\n    return trainer.fit(self, x, y)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Run a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of shape <code>(batch_size, n_inputs)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array of shape <code>(batch_size, 1)</code>.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Run a forward pass through the model.\n\n    Args:\n        x (np.ndarray): Input array of shape ``(batch_size, n_inputs)``.\n\n    Returns:\n        np.ndarray: Output array of shape ``(batch_size, 1)``.\n    \"\"\"\n    # Layer 1: Fuzzification - convert crisp inputs to membership degrees\n    membership_outputs = self.membership_layer.forward(x)\n\n    # Layer 2: Rule strength computation using T-norm (product)\n    rule_strengths = self.rule_layer.forward(membership_outputs)\n\n    # Layer 3: Normalization - ensure rule weights sum to 1.0\n    normalized_weights = self.normalization_layer.forward(rule_strengths)\n\n    # Layer 4: Consequent computation and final output\n    output = self.consequent_layer.forward(x, normalized_weights)\n\n    return output\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.get_gradients","title":"get_gradients","text":"<pre><code>get_gradients() -&gt; dict[str, np.ndarray]\n</code></pre> <p>Return the latest computed gradients.</p> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>dict[str, np.ndarray | dict]: Dictionary with two entries:</p> <code>dict[str, ndarray]</code> <ul> <li><code>\"membership\"</code>: dict mapping input name to a list of MF gradient dicts (one per membership function).</li> </ul> <code>dict[str, ndarray]</code> <ul> <li><code>\"consequent\"</code>: numpy array with consequent gradients.</li> </ul> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def get_gradients(self) -&gt; dict[str, np.ndarray]:\n    \"\"\"Return the latest computed gradients.\n\n    Returns:\n            dict[str, np.ndarray | dict]: Dictionary with two entries:\n\n            - ``\"membership\"``: dict mapping input name to a list of MF\n                gradient dicts (one per membership function).\n            - ``\"consequent\"``: numpy array with consequent gradients.\n    \"\"\"\n    gradients = {\"membership\": {}, \"consequent\": self.consequent_layer.gradients.copy()}\n\n    # Extract membership function gradients\n    for name in self.input_names:\n        gradients[\"membership\"][name] = []\n        for mf in self.input_mfs[name]:\n            mf_grads = mf.gradients.copy()\n            gradients[\"membership\"][name].append(mf_grads)\n\n    return gradients\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; dict[str, np.ndarray]\n</code></pre> <p>Return a snapshot of all trainable parameters.</p> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>dict[str, np.ndarray | dict]: Dictionary with two entries:</p> <ul> <li><code>\"membership\"</code>: dict mapping input name to a list of MF     parameter dicts (one per membership function).</li> <li><code>\"consequent\"</code>: numpy array with consequent parameters.</li> </ul> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def get_parameters(self) -&gt; dict[str, np.ndarray]:\n    \"\"\"Return a snapshot of all trainable parameters.\n\n    Returns:\n            dict[str, np.ndarray | dict]:\n                    Dictionary with two entries:\n\n                    - ``\"membership\"``: dict mapping input name to a list of MF\n                        parameter dicts (one per membership function).\n                    - ``\"consequent\"``: numpy array with consequent parameters.\n    \"\"\"\n    parameters = {\"membership\": {}, \"consequent\": self.consequent_layer.parameters.copy()}\n\n    # Extract membership function parameters\n    for name in self.input_names:\n        parameters[\"membership\"][name] = []\n        for mf in self.input_mfs[name]:\n            mf_params = mf.parameters.copy()\n            parameters[\"membership\"][name].append(mf_params)\n\n    return parameters\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.predict","title":"predict","text":"<pre><code>predict(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict using the current model parameters.</p> <p>Accepts Python lists, 1D or 2D arrays and coerces to the expected shape.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | list[float]</code> <p>Input data. If 1D, must have exactly <code>n_inputs</code> elements; if 2D, must be <code>(batch_size, n_inputs)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predictions of shape <code>(batch_size, 1)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensionality or feature count does not match the model configuration.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def predict(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict using the current model parameters.\n\n    Accepts Python lists, 1D or 2D arrays and coerces to the expected shape.\n\n    Args:\n        x (np.ndarray | list[float]): Input data. If 1D, must have\n            exactly ``n_inputs`` elements; if 2D, must be\n            ``(batch_size, n_inputs)``.\n\n    Returns:\n        np.ndarray: Predictions of shape ``(batch_size, 1)``.\n\n    Raises:\n        ValueError: If input dimensionality or feature count does not match\n            the model configuration.\n    \"\"\"\n    # Accept Python lists or 1D arrays by coercing to correct 2D shape\n    x_arr = np.asarray(x, dtype=float)\n    if x_arr.ndim == 1:\n        # Single sample; ensure feature count matches\n        if x_arr.size != self.n_inputs:\n            raise ValueError(f\"Expected {self.n_inputs} features, got {x_arr.size} in 1D input\")\n        x_arr = x_arr.reshape(1, self.n_inputs)\n    elif x_arr.ndim == 2:\n        # Validate feature count\n        if x_arr.shape[1] != self.n_inputs:\n            raise ValueError(f\"Expected input with {self.n_inputs} features, got {x_arr.shape[1]}\")\n    else:\n        raise ValueError(\"Expected input with shape (batch_size, n_inputs)\")\n\n    return self.forward(x_arr)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.reset_gradients","title":"reset_gradients","text":"<pre><code>reset_gradients()\n</code></pre> <p>Reset all accumulated gradients to zero.</p> <p>Call this before each optimization step to avoid mixing gradients across iterations.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def reset_gradients(self):\n    \"\"\"Reset all accumulated gradients to zero.\n\n    Call this before each optimization step to avoid mixing gradients\n    across iterations.\n    \"\"\"\n    # Reset membership function gradients\n    self.membership_layer.reset()\n\n    # Reset consequent layer gradients\n    self.consequent_layer.reset()\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.set_parameters","title":"set_parameters","text":"<pre><code>set_parameters(parameters: dict[str, ndarray])\n</code></pre> <p>Load parameters into the model.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, ndarray | dict]</code> <p>Dictionary with the same structure as returned by :meth:<code>get_parameters</code>.</p> required Source code in <code>anfis_toolbox/model.py</code> <pre><code>def set_parameters(self, parameters: dict[str, np.ndarray]):\n    \"\"\"Load parameters into the model.\n\n    Args:\n        parameters (dict[str, np.ndarray | dict]): Dictionary with the same\n            structure as returned by :meth:`get_parameters`.\n    \"\"\"\n    # Set consequent layer parameters\n    if \"consequent\" in parameters:\n        self.consequent_layer.parameters = parameters[\"consequent\"].copy()\n\n    # Set membership function parameters\n    if \"membership\" in parameters:\n        membership_params = parameters[\"membership\"]\n        for name in self.input_names:\n            mf_params_list = membership_params.get(name)\n            if not mf_params_list:\n                continue\n            # Only update up to the available MFs for this input\n            for mf, mf_params in zip(self.input_mfs[name], mf_params_list, strict=False):\n                mf.parameters = mf_params.copy()\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFIS.update_parameters","title":"update_parameters","text":"<pre><code>update_parameters(learning_rate: float)\n</code></pre> <p>Apply a single gradient descent update step.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Step size used to update parameters.</p> required Source code in <code>anfis_toolbox/model.py</code> <pre><code>def update_parameters(self, learning_rate: float):\n    \"\"\"Apply a single gradient descent update step.\n\n    Args:\n        learning_rate (float): Step size used to update parameters.\n    \"\"\"\n    # Update consequent layer parameters\n    self.consequent_layer.parameters -= learning_rate * self.consequent_layer.gradients\n\n    # Update membership function parameters\n    for name in self.input_names:\n        for mf in self.input_mfs[name]:\n            for param_name, gradient in mf.gradients.items():\n                mf.parameters[param_name] -= learning_rate * gradient\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier","title":"anfis_toolbox.model.ANFISClassifier","text":"<pre><code>ANFISClassifier(\n    input_mfs: dict[str, list[MembershipFunction]],\n    n_classes: int,\n    random_state: int | None = None,\n)\n</code></pre> <p>ANFIS variant for classification with a softmax head.</p> <p>Aggregates per-rule linear consequents into per-class logits and trains with cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>input_mfs</code> <code>dict[str, list[MembershipFunction]]</code> <p>Mapping from input variable name to its list of membership functions.</p> required <code>n_classes</code> <code>int</code> <p>Number of output classes (&gt;= 2).</p> required <code>random_state</code> <code>int | None</code> <p>Optional random seed for parameter init.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n_classes &lt; 2</code>.</p> <p>Attributes:</p> Name Type Description <code>input_mfs</code> <code>dict[str, list[MembershipFunction]]</code> <p>Membership functions per input.</p> <code>input_names</code> <code>list[str]</code> <p>Input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>n_classes</code> <code>int</code> <p>Number of classes.</p> <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules (product of MFs per input).</p> <code>membership_layer</code> <code>MembershipLayer</code> <p>Computes membership degrees.</p> <code>rule_layer</code> <code>RuleLayer</code> <p>Evaluates rule activations.</p> <code>normalization_layer</code> <code>NormalizationLayer</code> <p>Normalizes rule strengths.</p> <code>consequent_layer</code> <code>ClassificationConsequentLayer</code> <p>Computes class logits.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def __init__(self, input_mfs: dict[str, list[MembershipFunction]], n_classes: int, random_state: int | None = None):\n    \"\"\"Initialize the ANFIS model for classification.\n\n    Args:\n        input_mfs (dict[str, list[MembershipFunction]]): Mapping from input\n            variable name to its list of membership functions.\n        n_classes (int): Number of output classes (&gt;= 2).\n        random_state (int | None): Optional random seed for parameter init.\n\n    Raises:\n        ValueError: If ``n_classes &lt; 2``.\n\n    Attributes:\n        input_mfs (dict[str, list[MembershipFunction]]): Membership functions per input.\n        input_names (list[str]): Input variable names.\n        n_inputs (int): Number of input variables.\n        n_classes (int): Number of classes.\n        n_rules (int): Number of fuzzy rules (product of MFs per input).\n        membership_layer (MembershipLayer): Computes membership degrees.\n        rule_layer (RuleLayer): Evaluates rule activations.\n        normalization_layer (NormalizationLayer): Normalizes rule strengths.\n        consequent_layer (ClassificationConsequentLayer): Computes class logits.\n    \"\"\"\n    if n_classes &lt; 2:\n        raise ValueError(\"n_classes must be &gt;= 2\")\n    self.input_mfs = input_mfs\n    self.input_names = list(input_mfs.keys())\n    self.n_inputs = len(input_mfs)\n    self.n_classes = int(n_classes)\n    mf_per_input = [len(mfs) for mfs in input_mfs.values()]\n    self.n_rules = int(np.prod(mf_per_input))\n    self.membership_layer = MembershipLayer(input_mfs)\n    self.rule_layer = RuleLayer(self.input_names, mf_per_input)\n    self.normalization_layer = NormalizationLayer()\n    self.consequent_layer = ClassificationConsequentLayer(\n        self.n_rules, self.n_inputs, self.n_classes, random_state=random_state\n    )\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.membership_functions","title":"membership_functions  <code>property</code>","text":"<pre><code>membership_functions: dict[str, list[MembershipFunction]]\n</code></pre> <p>Return the membership functions grouped by input.</p> <p>Returns:</p> Type Description <code>dict[str, list[MembershipFunction]]</code> <p>dict[str, list[MembershipFunction]]: Mapping from input name to</p> <code>dict[str, list[MembershipFunction]]</code> <p>its list of membership functions.</p>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return a string representation of the ANFISClassifier.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string describing the classifier configuration.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the ANFISClassifier.\n\n    Returns:\n        str: A formatted string describing the classifier configuration.\n    \"\"\"\n    return f\"ANFISClassifier(n_inputs={self.n_inputs}, n_rules={self.n_rules}, n_classes={self.n_classes})\"\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.backward","title":"backward","text":"<pre><code>backward(dL_dlogits: ndarray)\n</code></pre> <p>Backpropagate gradients through all layers.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dlogits</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. logits, shape <code>(batch_size, n_classes)</code>.</p> required Source code in <code>anfis_toolbox/model.py</code> <pre><code>def backward(self, dL_dlogits: np.ndarray):\n    \"\"\"Backpropagate gradients through all layers.\n\n    Args:\n        dL_dlogits (np.ndarray): Gradient of the loss w.r.t. logits,\n            shape ``(batch_size, n_classes)``.\n    \"\"\"\n    dL_dnorm_w, _ = self.consequent_layer.backward(dL_dlogits)\n    dL_dw = self.normalization_layer.backward(dL_dnorm_w)\n    gradients = self.rule_layer.backward(dL_dw)\n    self.membership_layer.backward(gradients)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    epochs: int = 100,\n    learning_rate: float = 0.01,\n    verbose: bool = True,\n) -&gt; list[float]\n</code></pre> <p>Train the classifier with cross-entropy using gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data of shape <code>(n_samples, n_inputs)</code>.</p> required <code>y</code> <code>ndarray</code> <p>Integer labels of shape <code>(n_samples,)</code> or one-hot array of shape <code>(n_samples, n_classes)</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs. Defaults to <code>100</code>.</p> <code>100</code> <code>learning_rate</code> <code>float</code> <p>Learning rate. Defaults to <code>0.01</code>.</p> <code>0.01</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Per-epoch cross-entropy loss values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provided one-hot labels do not match <code>n_classes</code>.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    epochs: int = 100,\n    learning_rate: float = 0.01,\n    verbose: bool = True,\n) -&gt; list[float]:\n    \"\"\"Train the classifier with cross-entropy using gradient descent.\n\n    Args:\n        X (np.ndarray): Input data of shape ``(n_samples, n_inputs)``.\n        y (np.ndarray): Integer labels of shape ``(n_samples,)`` or\n            one-hot array of shape ``(n_samples, n_classes)``.\n        epochs (int, optional): Number of epochs. Defaults to ``100``.\n        learning_rate (float, optional): Learning rate. Defaults to ``0.01``.\n        verbose (bool, optional): Whether to log progress. Defaults to ``True``.\n\n    Returns:\n        list[float]: Per-epoch cross-entropy loss values.\n\n    Raises:\n        ValueError: If provided one-hot labels do not match ``n_classes``.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    yt = np.asarray(y)\n    if yt.ndim == 1:\n        # integer labels -&gt; one-hot\n        n = yt.shape[0]\n        oh = np.zeros((n, self.n_classes), dtype=float)\n        oh[np.arange(n), yt.astype(int)] = 1.0\n        yt = oh\n    else:\n        if yt.shape[1] != self.n_classes:\n            raise ValueError(\"y one-hot must have n_classes columns\")\n    losses: list[float] = []\n    for _ in range(epochs):\n        self.reset_gradients()\n        # Forward all the way to logits\n        membership_outputs = self.membership_layer.forward(X)\n        rule_strengths = self.rule_layer.forward(membership_outputs)\n        norm_w = self.normalization_layer.forward(rule_strengths)\n        logits = self.consequent_layer.forward(X, norm_w)\n        # Compute CE loss and its gradient w.r.t logits\n        loss = cross_entropy(yt, logits)\n        probs = softmax(logits, axis=1)\n        dL_dlogits = (probs - yt) / yt.shape[0]\n        # Backprop\n        dL_dnorm_w, _ = self.consequent_layer.backward(dL_dlogits)\n        dL_dw = self.normalization_layer.backward(dL_dnorm_w)\n        gradients = self.rule_layer.backward(dL_dw)\n        self.membership_layer.backward(gradients)\n        # Update params\n        self.update_parameters(learning_rate)\n        losses.append(float(loss))\n    return losses\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Run a forward pass through the classifier.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of shape <code>(batch_size, n_inputs)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Logits of shape <code>(batch_size, n_classes)</code>.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Run a forward pass through the classifier.\n\n    Args:\n        x (np.ndarray): Input array of shape ``(batch_size, n_inputs)``.\n\n    Returns:\n        np.ndarray: Logits of shape ``(batch_size, n_classes)``.\n    \"\"\"\n    membership_outputs = self.membership_layer.forward(x)\n    rule_strengths = self.rule_layer.forward(membership_outputs)\n    normalized_weights = self.normalization_layer.forward(rule_strengths)\n    logits = self.consequent_layer.forward(x, normalized_weights)  # (b, k)\n    return logits\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.get_gradients","title":"get_gradients","text":"<pre><code>get_gradients() -&gt; dict[str, np.ndarray]\n</code></pre> <p>Return the latest computed gradients.</p> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>dict[str, np.ndarray | dict]: Dictionary containing:</p> <code>dict[str, ndarray]</code> <ul> <li><code>\"membership\"</code>: nested dict mapping input name to a list of MF gradient dicts.</li> </ul> <code>dict[str, ndarray]</code> <ul> <li><code>\"consequent\"</code>: numpy array with consequent gradients.</li> </ul> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def get_gradients(self) -&gt; dict[str, np.ndarray]:\n    \"\"\"Return the latest computed gradients.\n\n    Returns:\n            dict[str, np.ndarray | dict]: Dictionary containing:\n\n            - ``\"membership\"``: nested dict mapping input name to a list of MF\n                gradient dicts.\n            - ``\"consequent\"``: numpy array with consequent gradients.\n    \"\"\"\n    grads = {\"membership\": {}, \"consequent\": self.consequent_layer.gradients.copy()}\n    for name in self.input_names:\n        grads[\"membership\"][name] = []\n        for mf in self.input_mfs[name]:\n            grads[\"membership\"][name].append(mf.gradients.copy())\n    return grads\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; dict[str, np.ndarray]\n</code></pre> <p>Return a snapshot of all trainable parameters.</p> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>dict[str, np.ndarray | dict]: Dictionary containing:</p> <code>dict[str, ndarray]</code> <ul> <li><code>\"membership\"</code>: nested dict mapping input name to a list of MF parameter dicts.</li> </ul> <code>dict[str, ndarray]</code> <ul> <li><code>\"consequent\"</code>: numpy array with consequent parameters.</li> </ul> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def get_parameters(self) -&gt; dict[str, np.ndarray]:\n    \"\"\"Return a snapshot of all trainable parameters.\n\n    Returns:\n            dict[str, np.ndarray | dict]: Dictionary containing:\n\n            - ``\"membership\"``: nested dict mapping input name to a list of MF\n                parameter dicts.\n            - ``\"consequent\"``: numpy array with consequent parameters.\n    \"\"\"\n    params = {\"membership\": {}, \"consequent\": self.consequent_layer.parameters.copy()}\n    for name in self.input_names:\n        params[\"membership\"][name] = []\n        for mf in self.input_mfs[name]:\n            params[\"membership\"][name].append(mf.parameters.copy())\n    return params\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.predict","title":"predict","text":"<pre><code>predict(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict the most likely class label for each sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | list[float]</code> <p>Inputs. If 1D, must have exactly <code>n_inputs</code> elements; if 2D, must be <code>(batch_size, n_inputs)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted labels of shape <code>(batch_size,)</code>.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def predict(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict the most likely class label for each sample.\n\n    Args:\n        x (np.ndarray | list[float]): Inputs. If 1D, must have exactly\n            ``n_inputs`` elements; if 2D, must be ``(batch_size, n_inputs)``.\n\n    Returns:\n        np.ndarray: Predicted labels of shape ``(batch_size,)``.\n    \"\"\"\n    proba = self.predict_proba(x)\n    return np.argmax(proba, axis=1)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict per-class probabilities for the given inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | list[float]</code> <p>Inputs. If 1D, must have exactly <code>n_inputs</code> elements; if 2D, must be <code>(batch_size, n_inputs)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Probabilities of shape <code>(batch_size, n_classes)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensionality or feature count is invalid.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def predict_proba(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict per-class probabilities for the given inputs.\n\n    Args:\n        x (np.ndarray | list[float]): Inputs. If 1D, must have exactly\n            ``n_inputs`` elements; if 2D, must be ``(batch_size, n_inputs)``.\n\n    Returns:\n        np.ndarray: Probabilities of shape ``(batch_size, n_classes)``.\n\n    Raises:\n        ValueError: If input dimensionality or feature count is invalid.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    if x_arr.ndim == 1:\n        if x_arr.size != self.n_inputs:\n            raise ValueError(f\"Expected {self.n_inputs} features, got {x_arr.size} in 1D input\")\n        x_arr = x_arr.reshape(1, self.n_inputs)\n    elif x_arr.ndim == 2:\n        if x_arr.shape[1] != self.n_inputs:\n            raise ValueError(f\"Expected input with {self.n_inputs} features, got {x_arr.shape[1]}\")\n    else:\n        raise ValueError(\"Expected input with shape (batch_size, n_inputs)\")\n    logits = self.forward(x_arr)\n    return softmax(logits, axis=1)\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.reset_gradients","title":"reset_gradients","text":"<pre><code>reset_gradients()\n</code></pre> <p>Reset gradients accumulated in the model layers to zero.</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def reset_gradients(self):\n    \"\"\"Reset gradients accumulated in the model layers to zero.\"\"\"\n    self.membership_layer.reset()\n    self.consequent_layer.reset()\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.set_parameters","title":"set_parameters","text":"<pre><code>set_parameters(parameters: dict[str, ndarray])\n</code></pre> <p>Load parameters into the classifier.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, ndarray | dict]</code> <p>Dictionary with the same structure as returned by :meth:<code>get_parameters</code>.</p> required Source code in <code>anfis_toolbox/model.py</code> <pre><code>def set_parameters(self, parameters: dict[str, np.ndarray]):\n    \"\"\"Load parameters into the classifier.\n\n    Args:\n        parameters (dict[str, np.ndarray | dict]): Dictionary with the same\n            structure as returned by :meth:`get_parameters`.\n    \"\"\"\n    if \"consequent\" in parameters:\n        self.consequent_layer.parameters = parameters[\"consequent\"].copy()\n    if \"membership\" in parameters:\n        membership_params = parameters[\"membership\"]\n        for name in self.input_names:\n            mf_params_list = membership_params.get(name)\n            if not mf_params_list:\n                continue\n            for mf, mf_params in zip(self.input_mfs[name], mf_params_list, strict=False):\n                mf.parameters = mf_params.copy()\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier.update_parameters","title":"update_parameters","text":"<pre><code>update_parameters(learning_rate: float)\n</code></pre> <p>Updates the parameters of the model using gradient descent.</p> <p>This method applies the specified learning rate to update both the consequent layer parameters and the parameters of each membership function (MF) in the input layers. The update is performed by subtracting the product of the learning rate and the corresponding gradients from each parameter.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The step size used for updating the parameters during gradient descent.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/model.py</code> <pre><code>def update_parameters(self, learning_rate: float):\n    \"\"\"Updates the parameters of the model using gradient descent.\n\n    This method applies the specified learning rate to update both the consequent layer parameters\n    and the parameters of each membership function (MF) in the input layers. The update is performed\n    by subtracting the product of the learning rate and the corresponding gradients from each parameter.\n\n    Args:\n        learning_rate (float): The step size used for updating the parameters during gradient descent.\n\n    Returns:\n        None\n    \"\"\"\n    self.consequent_layer.parameters -= learning_rate * self.consequent_layer.gradients\n    for name in self.input_names:\n        for mf in self.input_mfs[name]:\n            for param_name, gradient in mf.gradients.items():\n                mf.parameters[param_name] -= learning_rate * gradient\n</code></pre>"},{"location":"api/optim/","title":"Optimization","text":""},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer","title":"anfis_toolbox.optim.base.BaseTrainer","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for ANFIS trainers.</p> <p>Subclasses must implement <code>fit</code> and return a list of per-epoch loss values.</p>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(model, X: ndarray, y: ndarray) -&gt; list[float]\n</code></pre> <p>Train the given model on (X, y).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>ANFIS-like model instance providing the methods required by    the specific trainer (see module docstring).</p> required <code>X</code> <code>ndarray</code> <p>Input array of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target array of shape (n_samples,) or (n_samples, 1)             for regression; shape may vary for classification             depending on the trainer being used.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Sequence of loss values, one per epoch.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef fit(self, model, X: np.ndarray, y: np.ndarray) -&gt; list[float]:  # pragma: no cover - abstract\n    \"\"\"Train the given model on (X, y).\n\n    Parameters:\n        model: ANFIS-like model instance providing the methods required by\n               the specific trainer (see module docstring).\n        X (np.ndarray): Input array of shape (n_samples, n_features).\n        y (np.ndarray): Target array of shape (n_samples,) or (n_samples, 1)\n                        for regression; shape may vary for classification\n                        depending on the trainer being used.\n\n    Returns:\n        list[float]: Sequence of loss values, one per epoch.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.init_state","title":"init_state  <code>abstractmethod</code>","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize and return any optimizer-specific state.</p> <p>Called once before training begins. Trainers that don't require state may return None.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to be trained.</p> required <code>X</code> <code>ndarray</code> <p>The full training inputs.</p> required <code>y</code> <code>ndarray</code> <p>The full training targets.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Optimizer state (or None) to be threaded through <code>train_step</code>.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef init_state(self, model, X: np.ndarray, y: np.ndarray):  # pragma: no cover - abstract\n    \"\"\"Initialize and return any optimizer-specific state.\n\n    Called once before training begins. Trainers that don't require state may\n    return None.\n\n    Parameters:\n        model: The model to be trained.\n        X (np.ndarray): The full training inputs.\n        y (np.ndarray): The full training targets.\n\n    Returns:\n        Any: Optimizer state (or None) to be threaded through ``train_step``.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.train_step","title":"train_step  <code>abstractmethod</code>","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform a single training step on a batch and return (loss, new_state).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to be trained.</p> required <code>Xb</code> <code>ndarray</code> <p>A batch of inputs.</p> required <code>yb</code> <code>ndarray</code> <p>A batch of targets.</p> required <code>state</code> <p>Optimizer state produced by <code>init_state</code>.</p> required <p>Returns:</p> Type Description <p>tuple[float, Any]: The batch loss and the updated optimizer state.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):  # pragma: no cover - abstract\n    \"\"\"Perform a single training step on a batch and return (loss, new_state).\n\n    Parameters:\n        model: The model to be trained.\n        Xb (np.ndarray): A batch of inputs.\n        yb (np.ndarray): A batch of targets.\n        state: Optimizer state produced by ``init_state``.\n\n    Returns:\n        tuple[float, Any]: The batch loss and the updated optimizer state.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer","title":"anfis_toolbox.optim.hybrid.HybridTrainer  <code>dataclass</code>","text":"<pre><code>HybridTrainer(\n    learning_rate: float = 0.01,\n    epochs: int = 100,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Original Jang (1993) hybrid training: LSM for consequents + GD for antecedents.</p>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.fit","title":"fit","text":"<pre><code>fit(model, X: ndarray, y: ndarray) -&gt; list[float]\n</code></pre> <p>Train using the hybrid algorithm (LSM for consequents + GD for antecedents).</p> <p>This does not require any special method on the model besides its existing forward/backward-capable layers and membership/consequent parameter accessors.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def fit(self, model, X: np.ndarray, y: np.ndarray) -&gt; list[float]:\n    \"\"\"Train using the hybrid algorithm (LSM for consequents + GD for antecedents).\n\n    This does not require any special method on the model besides its existing\n    forward/backward-capable layers and membership/consequent parameter accessors.\n    \"\"\"\n    X, y = self._prepare_data(X, y)\n\n    logger = logging.getLogger(__name__)\n    losses: list[float] = []\n    for _ in range(self.epochs):\n        # Ensure gradients are reset\n        model.reset_gradients()\n\n        # Forward through layers to compute normalized rule weights\n        membership_outputs = model.membership_layer.forward(X)\n        rule_strengths = model.rule_layer.forward(membership_outputs)\n        normalized_weights = model.normalization_layer.forward(rule_strengths)\n\n        # Build least-squares design matrix A for consequent parameters\n        batch_size = X.shape[0]\n        ones_col = np.ones((batch_size, 1), dtype=float)\n        x_bar = np.concatenate([X, ones_col], axis=1)\n        A_blocks = [normalized_weights[:, j : j + 1] * x_bar for j in range(model.n_rules)]\n        A = np.concatenate(A_blocks, axis=1)\n\n        # Solve for consequent parameters with small Tikhonov regularization\n        try:\n            regularization = 1e-6 * np.eye(A.shape[1])\n            ATA_reg = A.T @ A + regularization\n            theta = np.linalg.solve(ATA_reg, A.T @ y.flatten())\n        except np.linalg.LinAlgError:\n            logger.warning(\"Matrix singular in LSM, using pseudo-inverse\")\n            theta = np.linalg.pinv(A) @ y.flatten()\n\n        model.consequent_layer.parameters = theta.reshape(model.n_rules, model.n_inputs + 1)\n\n        # Compute output and loss with updated consequents\n        y_pred = model.consequent_layer.forward(X, normalized_weights)\n        loss = mse_loss(y, y_pred)\n\n        # Backpropagate for antecedent (membership) parameters only\n        dL_dy = 2 * (y_pred - y) / y.shape[0]\n        dL_dnorm_w, _ = model.consequent_layer.backward(dL_dy)\n        dL_dw = model.normalization_layer.backward(dL_dnorm_w)\n        gradients = model.rule_layer.backward(dL_dw)\n        model.membership_layer.backward(gradients)\n\n        # Apply only membership parameter updates\n        model._apply_membership_gradients(self.learning_rate)\n\n        losses.append(loss)\n    return losses\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Hybrid trainer doesn't maintain optimizer state; returns None.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Hybrid trainer doesn't maintain optimizer state; returns None.\"\"\"\n    return None\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one hybrid step on a batch and return (loss, state).</p> <p>Equivalent to one iteration of the hybrid algorithm on the given batch.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one hybrid step on a batch and return (loss, state).\n\n    Equivalent to one iteration of the hybrid algorithm on the given batch.\n    \"\"\"\n    Xb, yb = self._prepare_data(Xb, yb)\n    # Forward to get normalized weights\n    membership_outputs = model.membership_layer.forward(Xb)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n\n    # Build LSM system for batch\n    ones_col = np.ones((Xb.shape[0], 1), dtype=float)\n    x_bar = np.concatenate([Xb, ones_col], axis=1)\n    A_blocks = [normalized_weights[:, j : j + 1] * x_bar for j in range(model.n_rules)]\n    A = np.concatenate(A_blocks, axis=1)\n    try:\n        regularization = 1e-6 * np.eye(A.shape[1])\n        ATA_reg = A.T @ A + regularization\n        theta = np.linalg.solve(ATA_reg, A.T @ yb.flatten())\n    except np.linalg.LinAlgError:\n        theta = np.linalg.pinv(A) @ yb.flatten()\n    model.consequent_layer.parameters = theta.reshape(model.n_rules, model.n_inputs + 1)\n\n    # Loss and backward for antecedents only\n    y_pred = model.consequent_layer.forward(Xb, normalized_weights)\n    loss = mse_loss(yb, y_pred)\n    dL_dy = mse_grad(yb, y_pred)\n    dL_dnorm_w, _ = model.consequent_layer.backward(dL_dy)\n    dL_dw = model.normalization_layer.backward(dL_dnorm_w)\n    gradients = model.rule_layer.backward(dL_dw)\n    model.membership_layer.backward(gradients)\n    model._apply_membership_gradients(self.learning_rate)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer","title":"anfis_toolbox.optim.sgd.SGDTrainer  <code>dataclass</code>","text":"<pre><code>SGDTrainer(\n    learning_rate: float = 0.01,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Stochastic gradient descent trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Step size for gradient descent.</p> <code>0.01</code> <code>epochs</code> <code>int</code> <p>Number of passes over the data.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>Mini-batch size; if None uses full batch.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data each epoch.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress (delegated to model logging settings).</p> <code>True</code> Notes <ul> <li>Optimizes mean squared error (MSE) between <code>model.forward(X)</code> and <code>y</code>.     For regression ANFIS this is the standard objective.</li> <li>With <code>ANFISClassifier</code>, this trainer will still run but will minimize MSE     on logits/probabilities rather than cross\u2011entropy. For classification, prefer     using <code>ANFISClassifier.fit(...)</code> which uses softmax + cross\u2011entropy.</li> </ul>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.fit","title":"fit","text":"<pre><code>fit(model, X: ndarray, y: ndarray) -&gt; list[float]\n</code></pre> <p>Train the model using pure backpropagation.</p> <p>Uses the model's forward/backward/update APIs directly, without requiring a model.train_step method. Returns a list of loss values per epoch. Loss is MSE, computed as <code>mean((y_pred - y)**2)</code>; 1D <code>y</code> is reshaped to <code>(n,1)</code> for convenience.</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def fit(self, model, X: np.ndarray, y: np.ndarray) -&gt; list[float]:\n    \"\"\"Train the model using pure backpropagation.\n\n    Uses the model's forward/backward/update APIs directly, without requiring\n    a model.train_step method. Returns a list of loss values per epoch.\n    Loss is MSE, computed as ``mean((y_pred - y)**2)``; 1D ``y`` is reshaped\n    to ``(n,1)`` for convenience.\n    \"\"\"\n    X, y = self._prepare_data(X, y)\n\n    n_samples = X.shape[0]\n    losses: list[float] = []\n\n    for _ in range(self.epochs):\n        if self.batch_size is None:\n            # Full-batch gradient descent\n            loss = self._compute_mse_backward_and_update(model, X, y)\n            losses.append(loss)\n        else:\n            # Mini-batch SGD\n            indices = np.arange(n_samples)\n            if self.shuffle:\n                np.random.shuffle(indices)\n            batch_losses: list[float] = []\n            for start in range(0, n_samples, self.batch_size):\n                end = start + self.batch_size\n                batch_idx = indices[start:end]\n                batch_loss = self._compute_mse_backward_and_update(model, X[batch_idx], y[batch_idx])\n                batch_losses.append(batch_loss)\n            # For compatibility, record epoch loss as mean of batch losses\n            losses.append(float(np.mean(batch_losses)))\n\n    return losses\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>SGD has no persistent optimizer state; returns None.</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"SGD has no persistent optimizer state; returns None.\"\"\"\n    return None\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one SGD step on a batch and return (loss, state).</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one SGD step on a batch and return (loss, state).\"\"\"\n    loss = self._compute_mse_backward_and_update(model, Xb, yb)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer","title":"anfis_toolbox.optim.adam.AdamTrainer  <code>dataclass</code>","text":"<pre><code>AdamTrainer(\n    learning_rate: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Adam optimizer-based trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Base step size (alpha).</p> <code>0.001</code> <code>beta1</code> <code>float</code> <p>Exponential decay rate for the first moment estimates.</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>Exponential decay rate for the second moment estimates.</p> <code>0.999</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <code>epochs</code> <code>int</code> <p>Number of passes over the dataset.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>If None, use full-batch; otherwise mini-batches of this size.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at each epoch when using mini-batches.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>True</code> Notes <ul> <li>Optimizes mean squared error (MSE) between <code>model.forward(X)</code> and <code>y</code>.     For regression ANFIS this is the intended objective.</li> <li>With <code>ANFISClassifier</code>, this trainer will still execute (treating integer     labels reshaped to <code>(n,1)</code> or one-hot targets) but it will minimize MSE on     logits/probabilities. For classification tasks, prefer <code>ANFISClassifier.fit</code>     which uses cross-entropy with softmax.</li> </ul>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.fit","title":"fit","text":"<pre><code>fit(model, X: ndarray, y: ndarray) -&gt; list[float]\n</code></pre> <p>Train the model using Adam optimization.</p> <p>This involves computing the forward pass, loss, backward pass, and applying the Adam update step for each training iteration.</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def fit(self, model, X: np.ndarray, y: np.ndarray) -&gt; list[float]:\n    \"\"\"Train the model using Adam optimization.\n\n    This involves computing the forward pass, loss, backward pass, and applying\n    the Adam update step for each training iteration.\n    \"\"\"\n    X, y = self._prepare_data(X, y)\n\n    n_samples = X.shape[0]\n    # Initialize Adam state structures matching parameter shapes\n    params = model.get_parameters()\n    m = _zeros_like_structure(params)\n    v = _zeros_like_structure(params)\n    t = 0  # time step\n\n    losses: list[float] = []\n    for _ in range(self.epochs):\n        if self.batch_size is None:\n            # Full-batch Adam step\n            loss, grads = self._compute_mse_and_grads(model, X, y)\n            t = self._apply_adam_step(model, params, grads, m, v, t)\n            losses.append(loss)\n        else:\n            indices = np.arange(n_samples)\n            if self.shuffle:\n                np.random.shuffle(indices)\n            batch_losses: list[float] = []\n            for start in range(0, n_samples, self.batch_size):\n                end = start + self.batch_size\n                batch_idx = indices[start:end]\n                batch_loss, grads_b = self._compute_mse_and_grads(model, X[batch_idx], y[batch_idx])\n                t = self._apply_adam_step(model, params, grads_b, m, v, t)\n                batch_losses.append(batch_loss)\n            losses.append(float(np.mean(batch_losses)))\n\n    return losses\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize Adam's first and second moments and time step.</p> <p>Returns a dict with keys: params, m, v, t.</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize Adam's first and second moments and time step.\n\n    Returns a dict with keys: params, m, v, t.\n    \"\"\"\n    params = model.get_parameters()\n    return {\n        \"params\": params,\n        \"m\": _zeros_like_structure(params),\n        \"v\": _zeros_like_structure(params),\n        \"t\": 0,\n    }\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>One Adam step on a batch; returns (loss, updated_state).</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"One Adam step on a batch; returns (loss, updated_state).\"\"\"\n    loss, grads = self._compute_mse_and_grads(model, Xb, yb)\n    t_new = self._apply_adam_step(model, state[\"params\"], grads, state[\"m\"], state[\"v\"], state[\"t\"])\n    state[\"t\"] = t_new\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer","title":"anfis_toolbox.optim.rmsprop.RMSPropTrainer  <code>dataclass</code>","text":"<pre><code>RMSPropTrainer(\n    learning_rate: float = 0.001,\n    rho: float = 0.9,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>RMSProp optimizer-based trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Base step size (alpha).</p> <code>0.001</code> <code>rho</code> <code>float</code> <p>Exponential decay rate for the squared gradient moving average.</p> <code>0.9</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <code>epochs</code> <code>int</code> <p>Number of passes over the dataset.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>If None, use full-batch; otherwise mini-batches of this size.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at each epoch when using mini-batches.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>True</code> Notes <ul> <li>Optimizes mean squared error (MSE) between <code>model.forward(X)</code> and <code>y</code>.   For regression ANFIS this is the intended objective.</li> <li>With <code>ANFISClassifier</code>, this trainer will still execute (treating integer   labels reshaped to <code>(n,1)</code> or one-hot targets) but it will minimize MSE on   logits/probabilities. For classification tasks, prefer <code>ANFISClassifier.fit</code>   which uses cross-entropy with softmax.</li> </ul>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.fit","title":"fit","text":"<pre><code>fit(model, X: ndarray, y: ndarray) -&gt; list[float]\n</code></pre> <p>Train the model using RMSProp optimization.</p> <p>Steps per update: 1) forward -&gt; compute MSE loss 2) backward -&gt; obtain gradients via <code>model.get_gradients()</code> 3) update parameters with RMSProp rule using per-parameter caches</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def fit(self, model, X: np.ndarray, y: np.ndarray) -&gt; list[float]:\n    \"\"\"Train the model using RMSProp optimization.\n\n    Steps per update:\n    1) forward -&gt; compute MSE loss\n    2) backward -&gt; obtain gradients via ``model.get_gradients()``\n    3) update parameters with RMSProp rule using per-parameter caches\n    \"\"\"\n    X, y = self._prepare_data(X, y)\n\n    n_samples = X.shape[0]\n\n    # Parameter structures and RMSProp caches\n    params = model.get_parameters()\n    cache = _zeros_like_structure(params)\n\n    losses: list[float] = []\n    for _ in range(self.epochs):\n        if self.batch_size is None:\n            # Full-batch RMSProp step\n            loss, grads = self._compute_mse_and_grads(model, X, y)\n            self._apply_rmsprop_step(model, params, cache, grads)\n            losses.append(loss)\n        else:\n            indices = np.arange(n_samples)\n            if self.shuffle:\n                np.random.shuffle(indices)\n            batch_losses: list[float] = []\n            for start in range(0, n_samples, self.batch_size):\n                end = start + self.batch_size\n                batch_idx = indices[start:end]\n                batch_loss, grads_b = self._compute_mse_and_grads(model, X[batch_idx], y[batch_idx])\n                self._apply_rmsprop_step(model, params, cache, grads_b)\n                batch_losses.append(batch_loss)\n            losses.append(float(np.mean(batch_losses)))\n\n    return losses\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize RMSProp caches for consequents and membership scalars.</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize RMSProp caches for consequents and membership scalars.\"\"\"\n    params = model.get_parameters()\n    return {\"params\": params, \"cache\": _zeros_like_structure(params)}\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>One RMSProp step on a batch; returns (loss, updated_state).</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"One RMSProp step on a batch; returns (loss, updated_state).\"\"\"\n    loss, grads = self._compute_mse_and_grads(model, Xb, yb)\n    self._apply_rmsprop_step(model, state[\"params\"], state[\"cache\"], grads)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer","title":"anfis_toolbox.optim.pso.PSOTrainer  <code>dataclass</code>","text":"<pre><code>PSOTrainer(\n    swarm_size: int = 20,\n    inertia: float = 0.7,\n    cognitive: float = 1.5,\n    social: float = 1.5,\n    epochs: int = 100,\n    init_sigma: float = 0.1,\n    clamp_velocity: None | tuple[float, float] = None,\n    clamp_position: None | tuple[float, float] = None,\n    random_state: None | int = None,\n    verbose: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Particle Swarm Optimization (PSO) trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>swarm_size</code> <code>int</code> <p>Number of particles.</p> <code>20</code> <code>inertia</code> <code>float</code> <p>Inertia weight (w).</p> <code>0.7</code> <code>cognitive</code> <code>float</code> <p>Cognitive coefficient (c1).</p> <code>1.5</code> <code>social</code> <code>float</code> <p>Social coefficient (c2).</p> <code>1.5</code> <code>epochs</code> <code>int</code> <p>Number of iterations of the swarm update.</p> <code>100</code> <code>init_sigma</code> <code>float</code> <p>Std-dev for initializing particle positions around current params.</p> <code>0.1</code> <code>clamp_velocity</code> <code>None | tuple[float, float]</code> <p>Optional (min, max) to clip velocities element-wise.</p> <code>None</code> <code>clamp_position</code> <code>None | tuple[float, float]</code> <p>Optional (min, max) to clip positions element-wise.</p> <code>None</code> <code>random_state</code> <code>None | int</code> <p>Seed for RNG to ensure determinism.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>True</code> Notes <ul> <li>Optimizes mean squared error (MSE) between <code>model.forward(X)</code> and <code>y</code> by   searching directly in parameter space (no gradients).</li> <li>With <code>ANFISClassifier</code>, this will still run but will minimize MSE on   logits/probabilities. Prefer <code>ANFISClassifier.fit</code> (cross-entropy) for   classification training.</li> </ul>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.fit","title":"fit","text":"<pre><code>fit(model, X: ndarray, y: ndarray) -&gt; list[float]\n</code></pre> <p>Run PSO for a number of iterations and return per-epoch best loss.</p> <p>Loss is MSE; if <code>y</code> is 1D, it is reshaped to (n,1) for convenience.</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def fit(self, model, X: np.ndarray, y: np.ndarray) -&gt; list[float]:\n    \"\"\"Run PSO for a number of iterations and return per-epoch best loss.\n\n    Loss is MSE; if ``y`` is 1D, it is reshaped to (n,1) for convenience.\n    \"\"\"\n    X, y = self._prepare_data(X, y)\n    rng = np.random.default_rng(self.random_state)\n\n    # Flatten current model parameters\n    base_params = model.get_parameters()\n    theta0, meta = _flatten_params(base_params)\n    D = theta0.size\n\n    # Initialize swarm around current parameters\n    positions = theta0[None, :] + self.init_sigma * rng.normal(size=(self.swarm_size, D))\n    velocities = np.zeros((self.swarm_size, D), dtype=float)\n\n    # Evaluate initial swarm\n    personal_best_pos = positions.copy()\n    personal_best_val = np.empty(self.swarm_size, dtype=float)\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, base_params)\n        model.set_parameters(params_i)\n        personal_best_val[i] = _mse_loss(model, X, y)\n    g_idx = int(np.argmin(personal_best_val))\n    global_best_pos = personal_best_pos[g_idx].copy()\n    global_best_val = float(personal_best_val[g_idx])\n\n    losses: list[float] = []\n    for _ in range(self.epochs):\n        # Update velocities and positions\n        r1 = rng.random(size=(self.swarm_size, D))\n        r2 = rng.random(size=(self.swarm_size, D))\n        cognitive_term = self.cognitive * r1 * (personal_best_pos - positions)\n        social_term = self.social * r2 * (global_best_pos[None, :] - positions)\n        velocities = self.inertia * velocities + cognitive_term + social_term\n        if self.clamp_velocity is not None:\n            vmin, vmax = self.clamp_velocity\n            velocities = np.clip(velocities, vmin, vmax)\n        positions = positions + velocities\n        if self.clamp_position is not None:\n            pmin, pmax = self.clamp_position\n            positions = np.clip(positions, pmin, pmax)\n\n        # Evaluate and update personal/global bests\n        for i in range(self.swarm_size):\n            params_i = _unflatten_params(positions[i], meta, base_params)\n            model.set_parameters(params_i)\n            val = _mse_loss(model, X, y)\n            if val &lt; personal_best_val[i]:\n                personal_best_val[i] = val\n                personal_best_pos[i] = positions[i].copy()\n                if val &lt; global_best_val:\n                    global_best_val = float(val)\n                    global_best_pos = positions[i].copy()\n\n        # Set model to global best and record loss\n        best_params = _unflatten_params(global_best_pos, meta, base_params)\n        model.set_parameters(best_params)\n        losses.append(global_best_val)\n\n    return losses\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize PSO swarm state and return as a dict.</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize PSO swarm state and return as a dict.\"\"\"\n    X, y = self._prepare_data(X, y)\n    rng = np.random.default_rng(self.random_state)\n    base_params = model.get_parameters()\n    theta0, meta = _flatten_params(base_params)\n    D = theta0.size\n    positions = theta0[None, :] + self.init_sigma * rng.normal(size=(self.swarm_size, D))\n    velocities = np.zeros((self.swarm_size, D), dtype=float)\n    # Initialize personal/global bests on provided data\n    personal_best_pos = positions.copy()\n    personal_best_val = np.empty(self.swarm_size, dtype=float)\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, base_params)\n        model.set_parameters(params_i)\n        personal_best_val[i] = _mse_loss(model, X, y)\n    g_idx = int(np.argmin(personal_best_val))\n    global_best_pos = personal_best_pos[g_idx].copy()\n    global_best_val = float(personal_best_val[g_idx])\n    return {\n        \"meta\": meta,\n        \"template\": base_params,\n        \"positions\": positions,\n        \"velocities\": velocities,\n        \"pbest_pos\": personal_best_pos,\n        \"pbest_val\": personal_best_val,\n        \"gbest_pos\": global_best_pos,\n        \"gbest_val\": global_best_val,\n        \"rng\": rng,\n    }\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one PSO iteration over the swarm on a batch and return (best_loss, state).</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one PSO iteration over the swarm on a batch and return (best_loss, state).\"\"\"\n    Xb, yb = self._prepare_data(Xb, yb)\n    positions = state[\"positions\"]\n    velocities = state[\"velocities\"]\n    personal_best_pos = state[\"pbest_pos\"]\n    personal_best_val = state[\"pbest_val\"]\n    global_best_pos = state[\"gbest_pos\"]\n    global_best_val = state[\"gbest_val\"]\n    meta = state[\"meta\"]\n    template = state[\"template\"]\n    rng = state[\"rng\"]\n\n    D = positions.shape[1]\n    r1 = rng.random(size=(self.swarm_size, D))\n    r2 = rng.random(size=(self.swarm_size, D))\n    cognitive_term = self.cognitive * r1 * (personal_best_pos - positions)\n    social_term = self.social * r2 * (global_best_pos[None, :] - positions)\n    velocities = self.inertia * velocities + cognitive_term + social_term\n    if self.clamp_velocity is not None:\n        vmin, vmax = self.clamp_velocity\n        velocities = np.clip(velocities, vmin, vmax)\n    positions = positions + velocities\n    if self.clamp_position is not None:\n        pmin, pmax = self.clamp_position\n        positions = np.clip(positions, pmin, pmax)\n\n    # Evaluate swarm and update bests\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, template)\n        model.set_parameters(params_i)\n        val = _mse_loss(model, Xb, yb)\n        if val &lt; personal_best_val[i]:\n            personal_best_val[i] = val\n            personal_best_pos[i] = positions[i].copy()\n            if val &lt; global_best_val:\n                global_best_val = float(val)\n                global_best_pos = positions[i].copy()\n\n    # Update state and set model to global best\n    state.update(\n        {\n            \"positions\": positions,\n            \"velocities\": velocities,\n            \"pbest_pos\": personal_best_pos,\n            \"pbest_val\": personal_best_val,\n            \"gbest_pos\": global_best_pos,\n            \"gbest_val\": global_best_val,\n        }\n    )\n    best_params = _unflatten_params(global_best_pos, meta, template)\n    model.set_parameters(best_params)\n    return float(global_best_val), state\n</code></pre>"},{"location":"examples/advanced_example/","title":"Advanced ANFIS Example","text":"In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox import QuickANFIS, quick_evaluate\nfrom anfis_toolbox.metrics import r2_score\nfrom anfis_toolbox.model_selection import KFold, train_test_split\n\nnp.random.seed(42)  # reproducibility\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox import QuickANFIS, quick_evaluate from anfis_toolbox.metrics import r2_score from anfis_toolbox.model_selection import KFold, train_test_split  np.random.seed(42)  # reproducibility In\u00a0[5]: Copied! <pre>n = 800\nx1 = np.random.uniform(-3.0, 3.0, size=n)\nx2 = np.random.uniform(-2.0, 2.0, size=n)\nx3 = np.random.uniform(-1.5, 1.5, size=n)\nX = np.column_stack([x1, x2, x3])\ny = np.sin(x1) + 0.3 * (x2 ** 2) + 0.5 * np.cos(1.5 * x3) + 0.1 * np.random.randn(n)\ny = y.reshape(-1, 1)\n\n# Standardize features\nmu = X.mean(axis=0)\nsd = X.std(axis=0) + 1e-12\nX_std = (X - mu) / sd\nX.shape, X_std.shape, y.shape\n</pre> n = 800 x1 = np.random.uniform(-3.0, 3.0, size=n) x2 = np.random.uniform(-2.0, 2.0, size=n) x3 = np.random.uniform(-1.5, 1.5, size=n) X = np.column_stack([x1, x2, x3]) y = np.sin(x1) + 0.3 * (x2 ** 2) + 0.5 * np.cos(1.5 * x3) + 0.1 * np.random.randn(n) y = y.reshape(-1, 1)  # Standardize features mu = X.mean(axis=0) sd = X.std(axis=0) + 1e-12 X_std = (X - mu) / sd X.shape, X_std.shape, y.shape Out[5]: <pre>((800, 3), (800, 3), (800, 1))</pre> In\u00a0[6]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</pre> X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=0) X_train.shape, X_test.shape, y_train.shape, y_test.shape Out[6]: <pre>((640, 3), (160, 3), (640, 1), (160, 1))</pre> In\u00a0[7]: Copied! <pre>param_grid = {\n    'n_mfs': [5, 7],\n    'mf_type': ['gaussian', 'bell'],\n    'epochs': [80, 120]\n}\n\nkf = KFold(n_splits=3, shuffle=True, random_state=0)\n\ndef evaluate_config(n_mfs, mf_type, epochs):\n    r2s = []\n    for tr_idx, va_idx in kf.split(X_train):\n        X_tr, X_va = X_train[tr_idx], X_train[va_idx]\n        y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n        model = QuickANFIS.for_regression(X_tr, n_mfs=n_mfs, mf_type=mf_type, init='fcm', random_state=42)\n        _ = model.fit(X_tr, y_tr, epochs=epochs, learning_rate=0.02, verbose=False)\n        y_va_pred = model.predict(X_va)\n        r2s.append(r2_score(y_va, y_va_pred))\n    return float(np.mean(r2s))\n\nbest_cfg, best_score = None, -np.inf\nresults = []\nprint(\"Evaluating configurations:\")\nfor n_mfs in param_grid['n_mfs']:\n    for mf_type in param_grid['mf_type']:\n        for epochs in param_grid['epochs']:\n            score = evaluate_config(n_mfs, mf_type, epochs)\n            results.append({'n_mfs': n_mfs, 'mf_type': mf_type, 'epochs': epochs, 'mean_r2': score})\n            print(score)\n            if score &gt; best_score:\n                best_score = score\n                best_cfg = {'n_mfs': n_mfs, 'mf_type': mf_type, 'epochs': epochs}\nbest_cfg, best_score\n</pre> param_grid = {     'n_mfs': [5, 7],     'mf_type': ['gaussian', 'bell'],     'epochs': [80, 120] }  kf = KFold(n_splits=3, shuffle=True, random_state=0)  def evaluate_config(n_mfs, mf_type, epochs):     r2s = []     for tr_idx, va_idx in kf.split(X_train):         X_tr, X_va = X_train[tr_idx], X_train[va_idx]         y_tr, y_va = y_train[tr_idx], y_train[va_idx]         model = QuickANFIS.for_regression(X_tr, n_mfs=n_mfs, mf_type=mf_type, init='fcm', random_state=42)         _ = model.fit(X_tr, y_tr, epochs=epochs, learning_rate=0.02, verbose=False)         y_va_pred = model.predict(X_va)         r2s.append(r2_score(y_va, y_va_pred))     return float(np.mean(r2s))  best_cfg, best_score = None, -np.inf results = [] print(\"Evaluating configurations:\") for n_mfs in param_grid['n_mfs']:     for mf_type in param_grid['mf_type']:         for epochs in param_grid['epochs']:             score = evaluate_config(n_mfs, mf_type, epochs)             results.append({'n_mfs': n_mfs, 'mf_type': mf_type, 'epochs': epochs, 'mean_r2': score})             print(score)             if score &gt; best_score:                 best_score = score                 best_cfg = {'n_mfs': n_mfs, 'mf_type': mf_type, 'epochs': epochs} best_cfg, best_score <pre>Evaluating configurations:\n-9.564034368864718\n-24.113957366133153\n-6.633278023678602\n-16.771932607788827\n0.5611883161213395\n0.4345172625778757\n0.1489655257641426\n0.3683488532923132\n</pre> Out[7]: <pre>({'n_mfs': 7, 'mf_type': 'gaussian', 'epochs': 80}, 0.5611883161213395)</pre> In\u00a0[8]: Copied! <pre>model_best = QuickANFIS.for_regression(X_train, n_mfs=best_cfg['n_mfs'], mf_type=best_cfg['mf_type'], init='fcm', random_state=123)\nlosses = model_best.fit(X_train, y_train, epochs=best_cfg['epochs'], learning_rate=0.02, verbose=False)\nmetrics_train = quick_evaluate(model_best, X_train, y_train, print_results=False)\nmetrics_test = quick_evaluate(model_best, X_test, y_test, print_results=False)\nmetrics_train, metrics_test\n</pre> model_best = QuickANFIS.for_regression(X_train, n_mfs=best_cfg['n_mfs'], mf_type=best_cfg['mf_type'], init='fcm', random_state=123) losses = model_best.fit(X_train, y_train, epochs=best_cfg['epochs'], learning_rate=0.02, verbose=False) metrics_train = quick_evaluate(model_best, X_train, y_train, print_results=False) metrics_test = quick_evaluate(model_best, X_test, y_test, print_results=False) metrics_train, metrics_test Out[8]: <pre>({'mse': 3.284219167484291e-08,\n  'rmse': np.float64(0.00018122414760412837),\n  'mae': 3.780887981312326e-05,\n  'r2': 0.999999954196575,\n  'mape': np.float64(0.012982695978181805),\n  'max_error': np.float64(0.003173606934868234),\n  'std_error': np.float64(0.00018122411543007529)},\n {'mse': 0.2939164446687753,\n  'rmse': np.float64(0.5421406133732976),\n  'mae': 0.34670961382480164,\n  'r2': 0.4986060712238396,\n  'mape': np.float64(145.7614512309781),\n  'max_error': np.float64(3.205974893698987),\n  'std_error': np.float64(0.5416649670141173)})</pre> In\u00a0[9]: Copied! <pre>plt.figure(figsize=(5,3))\nplt.plot(losses, color='tab:blue')\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training curve (Hybrid)')\n# zoom if nearly flat\nif len(losses) &gt; 5 and (max(losses) - min(losses)) &lt; 1e-2:\n    lo, hi = min(losses), max(losses)\n    plt.ylim(lo - 0.02 * (hi - lo + 1e-9), hi + 0.02 * (hi - lo + 1e-9))\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(5,3)) plt.plot(losses, color='tab:blue') plt.xlabel('Epoch') plt.ylabel('MSE Loss') plt.title('Training curve (Hybrid)') # zoom if nearly flat if len(losses) &gt; 5 and (max(losses) - min(losses)) &lt; 1e-2:     lo, hi = min(losses), max(losses)     plt.ylim(lo - 0.02 * (hi - lo + 1e-9), hi + 0.02 * (hi - lo + 1e-9)) plt.tight_layout() plt.show() In\u00a0[10]: Copied! <pre>y_pred_test = model_best.predict(X_test)\nplt.figure(figsize=(4,4))\nplt.scatter(y_test, y_pred_test, s=10, alpha=0.4, color='tab:green')\nmn, mx = float(np.min(y_test)), float(np.max(y_test))\nplt.plot([mn, mx], [mn, mx], 'r--', lw=2, label='ideal')\nplt.xlabel('y true')\nplt.ylabel('y pred')\nplt.title('Parity plot (test)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> y_pred_test = model_best.predict(X_test) plt.figure(figsize=(4,4)) plt.scatter(y_test, y_pred_test, s=10, alpha=0.4, color='tab:green') mn, mx = float(np.min(y_test)), float(np.max(y_test)) plt.plot([mn, mx], [mn, mx], 'r--', lw=2, label='ideal') plt.xlabel('y true') plt.ylabel('y pred') plt.title('Parity plot (test)') plt.legend() plt.tight_layout() plt.show() In\u00a0[11]: Copied! <pre>res = (y_test - y_pred_test).ravel()\nplt.figure(figsize=(5,3))\nplt.hist(res, bins=30, color='tab:purple', alpha=0.75)\nplt.xlabel('Residual')\nplt.ylabel('Count')\nplt.title('Residuals histogram (test)')\nplt.tight_layout()\nplt.show()\n</pre> res = (y_test - y_pred_test).ravel() plt.figure(figsize=(5,3)) plt.hist(res, bins=30, color='tab:purple', alpha=0.75) plt.xlabel('Residual') plt.ylabel('Count') plt.title('Residuals histogram (test)') plt.tight_layout() plt.show() In\u00a0[12]: Copied! <pre># Choose to vary dimensions 0 and 1; fix dim 2 at its train median\ndim_x, dim_y, dim_fix = 0, 1, 2\nfixed_val = np.median(X_train[:, dim_fix])\n\ngx = np.linspace(np.percentile(X_train[:, dim_x], 2), np.percentile(X_train[:, dim_x], 98), 60)\ngy = np.linspace(np.percentile(X_train[:, dim_y], 2), np.percentile(X_train[:, dim_y], 98), 60)\nGX, GY = np.meshgrid(gx, gy)\ngrid = np.stack([GX.ravel(), GY.ravel(), np.full(GX.size, fixed_val)], axis=1)\nZ = model_best.predict(grid).reshape(GX.shape)\n\nplt.figure(figsize=(5,4))\ncs = plt.contourf(GX, GY, Z, levels=30, cmap='viridis')\nplt.colorbar(cs, shrink=0.8, label='prediction')\nplt.xlabel(f'feature {dim_x} (std)')\nplt.ylabel(f'feature {dim_y} (std)')\nplt.title('2D prediction surface (slice, fix dim 2)')\nplt.tight_layout()\nplt.show()\n</pre> # Choose to vary dimensions 0 and 1; fix dim 2 at its train median dim_x, dim_y, dim_fix = 0, 1, 2 fixed_val = np.median(X_train[:, dim_fix])  gx = np.linspace(np.percentile(X_train[:, dim_x], 2), np.percentile(X_train[:, dim_x], 98), 60) gy = np.linspace(np.percentile(X_train[:, dim_y], 2), np.percentile(X_train[:, dim_y], 98), 60) GX, GY = np.meshgrid(gx, gy) grid = np.stack([GX.ravel(), GY.ravel(), np.full(GX.size, fixed_val)], axis=1) Z = model_best.predict(grid).reshape(GX.shape)  plt.figure(figsize=(5,4)) cs = plt.contourf(GX, GY, Z, levels=30, cmap='viridis') plt.colorbar(cs, shrink=0.8, label='prediction') plt.xlabel(f'feature {dim_x} (std)') plt.ylabel(f'feature {dim_y} (std)') plt.title('2D prediction surface (slice, fix dim 2)') plt.tight_layout() plt.show()"},{"location":"examples/advanced_example/#advanced-anfis-example","title":"Advanced ANFIS Example\u00b6","text":"<p>This notebook demonstrates a more advanced workflow for ANFIS-Toolbox:</p> <ul> <li>3D nonlinear regression dataset with noise</li> <li>Feature standardization</li> <li>Lightweight hyperparameter search with 3-fold CV (grid over <code>n_mfs</code>, <code>mf_type</code>, <code>epochs</code>)</li> <li>Final training and hold-out evaluation</li> <li>Visualizations: training curve, parity plot, residuals histogram, and a 2D surface (slice)</li> </ul>"},{"location":"examples/advanced_example/#1-imports-and-setup","title":"1) Imports and setup\u00b6","text":"<p>We import NumPy, plotting, and core utilities from ANFIS-Toolbox.</p>"},{"location":"examples/advanced_example/#2-create-a-3d-nonlinear-dataset-and-standardize","title":"2) Create a 3D nonlinear dataset and standardize\u00b6","text":"<p>Target: <code>y = sin(x1) + 0.3 * x2^2 + 0.5 * cos(1.5 * x3) + noise</code>. We standardize inputs to help training stability.</p>"},{"location":"examples/advanced_example/#3-traintest-split-8020","title":"3) Train/test split (80/20)\u00b6","text":""},{"location":"examples/advanced_example/#4-hyperparameter-grid-and-3-fold-cross-validation","title":"4) Hyperparameter grid and 3-fold cross-validation\u00b6","text":"<p>We search over MF count, MF type, and epochs. Metric: mean validation R\u00b2.</p>"},{"location":"examples/advanced_example/#5-train-final-model-with-best-config-and-evaluate-on-hold-out","title":"5) Train final model with best config and evaluate on hold-out\u00b6","text":""},{"location":"examples/advanced_example/#training-curve-loss-vs-epoch","title":"Training curve (loss vs. epoch)\u00b6","text":""},{"location":"examples/advanced_example/#parity-plot-y-true-vs-y-pred-on-test-set","title":"Parity plot (y true vs. y pred) on test set\u00b6","text":""},{"location":"examples/advanced_example/#residuals-histogram-test-set","title":"Residuals histogram (test set)\u00b6","text":""},{"location":"examples/advanced_example/#6-2d-prediction-surface-slice","title":"6) 2D prediction surface (slice)\u00b6","text":"<p>We fix one feature at its median (from train set) and visualize predictions over a grid for the other two features.</p>"},{"location":"examples/basic_example/","title":"Basic Usage","text":"<p>We import NumPy and helper utilities from ANFIS-Toolbox, then generate a simple noisy sine dataset for regression:</p> <ul> <li>Inputs <code>X</code> are evenly spaced in [-\u03c0, \u03c0].</li> <li>Targets <code>y</code> follow <code>sin(x)</code> with Gaussian noise. This small problem is ideal to showcase ANFIS function approximation.</li> </ul> In\u00a0[18]: Copied! <pre>import numpy as np\n\nnp.random.seed(42)  # For reproducibility\n\nn = 200\nX = np.linspace(-np.pi, np.pi, n).reshape(-1, 1)\ny = np.sin(X[:, 0]) + 0.2 * np.random.randn(n)\ny = y.reshape(-1, 1)\n</pre> import numpy as np  np.random.seed(42)  # For reproducibility  n = 200 X = np.linspace(-np.pi, np.pi, n).reshape(-1, 1) y = np.sin(X[:, 0]) + 0.2 * np.random.randn(n) y = y.reshape(-1, 1) In\u00a0[19]: Copied! <pre>from anfis_toolbox import QuickANFIS, quick_evaluate\n\n# Build ANFIS model with Gaussian MFs (FCM init for centers)\nmodel = QuickANFIS.for_regression(X, random_state=42)\nlosses = model.fit(X, y)\nmetrics = quick_evaluate(model, X, y)\n</pre> from anfis_toolbox import QuickANFIS, quick_evaluate  # Build ANFIS model with Gaussian MFs (FCM init for centers) model = QuickANFIS.for_regression(X, random_state=42) losses = model.fit(X, y) metrics = quick_evaluate(model, X, y) <pre>==================================================\nANFIS Model Evaluation Results\n==================================================\nMean Squared Error (MSE):     0.033254\nRoot Mean Squared Error:      0.182356\nMean Absolute Error (MAE):    0.145541\nR-squared (R\u00b2):               0.9402\nMean Abs. Percentage Error:   61.73%\nMaximum Error:                0.543382\nStandard Deviation of Error:  0.182356\n==================================================\n</pre> In\u00a0[20]: Copied! <pre>import matplotlib.pyplot as plt\n\ny_pred = model.predict(X)\n\nplt.scatter(X, y, s=20, alpha=0.2, label=\"y\")\nplt.plot(X, y_pred, label=\"ANFIS prediction\")\nplt.legend()\nplt.show()\n</pre> import matplotlib.pyplot as plt  y_pred = model.predict(X)  plt.scatter(X, y, s=20, alpha=0.2, label=\"y\") plt.plot(X, y_pred, label=\"ANFIS prediction\") plt.legend() plt.show()"},{"location":"examples/basic_example/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"examples/basic_example/#1-imports-and-synthetic-dataset","title":"1) Imports and synthetic dataset\u00b6","text":""},{"location":"examples/basic_example/#2-build-train-and-evaluate-anfis","title":"2) Build, train, and evaluate ANFIS\u00b6","text":"<p>We create a quick ANFIS model for regression using Gaussian membership functions. Then we:</p> <ul> <li>train with the default Hybrid trainer via <code>fit</code>,</li> <li>compute metrics with <code>quick_evaluate</code>, and</li> <li>keep the loss history to inspect training behavior.</li> </ul>"},{"location":"examples/basic_example/#3-quick-visualization-with-matplotlib","title":"3) Quick visualization with Matplotlib\u00b6","text":"<p>We compare the ground truth samples against the ANFIS prediction curve:</p> <ul> <li>The scatter shows noisy <code>y</code> values.</li> <li>The line is the model\u2019s prediction <code>y_pred = model.predict(X)</code>. This gives a fast sense of fit quality.</li> </ul>"},{"location":"examples/intermediate_example/","title":"Basic: Exploring QuickANFIS Options","text":"In\u00a0[19]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox import QuickANFIS, quick_evaluate\nfrom anfis_toolbox.optim import SGDTrainer\n\nnp.random.seed(42)  # For reproducibility\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox import QuickANFIS, quick_evaluate from anfis_toolbox.optim import SGDTrainer  np.random.seed(42)  # For reproducibility In\u00a0[20]: Copied! <pre>n = 300\nX1 = np.random.uniform(-3, 3, size=n)\nX2 = np.random.uniform(-2, 2, size=n)\nX = np.column_stack([X1, X2])\ny = np.sin(X1) + 0.3 * (X2 ** 2) + 0.1 * np.random.randn(n)\ny = y.reshape(-1, 1)\nX.shape, y.shape\n</pre> n = 300 X1 = np.random.uniform(-3, 3, size=n) X2 = np.random.uniform(-2, 2, size=n) X = np.column_stack([X1, X2]) y = np.sin(X1) + 0.3 * (X2 ** 2) + 0.1 * np.random.randn(n) y = y.reshape(-1, 1) X.shape, y.shape Out[20]: <pre>((300, 2), (300, 1))</pre> In\u00a0[\u00a0]: Copied! <pre>model = QuickANFIS.for_regression(\n    X,\n    n_mfs=3,\n    mf_type=\"gaussian\",\n    init=\"grid\"\n)\nlosses = model.fit(X, y, epochs=80, learning_rate=0.02, verbose=False)\nmetrics = quick_evaluate(model, X, y, print_results=True)\n</pre> model = QuickANFIS.for_regression(     X,     n_mfs=3,     mf_type=\"gaussian\",     init=\"grid\" ) losses = model.fit(X, y, epochs=80, learning_rate=0.02, verbose=False) metrics = quick_evaluate(model, X, y, print_results=True) <pre>==================================================\nANFIS Model Evaluation Results\n==================================================\nMean Squared Error (MSE):     0.009289\nRoot Mean Squared Error:      0.096377\nMean Absolute Error (MAE):    0.077391\nR-squared (R\u00b2):               0.9861\nMean Abs. Percentage Error:   55.15%\nMaximum Error:                0.296354\nStandard Deviation of Error:  0.096377\n==================================================\n</pre> In\u00a0[29]: Copied! <pre>mf_types = [\"gaussian\", \"triangular\", \"bell\", \"sigmoidal\"]\nresults_mf = {}\nfor mft in mf_types:\n    m = QuickANFIS.for_regression(X, n_mfs=3, mf_type=mft, init=\"grid\")\n    ls = m.fit(X, y, epochs=60, learning_rate=0.02, verbose=False)\n    results_mf[mft] = float(ls[-1])\n    print(f\"MF type: {mft}, final training loss: {ls[-1]:.4f}\")\n</pre> mf_types = [\"gaussian\", \"triangular\", \"bell\", \"sigmoidal\"] results_mf = {} for mft in mf_types:     m = QuickANFIS.for_regression(X, n_mfs=3, mf_type=mft, init=\"grid\")     ls = m.fit(X, y, epochs=60, learning_rate=0.02, verbose=False)     results_mf[mft] = float(ls[-1])     print(f\"MF type: {mft}, final training loss: {ls[-1]:.4f}\") <pre>MF type: gaussian, final training loss: 0.0093\nMF type: triangular, final training loss: 0.0119\nMF type: triangular, final training loss: 0.0119\nMF type: bell, final training loss: 0.0092\nMF type: bell, final training loss: 0.0092\nMF type: sigmoidal, final training loss: 0.0093\nMF type: sigmoidal, final training loss: 0.0093\n</pre> In\u00a0[\u00a0]: Copied! <pre># Compare grid vs fcm for the same MF type\nm_grid = QuickANFIS.for_regression(\n    X,\n    n_mfs=5,\n    mf_type=\"gaussian\",\n    init=\"grid\"\n)\nloss_grid = m_grid.fit(\n    X,\n    y,\n    epochs=60,\n    learning_rate=0.02,\n    verbose=False\n)[-1]\n\nm_fcm = QuickANFIS.for_regression(\n    X,\n    n_mfs=5,\n    mf_type=\"gaussian\",\n    init=\"fcm\",\n    random_state=42\n)\nloss_fcm = m_fcm.fit(\n    X,\n    y,\n    epochs=60,\n    learning_rate=0.02,\n    verbose=False\n)[-1]\n\nprint(f\"Final training loss (grid init): {loss_grid:.4f}\")\nprint(f\"Final training loss (fcm init): {loss_fcm:.4f}\")\n</pre> # Compare grid vs fcm for the same MF type m_grid = QuickANFIS.for_regression(     X,     n_mfs=5,     mf_type=\"gaussian\",     init=\"grid\" ) loss_grid = m_grid.fit(     X,     y,     epochs=60,     learning_rate=0.02,     verbose=False )[-1]  m_fcm = QuickANFIS.for_regression(     X,     n_mfs=5,     mf_type=\"gaussian\",     init=\"fcm\",     random_state=42 ) loss_fcm = m_fcm.fit(     X,     y,     epochs=60,     learning_rate=0.02,     verbose=False )[-1]  print(f\"Final training loss (grid init): {loss_grid:.4f}\") print(f\"Final training loss (fcm init): {loss_fcm:.4f}\") <pre>Final training loss (grid init): 0.0082\nFinal training loss (fcm init): 0.0095\n</pre> In\u00a0[31]: Copied! <pre>nmfs_list = [2, 3, 5, 7]\nresults_nmfs = {}\nfor k in nmfs_list:\n    m = QuickANFIS.for_regression(X, n_mfs=k, mf_type=\"gaussian\", init=\"fcm\", random_state=42)\n    ls = m.fit(X, y, epochs=60, learning_rate=0.02, verbose=False)\n    results_nmfs[k] = float(ls[-1])\n    print(f\"n_mfs: {k}, final training loss: {ls[-1]:.4f}\")\n</pre> nmfs_list = [2, 3, 5, 7] results_nmfs = {} for k in nmfs_list:     m = QuickANFIS.for_regression(X, n_mfs=k, mf_type=\"gaussian\", init=\"fcm\", random_state=42)     ls = m.fit(X, y, epochs=60, learning_rate=0.02, verbose=False)     results_nmfs[k] = float(ls[-1])     print(f\"n_mfs: {k}, final training loss: {ls[-1]:.4f}\") <pre>n_mfs: 2, final training loss: 0.0481\nn_mfs: 3, final training loss: 0.0192\nn_mfs: 3, final training loss: 0.0192\nn_mfs: 5, final training loss: 0.0095\nn_mfs: 5, final training loss: 0.0095\nn_mfs: 7, final training loss: 0.0054\nn_mfs: 7, final training loss: 0.0054\n</pre> In\u00a0[32]: Copied! <pre># Deterministic FCM init\nm1 = QuickANFIS.for_regression(X, n_mfs=4, mf_type=\"bell\", init=\"fcm\", random_state=123)\nloss1 = m1.fit(X, y, epochs=40, learning_rate=0.02, verbose=False)[-1]\n\nm2 = QuickANFIS.for_regression(X, n_mfs=4, mf_type=\"bell\", init=\"fcm\", random_state=123)\nloss2 = m2.fit(X, y, epochs=40, learning_rate=0.02, verbose=False)[-1]\n\n# Explicit SGD trainer\nsgd = SGDTrainer(learning_rate=0.02, epochs=40, batch_size=32, shuffle=True, verbose=False)\nm_sgd = QuickANFIS.for_regression(X, n_mfs=4, mf_type=\"bell\", init=\"fcm\", random_state=123)\nloss_sgd = m_sgd.fit(X, y, trainer=sgd)[-1]\n\n# Simple parity plot for the last model\ny_pred = m_sgd.predict(X)\nplt.scatter(y, y_pred, s=8, alpha=0.5)\nmn, mx = float(np.min(y)), float(np.max(y))\nplt.plot([mn, mx], [mn, mx], \"r--\", lw=2)\nplt.xlabel(\"y true\")\nplt.ylabel(\"y pred\")\nplt.title(\"Parity plot (SGD-trained)\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final training loss (deterministic FCM init): {loss1:.4f}\")\nprint(f\"Final training loss (SGD trainer): {loss_sgd:.4f}\")\n</pre> # Deterministic FCM init m1 = QuickANFIS.for_regression(X, n_mfs=4, mf_type=\"bell\", init=\"fcm\", random_state=123) loss1 = m1.fit(X, y, epochs=40, learning_rate=0.02, verbose=False)[-1]  m2 = QuickANFIS.for_regression(X, n_mfs=4, mf_type=\"bell\", init=\"fcm\", random_state=123) loss2 = m2.fit(X, y, epochs=40, learning_rate=0.02, verbose=False)[-1]  # Explicit SGD trainer sgd = SGDTrainer(learning_rate=0.02, epochs=40, batch_size=32, shuffle=True, verbose=False) m_sgd = QuickANFIS.for_regression(X, n_mfs=4, mf_type=\"bell\", init=\"fcm\", random_state=123) loss_sgd = m_sgd.fit(X, y, trainer=sgd)[-1]  # Simple parity plot for the last model y_pred = m_sgd.predict(X) plt.scatter(y, y_pred, s=8, alpha=0.5) mn, mx = float(np.min(y)), float(np.max(y)) plt.plot([mn, mx], [mn, mx], \"r--\", lw=2) plt.xlabel(\"y true\") plt.ylabel(\"y pred\") plt.title(\"Parity plot (SGD-trained)\") plt.tight_layout() plt.show()  print(f\"Final training loss (deterministic FCM init): {loss1:.4f}\") print(f\"Final training loss (SGD trainer): {loss_sgd:.4f}\") <pre>Final training loss (deterministic FCM init): 0.0124\nFinal training loss (SGD trainer): 0.1083\n</pre>"},{"location":"examples/intermediate_example/#basic-exploring-quickanfis-options","title":"Basic: Exploring QuickANFIS Options\u00b6","text":"<p>This notebook shows how to use <code>QuickANFIS</code> effectively: selecting membership function types, choosing initialization (grid vs FCM), controlling the number of MFs, using <code>random_state</code> for determinism, and the alternative <code>for_function_approximation</code> helper.</p>"},{"location":"examples/intermediate_example/#1-imports-and-setup","title":"1) Imports and setup\u00b6","text":"<p>We'll import <code>QuickANFIS</code> and helpers. We'll also set a random seed for reproducibility.</p>"},{"location":"examples/intermediate_example/#2-create-a-small-2d-regression-dataset","title":"2) Create a small 2D regression dataset\u00b6","text":"<p>We'll reuse this dataset to compare QuickANFIS options.</p>"},{"location":"examples/intermediate_example/#3-quick-start-defaults-gaussian-mfs-grid-init","title":"3) Quick start: defaults (gaussian MFs, grid init)\u00b6","text":"<p><code>QuickANFIS.for_regression</code> infers input ranges from data, distributes MFs on a grid, and uses Gaussian MFs by default. Training via <code>model.fit</code> uses the Hybrid trainer by default.</p>"},{"location":"examples/intermediate_example/#4-choosing-membership-function-type-mf_type","title":"4) Choosing membership function type (<code>mf_type</code>)\u00b6","text":"<p><code>mf_type</code> controls the shape family used for all inputs. Supported types include: <code>gaussian</code>, <code>triangular</code>, <code>trapezoidal</code>, <code>bell</code> (aka <code>gbell</code>), <code>sigmoidal</code> (aka <code>sigmoid</code>), <code>sshape</code> (aka <code>s</code>), <code>zshape</code> (aka <code>z</code>), and <code>pi</code> (aka <code>pimf</code>). We'll compare a few quickly.</p>"},{"location":"examples/intermediate_example/#5-initialization-initgrid-vs-initfcm","title":"5) Initialization: <code>init=\"grid\"</code> vs <code>init=\"fcm\"</code>\u00b6","text":"<ul> <li>Grid: places MFs evenly across observed range (with small margins).</li> <li>FCM: uses Fuzzy C-Means on each input column to set centers and widths; supports all MF families used above.</li> </ul>"},{"location":"examples/intermediate_example/#6-number-of-mfs-per-input-n_mfs","title":"6) Number of MFs per input (<code>n_mfs</code>)\u00b6","text":"<p>Increasing <code>n_mfs</code> adds rules and capacity. More isn't always better\u2014watch for overfitting and compute cost.</p>"},{"location":"examples/intermediate_example/#7-determinism-and-alternative-trainer","title":"7) Determinism and alternative trainer\u00b6","text":"<ul> <li>Use <code>random_state</code> with <code>init=\"fcm\"</code> to get repeatable MF initialization.</li> <li>You can also pass a trainer explicitly to <code>fit</code>, e.g., <code>SGDTrainer</code> for pure backprop.</li> </ul>"},{"location":"examples/intermediate_example/#tips-next-steps","title":"Tips &amp; Next steps\u00b6","text":"<ul> <li>Start with <code>mf_type=\"gaussian\"</code> and <code>init=\"fcm\"</code> for robust baselines.</li> <li>Sweep <code>n_mfs</code> to balance fit quality and complexity; monitor R\u00b2 or validation MSE.</li> <li>Try different MF families (<code>triangular</code>, <code>bell</code>, <code>pi</code>) when residuals show shape mismatches.</li> <li>For larger problems or streaming data, use <code>SGDTrainer</code> with mini-batches.</li> <li>For function approximation on known ranges, use <code>QuickANFIS.for_function_approximation([(min,max), ...])</code>.</li> </ul>"},{"location":"hooks/test_hook/","title":"Test hook","text":"In\u00a0[\u00a0]: Copied! <pre>def on_post_page(output, page, config):\n    path = str(page.file.src_uri)\n    if not path.endswith(\".ipynb\"):\n        return output\n\n    output = output.replace(\n        \"\"\"&lt;div class=\"highlight-ipynb hl-python\"&gt;\"\"\",\n        \"\"\"&lt;div class=\"language-python highlight\"&gt;\"\"\"\n        )\n\n    return output\n</pre> def on_post_page(output, page, config):     path = str(page.file.src_uri)     if not path.endswith(\".ipynb\"):         return output      output = output.replace(         \"\"\"\"\"\",         \"\"\"\"\"\"         )      return output"},{"location":"membership_functions/bell/","title":"Bell-shaped","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox import BellMF\n\nbellmf = BellMF(a=2, b=4, c=5)\n\nx = np.linspace(0, 10, 100)\ny = bellmf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox import BellMF  bellmf = BellMF(a=2, b=4, c=5)  x = np.linspace(0, 10, 100) y = bellmf(x)  plt.plot(x, y) plt.show() <p>The image displays a series of symmetrical bell-shaped curves. You can observe how increasing the width ($a$) makes the curve broader, while increasing the slope ($b$) makes the sides of the curve steeper.</p>"},{"location":"membership_functions/bell/#bell-shaped","title":"Bell-shaped\u00b6","text":"<p>The Generalized Bell membership function (BellMF), also known as the Bell-shaped curve, is a versatile function used in fuzzy logic to define fuzzy sets. Like other membership functions, it assigns a degree of membership to an element, but it offers greater flexibility than the GaussianMF by using an additional parameter to control its shape. Its form is a smooth, symmetrical bell curve.</p> <p>The function is defined by three parameters:</p> <ul> <li>center ($c$): This parameter determines the center of the curve, representing the point in the domain with a maximum membership value of 1.</li> <li>width ($a$): This parameter controls the width or spread of the curve. A larger value of $a$ results in a wider curve, while a smaller value produces a narrower curve.</li> <li>slope ($b$): This parameter, which must be a positive value, determines the slope of the curve's sides. It directly impacts the steepness of the curve's transition from 0 to 1. A larger $b$ value creates a steeper curve, making the fuzzy set sharper and less \"fuzzy.\"</li> </ul> <p>The mathematical formula for the Generalized Bell membership function is given by:</p> <p>$$\\mu(x) = \\frac{1}{1 + \\left|\\frac{x-c}{a}\\right|^{2b}}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$c$ is the center of the curve.</li> <li>$a$ is the width of the curve.</li> <li>$b$ is the slope of the curve.</li> </ul>"},{"location":"membership_functions/bell/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the GbellMF are essential for training and optimizing fuzzy systems. They show how the membership value changes with respect to small changes in each of the three parameters, which is vital for algorithms like backpropagation used in fuzzy-neural networks.</p> Derivative with respect to the center ($c$) <p>The partial derivative of the function with respect to its center ($c$) is:</p> <p>$$\\frac{\\partial f}{\\partial c} = \\frac{2b(x-c)}{a^2} \\left(1 + \\left|\\frac{x-c}{a}\\right|^{2b}\\right)^{-2}$$</p> <p>This derivative indicates how the membership value changes when the curve is shifted along the x-axis.</p> Derivative with respect to the width ($a$) <p>The partial derivative with respect to the width ($a$) is:</p> <p>$$\\frac{\\partial f}{\\partial a} = \\frac{2b(x-c)^2}{a^3} \\left(1 + \\left|\\frac{x-c}{a}\\right|^{2b}\\right)^{-2}$$</p> <p>This derivative helps in adjusting the spread of the fuzzy set to encompass a broader or narrower range of values.</p> Derivative with respect to the slope ($b$) <p>The partial derivative with respect to the slope ($b$) is:</p> <p>$$\\frac{\\partial f}{\\partial b} = -\\frac{2}{b} \\frac{(x-c)^2}{a^2} \\left(\\frac{1}{1 + \\left|\\frac{x-c}{a}\\right|^{2b}}\\right)^2 \\ln\\left|\\frac{x-c}{a}\\right|$$</p> <p>This derivative is used to modify the steepness of the curve, allowing for fine-tuning of the transition from non-membership to full membership.</p>"},{"location":"membership_functions/bell/#python-example","title":"Python Example\u00b6","text":"<p>The following code demonstrates how to generate a Generalized Bell membership function using the <code>numpy</code> and <code>matplotlib</code> libraries in Python.</p>"},{"location":"membership_functions/bell/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of a Generalized Bell membership function, showing how its shape is influenced by the width ($a$) and slope ($b$) parameters, while the center ($c$) remains fixed.</p>"},{"location":"membership_functions/gaussian-combination/","title":"Gaussian Combination","text":"<p>The Gaussian combination membership function (Gaussian2MF) is a versatile fuzzy membership function that combines two Gaussian curves with an optional flat region in between. This function is particularly useful for representing fuzzy sets with asymmetric shapes or plateau regions, making it suitable for applications where the membership degree needs to be constant over a range of values.</p> <p>The function is characterized by four main parameters:</p> <ul> <li>sigma1 ($\\sigma_1$): Standard deviation of the left Gaussian tail (must be positive).</li> <li>c1 ($c_1$): Center of the left Gaussian tail.</li> <li>sigma2 ($\\sigma_2$): Standard deviation of the right Gaussian tail (must be positive).</li> <li>c2 ($c_2$): Center of the right Gaussian tail. Must satisfy $c_1 \\leq c_2$.</li> </ul> <p>The mathematical formula for the Gaussian combination membership function is defined piecewise:</p> <p>$$\\mu(x) =  \\begin{array}{ll}  e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}} &amp; x &lt; c_1 \\\\[6pt]  1 &amp; c_1 \\leq x \\leq c_2 \\\\[6pt]  e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}} &amp; x &gt; c_2  \\end{array}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$\\sigma_1, c_1$ define the left Gaussian tail.</li> <li>$\\sigma_2, c_2$ define the right Gaussian tail.</li> </ul> <p>When $c_1 = c_2$, the function becomes an asymmetric Gaussian centered at $c_1$ with different spreads on each side.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import Gaussian2MF\n\n\ngaussian2 = Gaussian2MF(sigma1=.5, c1=2, sigma2=1, c2=5)\n\nx = np.linspace(0, 10, 100)\ny = gaussian2(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import Gaussian2MF   gaussian2 = Gaussian2MF(sigma1=.5, c1=2, sigma2=1, c2=5)  x = np.linspace(0, 10, 100) y = gaussian2(x)  plt.plot(x, y) plt.show() <p>The visualization above demonstrates various configurations of the Gaussian2MF. This flexibility makes Gaussian2MF suitable for modeling complex fuzzy concepts with asymmetric uncertainty or plateau regions where membership should remain constant.</p>"},{"location":"membership_functions/gaussian-combination/#gaussian-combination","title":"Gaussian Combination\u00b6","text":""},{"location":"membership_functions/gaussian-combination/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the Gaussian combination membership function are essential for optimization in adaptive fuzzy systems. Since the function is piecewise, derivatives are computed separately for each region.</p> Derivative with respect to $c_1$ <p>For $x &lt; c_1$: $$\\frac{\\partial \\mu}{\\partial c_1} = \\frac{x-c_1}{\\sigma_1^2} e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}}$$</p> <p>For $c_1 \\leq x \\leq c_2$: The derivative is 0 (flat region).</p> <p>For $x &gt; c_2$: The derivative is 0.</p> Derivative with respect to $\\sigma_1$ <p>For $x &lt; c_1$: $$\\frac{\\partial \\mu}{\\partial \\sigma_1} = \\frac{(x-c_1)^2}{\\sigma_1^3} e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $c_2$ <p>For $x &lt; c_1$: The derivative is 0.</p> <p>For $c_1 \\leq x \\leq c_2$: The derivative is 0.</p> <p>For $x &gt; c_2$: $$\\frac{\\partial \\mu}{\\partial c_2} = \\frac{x-c_2}{\\sigma_2^2} e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}}$$</p> Derivative with respect to $\\sigma_2$ <p>For $x &gt; c_2$: $$\\frac{\\partial \\mu}{\\partial \\sigma_2} = \\frac{(x-c_2)^2}{\\sigma_2^3} e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}}$$</p> <p>For other regions: The derivative is 0.</p> <p>These derivatives enable gradient-based optimization of the membership function parameters.</p>"},{"location":"membership_functions/gaussian-combination/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/gaussian-combination/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the Gaussian2MF shape changes with different parameter combinations. We'll explore variations in the centers (c\u2081, c\u2082) and standard deviations (\u03c3\u2081, \u03c3\u2082).</p>"},{"location":"membership_functions/gaussian/","title":"Gaussian","text":"<p>The Gaussian membership function (GaussianMF) is a fundamental concept in fuzzy logic, widely used to define fuzzy sets. Unlike a classical set where an element either fully belongs or does not belong, a fuzzy set allows for partial membership, and the GaussianMF provides a smooth, continuous way to represent this degree of belonging. It possesses a smooth, bell-like shape, which contributes to its intuitive nature and popularity across various applications.</p> <p>The function is characterized by two main parameters:</p> <ul> <li>mean ($\\mu$): This parameter determines the center of the curve. It represents the point in the domain where the degree of membership is maximum, specifically 1.</li> <li>sigma ($\\sigma$): This parameter controls the width or spread of the curve. It must be a positive value. A larger $\\sigma$ results in a wider, flatter curve, indicating a broader range of values with high membership. Conversely, a smaller $\\sigma$ produces a sharper, more peaked curve, suggesting a narrower range of values with a high degree of belonging.</li> </ul> <p>The mathematical formula for the Gaussian membership function is given by:</p> <p>$$\\mu(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$\\mu$ is the mean of the curve.</li> <li>$\\sigma$ is the standard deviation (width) of the curve.</li> </ul> <p>The partial derivatives of the Gaussian membership function are crucial for optimization algorithms, especially in adaptive or machine learning-based fuzzy systems. They show how the membership value changes in response to small adjustments to the parameters, which is essential for training models to better fit data.</p> Derivative with respect to $\\mu$ <p>The partial derivative of the function with respect to the mean ($\\mu$) is:</p> <p>$$\\frac{\\partial f}{\\partial \\mu} = \\frac{x-\\mu}{\\sigma^2} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>This derivative indicates how the membership value is affected when the center of the bell curve is shifted. It is used to adjust the position of the function to better align with the data.</p> Derivative with respect to $\\sigma$** <p>The partial derivative with respect to the standard deviation ($\\sigma$) is:</p> <p>$$\\frac{\\partial f}{\\partial \\sigma} = \\frac{(x-\\mu)^2}{\\sigma^3} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>This derivative shows how the membership value changes as the width of the curve is adjusted. It is used to refine the spread of the function, making it sharper or wider as needed to represent the uncertainty in the data more accurately.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox import GaussianMF\n\ngaussian = GaussianMF(5, 1)\n\nx = np.linspace(0, 10, 100)\ny = gaussian(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox import GaussianMF  gaussian = GaussianMF(5, 1)  x = np.linspace(0, 10, 100) y = gaussian(x)  plt.plot(x, y) plt.show() <p>Below is a visual representation of a Gaussian membership function, showing how its shape is influenced by the mean $\\mu$ and sigma ($\\sigma$) parameters.</p> <p>The image displays a classic bell-shaped curve, illustrating how the membership value (on the y-axis) smoothly changes for different input values (on the x-axis). The peak of the curve is located at the mean, and the spread of the curve is controlled by sigma.</p>"},{"location":"membership_functions/gaussian/#gaussian","title":"Gaussian\u00b6","text":""},{"location":"membership_functions/gaussian/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"membership_functions/gaussian/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/gaussian/#visualization","title":"Visualization\u00b6","text":""},{"location":"membership_functions/pi/","title":"Pi-shaped","text":"<p>The Pi-shaped Membership Function is defined piecewise with smooth transitions:</p> <p>$$\\mu(x) = \\begin{array}{ll}  S(x; a, b) &amp; a \\leq x \\leq b \\\\[6pt] 1 &amp; b \\leq x \\leq c \\\\[6pt] Z(x; c, d) &amp; c \\leq x \\leq d \\\\[6pt] 0 &amp; \\text{otherwise} \\end{array}$$</p> <p>Where:</p> <ul> <li>S(x; a, b) is the rising S-shaped function: smooth transition from 0 to 1</li> <li>Z(x; c, d) is the falling Z-shaped function: smooth transition from 1 to 0</li> </ul> <p>The smooth transitions use cubic smoothstep functions:</p> <p>S-function (rising edge): $$S(x; a, b) = 3t^2 - 2t^3 \\quad \\text{where} \\quad t = \\frac{x - a}{b - a}$$</p> <p>Z-function (falling edge): $$Z(x; c, d) = 1 - S(x; c, d) = 1 - (3t^2 - 2t^3) \\quad \\text{where} \\quad t = \\frac{x - c}{d - c}$$</p> <p>The function is characterized by four parameters:</p> <ul> <li>a: Left foot - where the function starts rising from 0</li> <li>b: Left shoulder - where the function reaches the plateau (\u03bc = 1)</li> <li>c: Right shoulder - where the function starts falling from the plateau</li> <li>d: Right foot - where the function reaches 0</li> </ul> <p>For a valid Pi-shaped function, parameters must satisfy: a &lt; b \u2264 c &lt; d</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import PiMF\n\npimf = PiMF(a=1, b=3, c=7, d=9)\n\nx = np.linspace(0, 10, 200)\ny = pimf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import PiMF  pimf = PiMF(a=1, b=3, c=7, d=9)  x = np.linspace(0, 10, 200) y = pimf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/pi/#pi-shaped","title":"Pi-shaped\u00b6","text":"<p>The Pi-shaped Membership Function creates a smooth bell-like curve with a flat plateau at the top. It combines S-shaped rising and Z-shaped falling edges with a trapezoidal-like plateau, making it ideal for representing concepts with gradual transitions and stable regions.</p>"},{"location":"membership_functions/pi/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The PiMF has analytical gradients computed by region:</p> S-Function Region (a \u2264 x \u2264 b) For the rising edge: \u03bc(x) = S(x; a, b) = 3t\u00b2 - 2t\u00b3 where t = (x-a)/(b-a)  <ul> <li>\u2202\u03bc/\u2202a = dS/dt \u00b7 dt/da = [6t(1-t)] \u00b7 [(x-b)/(b-a)\u00b2]</li> <li>\u2202\u03bc/\u2202b = dS/dt \u00b7 dt/db = [6t(1-t)] \u00b7 [-(x-a)/(b-a)\u00b2]</li> <li>\u2202\u03bc/\u2202c = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202d = 0 (no effect in this region)</li> </ul> Plateau Region (b \u2264 x \u2264 c) \u03bc(x) = 1 (constant function)  <ul> <li>\u2202\u03bc/\u2202a = 0 (constant function)</li> <li>\u2202\u03bc/\u2202b = 0 (constant function)</li> <li>\u2202\u03bc/\u2202c = 0 (constant function)</li> <li>\u2202\u03bc/\u2202d = 0 (constant function)</li> </ul> Z-Function Region (c \u2264 x \u2264 d) For the falling edge: \u03bc(x) = Z(x; c, d) = 1 - S(x; c, d) where t = (x-c)/(d-c)  <ul> <li>\u2202\u03bc/\u2202a = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202b = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202c = dZ/dt \u00b7 dt/dc = [-6t(1-t)] \u00b7 [(x-d)/(d-c)\u00b2]</li> <li>\u2202\u03bc/\u2202d = dZ/dt \u00b7 dt/dd = [-6t(1-t)] \u00b7 [-(x-c)/(d-c)\u00b2]</li> </ul>"},{"location":"membership_functions/pi/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a Pi-shaped membership function and visualize it:</p>"},{"location":"membership_functions/pi/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different Pi-shaped membership functions with varying parameter combinations. Each subplot demonstrates how the plateau width and transition smoothness affect the overall shape.</p>"},{"location":"membership_functions/sigmoidal-difference/","title":"Difference of Sigmoidal","text":"<p>The Difference of Sigmoidal Membership Functions is defined as:</p> <p>$$\\mu(x) = s_1(x) - s_2(x)$$</p> <p>Where each sigmoid function is:</p> <p>$$s_1(x) = \\frac{1}{1 + e^{-a_1(x - c_1)}}$$ $$s_2(x) = \\frac{1}{1 + e^{-a_2(x - c_2)}}$$</p> <p>The function is characterized by four parameters (two for each sigmoid):</p> <ul> <li><p>a\u2081: Slope parameter for the first sigmoid (s\u2081)</p> <ul> <li>Positive values: standard sigmoid (0 \u2192 1 as x increases)</li> <li>Negative values: inverted sigmoid (1 \u2192 0 as x increases)</li> <li>Larger |a\u2081|: steeper transition for s\u2081</li> </ul> </li> <li><p>c\u2081: Center parameter for the first sigmoid (s\u2081)</p> <ul> <li>Controls the inflection point where s\u2081(c\u2081) = 0.5</li> <li>Shifts s\u2081 left/right along the x-axis</li> </ul> </li> <li><p>a\u2082: Slope parameter for the second sigmoid (s\u2082)</p> <ul> <li>Same interpretation as a\u2081 but for s\u2082</li> </ul> </li> <li><p>c\u2082: Center parameter for the second sigmoid (s\u2082)</p> <ul> <li>Same interpretation as c\u2081 but for s\u2082</li> </ul> </li> <li><p>a\u2081 \u2260 0 and a\u2082 \u2260 0: Cannot be zero (would result in constant functions)</p> </li> <li><p>All parameters can be any real numbers otherwise</p> </li> </ul> In\u00a0[5]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import DiffSigmoidalMF\n\ndiff_sigmoid = DiffSigmoidalMF(a1=2, c1=4, a2=5, c2=8)\n\nx = np.linspace(0, 10, 100)\ny = diff_sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import DiffSigmoidalMF  diff_sigmoid = DiffSigmoidalMF(a1=2, c1=4, a2=5, c2=8)  x = np.linspace(0, 10, 100) y = diff_sigmoid(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal-difference/#difference-of-sigmoidal","title":"Difference of Sigmoidal\u00b6","text":"<p>The Difference of Sigmoidal Membership Functions implements \u03bc(x) = s\u2081(x) - s\u2082(x), where each s is a logistic curve with its own slope and center parameters. This creates complex membership shapes by combining two sigmoid functions.</p>"},{"location":"membership_functions/sigmoidal-difference/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. Since \u03bc(x) = s\u2081(x) - s\u2082(x), the derivatives follow from the chain rule:</p> Derivative w.r.t. Parameters of First Sigmoid (s\u2081) <p>For s\u2081(x) = 1/(1 + exp(-a\u2081(x - c\u2081))):</p> <ul> <li>\u2202\u03bc/\u2202a\u2081 = \u2202s\u2081/\u2202a\u2081 = s\u2081(x) \u00b7 (1 - s\u2081(x)) \u00b7 (x - c\u2081)</li> <li>\u2202\u03bc/\u2202c\u2081 = \u2202s\u2081/\u2202c\u2081 = -a\u2081 \u00b7 s\u2081(x) \u00b7 (1 - s\u2081(x))</li> </ul> Derivative w.r.t. Parameters of Second Sigmoid (s\u2082) <p>For s\u2082(x) = 1/(1 + exp(-a\u2082(x - c\u2082))), and since \u03bc(x) = s\u2081(x) - s\u2082(x):</p> <ul> <li>\u2202\u03bc/\u2202a\u2082 = -\u2202s\u2082/\u2202a\u2082 = -s\u2082(x) \u00b7 (1 - s\u2082(x)) \u00b7 (x - c\u2082)</li> <li>\u2202\u03bc/\u2202c\u2082 = -\u2202s\u2082/\u2202c\u2082 = -(-a\u2082 \u00b7 s\u2082(x) \u00b7 (1 - s\u2082(x))) = a\u2082 \u00b7 s\u2082(x) \u00b7 (1 - s\u2082(x))</li> </ul> Derivative w.r.t. Input (Optional) <p>For chaining in neural networks:</p> <p>$$\\frac{d\\mu}{dx} = \\frac{ds_1}{dx} - \\frac{ds_2}{dx}$$</p> <p>Where: $$\\frac{ds_1}{dx} = a_1 \\cdot s_1(x) \\cdot (1 - s_1(x))$$ $$\\frac{ds_2}{dx} = a_2 \\cdot s_2(x) \\cdot (1 - s_2(x))$$</p> Gradient Computation Details <p>The gradients are computed using the fundamental sigmoid derivative property: $$\\frac{d}{dx}\\left(\\frac{1}{1+e^{-z}}\\right) = \\frac{1}{1+e^{-z}} \\cdot \\left(1 - \\frac{1}{1+e^{-z}}\\right) = s(x) \\cdot (1 - s(x))$$</p> <p>This property is used extensively in neural network backpropagation and makes the DiffSigmoidalMF computationally efficient for optimization.</p>"},{"location":"membership_functions/sigmoidal-difference/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a difference of sigmoidal membership functions and visualize its components:</p>"},{"location":"membership_functions/sigmoidal-difference/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different difference of sigmoidal membership functions with varying parameter combinations. Each subplot demonstrates how the combination of two sigmoids creates complex membership shapes.</p>"},{"location":"membership_functions/sigmoidal-product/","title":"Product of Sigmoidal","text":"<p>The function is defined as the product of two sigmoidal functions:</p> <p>$$\\mu(x) = f_1(x) \\cdot f_2(x)$$ $$f_1(x) = \\frac{1}{1 + e^{-a_1(x-c_1)}}$$ $$f_2(x) = \\frac{1}{1 + e^{-a_2(x-c_2)}}$$</p> <p>The function is controlled by four parameters, two for each component sigmoid:</p> <ul> <li><p>$a_1$: Slope of the first sigmoid function.</p> </li> <li><p>$c_1$: Center (inflection point) of the first sigmoid function.</p> </li> <li><p>$a_2$: Slope of the second sigmoid function.</p> </li> <li><p>$c_2$: Center (inflection point) of the second sigmoid function.</p> </li> <li><p>To form a proper bell-shaped curve, the slopes $a_1$ and $a_2$ must have opposite signs (e.g., if $a_1 &gt; 0$, then $a_2 &lt; 0$).</p> </li> <li><p>The centers $c_1$ and $c_2$ determine the width and position of the curve's peak.</p> </li> </ul> In\u00a0[12]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import ProdSigmoidalMF\n\nmf = ProdSigmoidalMF(a1=2, c1=3, a2=-2, c2=4)\n\nx = np.linspace(0, 10, 400)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import ProdSigmoidalMF  mf = ProdSigmoidalMF(a1=2, c1=3, a2=-2, c2=4)  x = np.linspace(0, 10, 400) y = mf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal-product/#product-of-sigmoidal","title":"Product of Sigmoidal\u00b6","text":"<p>The Product of Sigmoidal Membership Function creates a smooth, asymmetrical, bell-shaped curve by multiplying two distinct sigmoidal functions. It's used to model fuzzy sets that require a gradual but non-uniform transition, offering more flexibility than symmetric functions.</p>"},{"location":"membership_functions/sigmoidal-product/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives with respect to the parameters $a_1, c_1, a_2,$ and $c_2$ are:</p> <ol> <li><p>With respect to $a_1$: $$\\frac{\\partial \\mu}{\\partial a_1} = (x-c_1) \\cdot (1 - f_1(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $c_1$: $$\\frac{\\partial \\mu}{\\partial c_1} = -a_1 \\cdot (1 - f_1(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $a_2$: $$\\frac{\\partial \\mu}{\\partial a_2} = (x-c_2) \\cdot (1 - f_2(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $c_2$: $$\\frac{\\partial \\mu}{\\partial c_2} = -a_2 \\cdot (1 - f_2(x)) \\cdot \\mu(x)$$</p> </li> </ol>"},{"location":"membership_functions/sigmoidal-product/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/sigmoidal-product/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the ProdSigmoidalMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/sigmoidal/","title":"Sigmoidal","text":"<p>The Sigmoidal Membership Function is defined by the logistic function:</p> <p>$$\\mu(x) = \\frac{1}{1 + e^{-a(x - c)}}$$</p> <p>The function is characterized by two parameters:</p> <ul> <li><p>a: Slope parameter - controls the steepness of the S-curve</p> <ul> <li>Positive values: standard sigmoid (0 \u2192 1 as x increases)</li> <li>Negative values: inverted sigmoid (1 \u2192 0 as x increases)</li> <li>Larger |a|: steeper transition (more abrupt change)</li> <li>Smaller |a|: gentler transition (more gradual change)</li> </ul> </li> <li><p>c: Center parameter - controls the inflection point where \u03bc(c) = 0.5</p> <ul> <li>\u03bc(c) = 0.5 (50% membership)</li> <li>Shifts the curve left/right along the x-axis</li> </ul> </li> <li><p>a \u2260 0: Cannot be zero (would result in constant function \u03bc(x) = 0.5)</p> </li> <li><p>a and c can be any real numbers otherwise</p> </li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import SigmoidalMF\n\nsigmoid = SigmoidalMF(a=2, c=0)\n\nx = np.linspace(-5, 5, 200)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import SigmoidalMF  sigmoid = SigmoidalMF(a=2, c=0)  x = np.linspace(-5, 5, 200) y = sigmoid(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal/#sigmoidal","title":"Sigmoidal\u00b6","text":"<p>The Sigmoidal Membership Function implements a smooth S-shaped curve that transitions gradually from 0 to 1. It is widely used in fuzzy logic systems and neural networks for modeling smooth transitions and gradual changes.</p>"},{"location":"membership_functions/sigmoidal/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The sigmoid function has elegant derivative properties:</p> Derivative of the Sigmoid Function For \u03bc(x) = 1/(1 + e^{-a(x-c)}), the derivative with respect to x is:  <p>$$\\frac{d\\mu}{dx} = \\mu(x) \\cdot (1 - \\mu(x)) \\cdot a$$</p> <p>This is a fundamental property of the sigmoid function and is used extensively in neural networks.</p> Partial Derivatives w.r.t. Parameters <p>For \u03bc(x) = 1/(1 + exp(-a(x-c))):</p> <ul> <li>\u2202\u03bc/\u2202a = \u03bc(x) \u00b7 (1 - \u03bc(x)) \u00b7 (x - c)</li> <li>\u2202\u03bc/\u2202c = -a \u00b7 \u03bc(x) \u00b7 (1 - \u03bc(x))</li> </ul>"},{"location":"membership_functions/sigmoidal/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a sigmoidal membership function and visualize it:</p>"},{"location":"membership_functions/sigmoidal/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different sigmoidal membership functions with varying parameter combinations. Each subplot demonstrates how the slope (a) and center (c) parameters affect the shape of the S-curve.</p>"},{"location":"membership_functions/sshaped-linear/","title":"Linear S-shaped","text":"<p>The mathematical formula for the linear S-shaped membership function is given by:</p> <ul> <li>$\u03bc(x) = 0$, for $x \\le a$</li> <li>$\u03bc(x) = (x - a) / (b - a)$, for $a &lt; x &lt; b$</li> <li>$\u03bc(x) = 1$, for $x \\ge b$</li> </ul> <p>Where:</p> <ul> <li>$\u03bc(x)$ is the degree of membership for element $x$ in the fuzzy set.</li> <li>$a$ and $b$ are the parameters that define the start and end of the linear ramp.</li> </ul> <p>The function is characterized by two main parameters:</p> <ul> <li><code>a</code> (start point): The \"left foot\" of the curve. This is the point where the membership transition begins from 0.</li> <li><code>b</code> (end point): The \"right shoulder\" of the curve. This is the point where the membership reaches and stays at 1. The parameter <code>b</code> must always be greater than <code>a</code>.</li> </ul> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import LinSShapedMF\n\nmf = LinSShapedMF(a=3, b=7)\n\nx = np.linspace(0, 10, 100)\ny = mf.forward(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import LinSShapedMF  mf = LinSShapedMF(a=3, b=7)  x = np.linspace(0, 10, 100) y = mf.forward(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sshaped-linear/#linear-s-shaped","title":"Linear S-shaped\u00b6","text":"<p>The linear S-shaped membership function (<code>LinSShapedMF</code>) is a fundamental concept in fuzzy logic, used to define fuzzy sets. Unlike a classical set where an element either fully belongs or doesn't, a fuzzy set allows for partial membership, and the <code>LinSShapedMF</code> provides a smooth, continuous way to represent this degree of belonging.</p> <p>It's a piecewise linear function that transitions from 0 to 1 over a defined interval.</p>"},{"location":"membership_functions/sshaped-linear/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives of the membership function are crucial for optimization algorithms, such as backpropagation, which allow the system to adapt. They indicate how the membership value changes in response to small adjustments in the parameters <code>a</code> and <code>b</code>.</p> Derivative with respect to `a` ($\\frac{\\partial \\mu}{\\partial a}$) <p>The partial derivative with respect to <code>a</code> indicates how the membership value is affected when the starting point of the ramp is adjusted.</p> <p>$\\frac{\\partial \\mu}{\\partial a} = -\\frac{1}{b-a}$ (for the ramp region)</p> Derivative with respect to `b` ($\\frac{\\partial \\mu}{\\partial b}$) <p>The partial derivative with respect to <code>b</code> shows how the membership value changes when the end point of the ramp is adjusted.</p> <p>$\\frac{\\partial \\mu}{\\partial b} = \\frac{x-a}{(b-a)^2}$ (for the ramp region)</p>"},{"location":"membership_functions/sshaped-linear/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of the S-shaped membership function, showing how its shape is influenced by the parameters <code>a</code> and <code>b</code>.</p>"},{"location":"membership_functions/sshaped/","title":"S-shaped","text":"<p>The S-shaped function's curve is defined by two main parameters:</p> <ul> <li>$a$: The point where the function begins to rise from a membership degree of 0.0.</li> <li>$b$: The point where the function reaches a membership degree of 1.0.</li> </ul> <p>The transition between these two points is described by the following equation:</p> <p>$$ S(x; a, b) = \\begin{array}{ll} 0 &amp; x \\le a \\\\[6pt] 2 \\left( \\frac{x-a}{b-a} \\right)^2 &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt] 1 - 2 \\left( \\frac{x-b}{b-a} \\right)^2 &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt] 1 &amp; x \\ge b \\end{array}{ll} $$</p> <p>This formulation ensures a smooth and continuous transition between the different segments of the curve, which is fundamental for representing uncertainties and imprecision in fuzzy logic systems.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom anfis_toolbox.membership import SShapedMF\n\n\nmf = SShapedMF(a=2, b=8)\n\nx = np.linspace(0, 10, 200)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from anfis_toolbox.membership import SShapedMF   mf = SShapedMF(a=2, b=8)  x = np.linspace(0, 10, 200) y = mf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sshaped/#s-shaped","title":"S-shaped\u00b6","text":"<p>The S-shaped Membership Function (<code>SShapedMF</code>) is a type of membership function used in fuzzy logic. It models the gradual transition from a zero degree of membership (0.0) to a full degree (1.0). This form is ideal for representing concepts like \"hot\" or \"fast,\" where membership starts low and progressively increases to a certain point. The transition is defined by a cubic polynomial, resulting in a smooth, continuous curve without angular points.</p>"},{"location":"membership_functions/sshaped/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>To optimize the shape of the S-shaped membership function for a specific application, it's often necessary to calculate the partial derivatives with respect to its parameters, $a$ and $b$. These derivatives are crucial for optimization algorithms like gradient descent.</p> Partial Derivative with Respect to $a$ <p>The partial derivative of the S-shaped function with respect to the parameter $a$ is calculated as follows:</p> <p>$$ \\frac{\\partial S}{\\partial a} = \\begin{array}{ll}   0 &amp; x \\le a \\\\[6pt]   \\frac{-4(x-a)}{(b-a)^2} + \\frac{4(x-a)^2}{(b-a)^3} &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt]   \\frac{4(x-b)^2}{(b-a)^3} &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt]   0 &amp; x \\ge b \\end{array} $$</p> <p>This derivative shows how the membership value changes as the starting point of the curve, $a$, is adjusted.</p> Partial Derivative with Respect to $b$ <p>The partial derivative of the S-shaped function with respect to the parameter $b$ is calculated as follows:</p> <p>$$ \\frac{\\partial S}{\\partial b} = \\begin{array}{ll}   0 &amp; x \\le a \\\\[6pt]   \\frac{-4(x-a)^2}{(b-a)^3} &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt]   \\frac{-4(x-b)}{(b-a)^2} + \\frac{4(x-b)^2}{(b-a)^3} &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt]   0 &amp; x \\ge b \\end{array} $$</p> <p>This derivative indicates how the membership value changes as the ending point of the curve, $b$, is adjusted.</p> <p>These partial derivatives are essential tools for tuning the S-shaped membership function to better fit data or to meet specific system requirements. They enable gradient-based optimization by providing the direction and magnitude of the steepest ascent/descent for the parameters.</p>"},{"location":"membership_functions/sshaped/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/sshaped/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of the S-shaped membership function, showing how its shape is influenced by the parameters <code>a</code> and <code>b</code>.</p>"},{"location":"membership_functions/trapezoidal/","title":"Trapezoidal","text":"<p>The Trapezoidal Membership Function is defined by the piecewise linear equation:</p> <p>$$\\mu(x) =  \\begin{array}{ll} 0 &amp; x \\leq a \\\\ \\frac{x - a}{b - a} &amp; a &lt; x &lt; b \\\\ 1 &amp; b \\leq x \\leq c \\\\ \\frac{d - x}{d - c} &amp; c &lt; x &lt; d \\\\ 0 &amp; x \\geq d  \\end{array}$$</p> <p>The function is characterized by four parameters:</p> <ul> <li>a: Left base point (lower support bound) - where \u03bc(x) starts increasing from 0</li> <li>b: Left peak point (start of plateau) - where \u03bc(x) reaches 1</li> <li>c: Right peak point (end of plateau) - where \u03bc(x) starts decreasing from 1</li> <li>d: Right base point (upper support bound) - where \u03bc(x) returns to 0</li> </ul> <p>Parameter Constraints For a valid trapezoidal function, parameters must satisfy: a \u2264 b \u2264 c \u2264 d</p> <p>Geometric Interpretation</p> <ul> <li>The region [a, b] forms the left slope (rising edge)</li> <li>The region [b, c] forms the plateau (full membership region)</li> <li>The region [c, d] forms the right slope (falling edge)</li> <li>Outside [a, d], membership is zero</li> </ul> <p>This shape is particularly useful when you need:</p> <ul> <li>A stable region of full membership (plateau)</li> <li>Gradual transitions at the boundaries</li> <li>Robustness to small variations in input values</li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import TrapezoidalMF\n\ntrapezoidal = TrapezoidalMF(a=2, b=4, c=6, d=8)\n\n\nx = np.linspace(0, 10, 200)\ny = trapezoidal(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import TrapezoidalMF  trapezoidal = TrapezoidalMF(a=2, b=4, c=6, d=8)   x = np.linspace(0, 10, 200) y = trapezoidal(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/trapezoidal/#trapezoidal","title":"Trapezoidal\u00b6","text":"<p>The Trapezoidal Membership Function is a piecewise linear function that creates a trapezoid-shaped membership curve. It is widely used in fuzzy logic systems when you need a plateau region of full membership, providing robustness to noise and uncertainty.</p>"},{"location":"membership_functions/trapezoidal/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The derivatives are computed analytically for each region:</p> Left Slope Region (a &lt; x &lt; b) $$\\mu(x) = \\frac{x - a}{b - a}$$  <ul> <li>\u2202\u03bc/\u2202a = -1/(b-a)</li> <li>\u2202\u03bc/\u2202b = -(x-a)/(b-a)\u00b2</li> <li>\u2202\u03bc/\u2202c = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202d = 0 (no effect in this region)</li> </ul> Plateau Region (b \u2264 x \u2264 c) $$\\mu(x) = 1$$  <ul> <li>\u2202\u03bc/\u2202a = 0 (constant function)</li> <li>\u2202\u03bc/\u2202b = 0 (constant function)</li> <li>\u2202\u03bc/\u2202c = 0 (constant function)</li> <li>\u2202\u03bc/\u2202d = 0 (constant function)</li> </ul> Right Slope Region (c &lt; x &lt; d) $$\\mu(x) = \\frac{d - x}{d - c}$$  <ul> <li>\u2202\u03bc/\u2202a = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202b = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202c = (x-d)/(d-c)\u00b2</li> <li>\u2202\u03bc/\u2202d = (x-c)/(d-c)\u00b2</li> </ul>"},{"location":"membership_functions/trapezoidal/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a trapezoidal membership function and visualize it:</p>"},{"location":"membership_functions/trapezoidal/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different trapezoidal membership functions with varying parameter combinations. Each subplot demonstrates how the shape changes with different plateau widths and slope characteristics.</p>"},{"location":"membership_functions/triangular/","title":"Triangular","text":"<p>The Triangular membership function (TriangularMF) is one of the most fundamental and widely used membership functions in fuzzy logic. It represents fuzzy sets with a simple triangular shape, making it intuitive and computationally efficient. The function is defined by three key points that form the triangle: the left base point, the peak, and the right base point.</p> <p>The function is characterized by three main parameters:</p> <ul> <li>a: Left base point of the triangle (\u03bc(x) = 0 for x \u2264 a).</li> <li>b: Peak point of the triangle (\u03bc(b) = 1, the maximum membership value).</li> <li>c: Right base point of the triangle (\u03bc(x) = 0 for x \u2265 c).</li> </ul> <p>These parameters must satisfy the constraint: a \u2264 b \u2264 c.</p> <p>The mathematical formula for the triangular membership function is defined piecewise:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 0, &amp; x \\leq a, \\\\[4pt] \\dfrac{x-a}{b-a}, &amp; a &lt; x \\leq b, \\\\[4pt] \\dfrac{c-x}{c-b}, &amp; b &lt; x &lt; c, \\\\[4pt] 0, &amp; x \\geq c. \\end{array} $$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$a$ is the left base point.</li> <li>$b$ is the peak point.</li> <li>$c$ is the right base point.</li> </ul> <p>The triangular membership function is particularly useful for representing concepts like \"approximately equal to b\" or \"around b\", where the membership decreases linearly as we move away from the peak value.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import TriangularMF\n\ntriangular = TriangularMF(a=2, b=5, c=8)\n\nx = np.linspace(0, 10, 100)\ny = triangular(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import TriangularMF  triangular = TriangularMF(a=2, b=5, c=8)  x = np.linspace(0, 10, 100) y = triangular(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/triangular/#triangular","title":"Triangular\u00b6","text":""},{"location":"membership_functions/triangular/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the triangular membership function are essential for optimization in adaptive fuzzy systems. Since the function is piecewise linear, the derivatives are computed separately for each region.</p> Derivative with respect to $a$ <p>For $a &lt; x &lt; b$: $$\\frac{\\partial \\mu}{\\partial a} = \\frac{x-b}{(b-a)^2}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $b$ <p>For $a &lt; x &lt; b$: $$\\frac{\\partial \\mu}{\\partial b} = \\frac{-(x-a)}{(b-a)^2}$$</p> <p>For $b &lt; x &lt; c$: $$\\frac{\\partial \\mu}{\\partial b} = \\frac{x-c}{(c-b)^2}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $c$ <p>For $b &lt; x &lt; c$: $$\\frac{\\partial \\mu}{\\partial c} = \\frac{x-b}{(c-b)^2}$$</p> <p>For other regions: The derivative is 0.</p> <p>These derivatives enable gradient-based optimization of the triangular membership function parameters, allowing the triangle to adapt its shape during training to better fit the data.</p>"},{"location":"membership_functions/triangular/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/triangular/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the TriangularMF shape changes with different parameter combinations. We'll explore variations in the base points (a, c) and peak position (b).</p>"},{"location":"membership_functions/zshaped-linear/","title":"Linear Z-shaped","text":"<p>The formula for the <code>LinZShapedMF</code> is a piecewise linear function:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 1, &amp; x \\le a \\\\[6pt] \\frac{b - x}{b - a}, &amp; a &lt; x &lt; b \\\\[6pt] 0, &amp; x \\ge b \\end{array} $$</p> <p>Where $\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</p> <p>The function is defined by two parameters that delimit the linear transition region:</p> <ul> <li><code>a</code> (Left Shoulder): The point where the membership value begins to transition from 1.0. For input values less than or equal to <code>a</code>, the membership is always 1.0.</li> <li><code>b</code> (Right Foot): The point where the membership value reaches and stays at zero. For input values greater than or equal to <code>b</code>, the membership is always 0.0.</li> </ul> <p>It is crucial that the parameter <code>a</code> is less than <code>b</code> for the linear transition to occur correctly.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import LinZShapedMF\n\nmf = LinZShapedMF(a=15, b=35)\n\nx = np.linspace(0, 50, 100)\ny = mf.forward(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import LinZShapedMF  mf = LinZShapedMF(a=15, b=35)  x = np.linspace(0, 50, 100) y = mf.forward(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/zshaped-linear/#linear-z-shaped","title":"Linear Z-shaped\u00b6","text":"<p>The Linear Z-shaped Membership Function (<code>LinZShapedMF</code>) is a type of membership function in fuzzy logic that represents a smooth linear transition from a full degree of membership (1.0) to zero. Its shape is ideal for modeling concepts like \"cold\" or \"low,\" where membership is high up to a certain point and then decreases linearly to zero.</p>"},{"location":"membership_functions/zshaped-linear/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives are essential for optimizing the parameters <code>a</code> and <code>b</code> in adaptive fuzzy systems.</p> Derivative with respect to `a` <p>$$\\frac{\\partial \\mu}{\\partial a} = \\frac{b - x}{(b - a)^2}$$</p> Derivative with respect to `b` <p>$$\\frac{\\partial \\mu}{\\partial b} = -\\frac{x - a}{(b - a)^2}$$</p>"},{"location":"membership_functions/zshaped-linear/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/zshaped-linear/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the LinZShapedMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/zshaped/","title":"Z-shaped","text":"<p>The function is defined by two key parameters that delimit the transition region:</p> <ul> <li><code>a</code> (Left Shoulder): The point where the transition from a full degree of membership (1.0) begins. For input values less than or equal to <code>a</code>, the membership is always 1.0.</li> <li><code>b</code> (Right Foot): The point where the transition ends and the degree of membership becomes zero. For input values greater than or equal to <code>b</code>, the membership is always 0.0.</li> </ul> <p>It is crucial that <code>a</code> is less than <code>b</code> for the transition to occur correctly.</p> <p>The formula for the <code>ZShapedMF</code> is based on the smoothstep function, a third-degree polynomial. The function is defined in parts:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 1, &amp; x \\leq a, \\\\[6pt] 1 - \\bigl(3t^{2} - 2t^{3}\\bigr),  &amp; a &lt; x &lt; b \\\\[6pt] 0, &amp; x \\geq b. \\end{array} $$</p> <p>Where:</p> <p>$\\mu(x)$ is the degree of membership of element $x$ and $t = \\dfrac{x-a}{\\,b-a\\,}$.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom anfis_toolbox.membership import ZShapedMF\n\nmf = ZShapedMF(a=15, b=25)\n\nx = np.linspace(0, 40, 100)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from anfis_toolbox.membership import ZShapedMF  mf = ZShapedMF(a=15, b=25)  x = np.linspace(0, 40, 100) y = mf(x)  plt.plot(x, y) plt.show() <p>Below is a comprehensive visualization showing how the ZShapedMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/zshaped/#z-shaped","title":"Z-shaped\u00b6","text":"<p>The Z-shaped Membership Function (<code>ZShapedMF</code>) is a fundamental type of membership function in fuzzy logic. It provides a smooth and continuous transition from a full degree of membership (1.0) to zero. This form is ideal for modeling concepts like \"cold\" or \"slow,\" where membership is high up to a certain point and then decreases gradually. The transition is defined using a cubic polynomial, resulting in a smooth curve without angular points.</p>"},{"location":"membership_functions/zshaped/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives are crucial for optimizing the parameters <code>a</code> and <code>b</code> in adaptive fuzzy systems. They show how the function's output changes in response to small changes in these parameters.</p> Derivative with respect to `a` ($\\frac{\\partial \\mu}{\\partial a}$) <p>This derivative indicates how the degree of membership is affected by adjusting the starting point of the transition.</p> <p>$$\\frac{\\partial \\mu}{\\partial a} = \\frac{\\partial \\mu}{\\partial t} \\frac{\\partial t}{\\partial a} = - (6t(t-1)) \\cdot \\frac{x-b}{(b-a)^2}$$</p> Derivative with respect to `b` ($\\frac{\\partial \\mu}{\\partial b}$) <p>This derivative indicates how the degree of membership is affected by adjusting the endpoint of the transition.</p> <p>$$\\frac{\\partial \\mu}{\\partial b} = \\frac{\\partial \\mu}{\\partial t} \\frac{\\partial t}{\\partial b} = - (6t(t-1)) \\cdot - \\frac{x-a}{(b-a)^2}$$</p>"},{"location":"membership_functions/zshaped/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/zshaped/#visualization","title":"Visualization\u00b6","text":""},{"location":"models/anfis-classifier/","title":"ANFIS Classifier","text":"<p>The ANFIS Classifier is a variant of the Adaptive Neuro-Fuzzy Inference System designed for multi-class classification tasks. It extends the ANFIS architecture with a softmax output layer and trains using cross-entropy loss.</p>"},{"location":"models/anfis-classifier/#overview","title":"Overview","text":"<p>The ANFIS Classifier uses the same four-layer architecture as the regression model:</p> <ol> <li>Membership Layer: Fuzzifies crisp inputs using membership functions.</li> <li>Rule Layer: Computes rule strengths using T-norm operations.</li> <li>Normalization Layer: Normalizes rule weights to ensure they sum to 1.</li> <li>Consequent Layer: Computes class logits using Takagi-Sugeno-Kang (TSK) models with multiple outputs.</li> </ol>"},{"location":"models/anfis-classifier/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Each rule produces logits for all classes:</p> <p>If \\(x_1\\) is \\(A_1^i\\) and \\(x_2\\) is \\(A_2^i\\) ... and \\(x_n\\) is \\(A_n^i\\) then \\(y^i_k = p_{0,k}^i + p_{1,k}^i x_1 + \\dots + p_{n,k}^i x_n\\) for \\(k = 1, \\dots, K\\)</p> <p>Where \\(K\\) is the number of classes.</p> <p>The final logits are weighted sums: \\(z_k = \\sum_{i=1}^R w^i y^i_k / \\sum_{i=1}^R w^i\\)</p> <p>Probabilities are computed via softmax: \\(p_k = \\exp(z_k) / \\sum_{j=1}^K \\exp(z_j)\\)</p> <p>Training minimizes cross-entropy loss.</p>"},{"location":"models/anfis-classifier/#anfisclassifier-class","title":"ANFISClassifier Class","text":"<p>The <code>ANFISClassifier</code> class implements the classification variant of the ANFIS model.</p>"},{"location":"models/anfis-classifier/#initialization","title":"Initialization","text":"<pre><code>from anfis_toolbox.membership import GaussianMF\nfrom anfis_toolbox.model import ANFISClassifier\n\ninput_mfs = {\n    'x1': [GaussianMF(0, 1), GaussianMF(1, 1)],\n    'x2': [GaussianMF(0, 1), GaussianMF(1, 1)]\n}\nclassifier = ANFISClassifier(input_mfs, n_classes=3)\n</code></pre>"},{"location":"models/anfis-classifier/#key-methods","title":"Key Methods","text":"<ul> <li><code>forward(x)</code>: Returns logits for classification.</li> <li><code>predict_proba(x)</code>: Returns class probabilities.</li> <li><code>predict(x)</code>: Returns predicted class labels.</li> <li><code>fit(X, y, epochs=100, learning_rate=0.01)</code>: Trains the classifier.</li> <li><code>get_parameters()</code> / <code>set_parameters()</code>: For parameter management.</li> <li><code>update_parameters(learning_rate)</code>: Applies gradient descent updates.</li> </ul>"},{"location":"models/anfis-classifier/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\n\n# Generate classification data\nX = np.random.randn(100, 2)\ny = np.random.randint(0, 3, 100)\n\n# Train the classifier\nlosses = classifier.fit(X, y, epochs=50, learning_rate=0.01)\n\n# Make predictions\nprobabilities = classifier.predict_proba(X)\nlabels = classifier.predict(X)\n</code></pre>"},{"location":"models/anfis-classifier/#detailed-architecture","title":"Detailed Architecture","text":""},{"location":"models/anfis-classifier/#layer-1-membership-layer","title":"Layer 1: Membership Layer","text":"<p>For each input \\(x_j\\), computes membership degrees \\(\\mu_{A_j^i}(x_j)\\) for each fuzzy set \\(A_j^i\\).</p>"},{"location":"models/anfis-classifier/#layer-2-rule-layer","title":"Layer 2: Rule Layer","text":"<p>Computes firing strengths \\(w^i = \\prod_{j=1}^n \\mu_{A_j^i}(x_j)\\) using product T-norm.</p>"},{"location":"models/anfis-classifier/#layer-3-normalization-layer","title":"Layer 3: Normalization Layer","text":"<p>Normalizes weights: \\(\\bar{w}^i = w^i / \\sum_{k=1}^R w^k\\)</p>"},{"location":"models/anfis-classifier/#layer-4-consequent-layer","title":"Layer 4: Consequent Layer","text":"<p>Computes rule logits \\(y^i_k = \\sum_{m=0}^n p_{m,k}^i x_m\\), then final logits \\(z_k = \\sum_{i=1}^R \\bar{w}^i y^i_k\\)</p>"},{"location":"models/anfis-classifier/#training-process","title":"Training Process","text":"<p>The classifier uses gradient descent on cross-entropy loss:</p> <ol> <li>Forward Pass: Compute logits and probabilities.</li> <li>Loss Computation: Cross-entropy between true labels and predicted probabilities.</li> <li>Backward Pass: Compute gradients through softmax and all layers.</li> <li>Parameter Update: Update membership and consequent parameters.</li> </ol>"},{"location":"models/anfis-classifier/#choosing-membership-functions","title":"Choosing Membership Functions","text":"<ul> <li>Number of MFs per input: Start with 2-3, increase for complex decision boundaries.</li> <li>MF Types: Gaussian for smooth boundaries, triangular for piecewise linear.</li> <li>Initialization: Place MFs to cover the input range evenly.</li> </ul>"},{"location":"models/anfis-classifier/#evaluation-and-metrics","title":"Evaluation and Metrics","text":"<p>Use classification metrics like accuracy, cross-entropy:</p> <pre><code>from anfis_toolbox.metrics import accuracy, cross_entropy\n\nacc = accuracy(y_true, predictions)\nloss = cross_entropy(y_true_onehot, logits)\n</code></pre>"},{"location":"models/anfis-classifier/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/anfis-classifier/#custom-training","title":"Custom Training","text":"<pre><code># Manual training loop\nfor epoch in range(100):\n    classifier.reset_gradients()\n    logits = classifier.forward(X)\n    # Compute loss and gradients...\n    classifier.update_parameters(0.01)\n</code></pre>"},{"location":"models/anfis-classifier/#parameter-management","title":"Parameter Management","text":"<pre><code># Save parameters\nparams = classifier.get_parameters()\n\n# Load parameters\nclassifier.set_parameters(params)\n</code></pre>"},{"location":"models/anfis-classifier/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Scalability: Rule count grows exponentially; suitable for low-dimensional data.</li> <li>Training Time: May require more epochs than regression due to cross-entropy.</li> <li>Memory Usage: Scales with rules \u00d7 classes \u00d7 inputs.</li> </ul>"},{"location":"models/anfis-classifier/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Poor Accuracy: Increase membership functions or epochs.</li> <li>Overfitting: Reduce rules or add regularization.</li> <li>Class Imbalance: Ensure balanced training data.</li> </ul>"},{"location":"models/anfis-classifier/#references","title":"References","text":"<ul> <li>Jang, J.-S. R. (1993). ANFIS: Adaptive-network-based fuzzy inference system. IEEE Transactions on Systems, Man, and Cybernetics, 23(3), 665-685.</li> </ul>"},{"location":"models/anfis/","title":"ANFIS Model","text":"<p>The ANFIS (Adaptive Neuro-Fuzzy Inference System) model is the core component of the ANFIS Toolbox. It implements a fuzzy inference system that combines the benefits of fuzzy logic and neural networks for function approximation tasks.</p>"},{"location":"models/anfis/#overview","title":"Overview","text":"<p>The ANFIS architecture consists of four layers:</p> <ol> <li>Membership Layer: Fuzzifies crisp inputs using membership functions.</li> <li>Rule Layer: Computes rule strengths using T-norm operations.</li> <li>Normalization Layer: Normalizes rule weights to ensure they sum to 1.</li> <li>Consequent Layer: Computes the final output using Takagi-Sugeno-Kang (TSK) models.</li> </ol>"},{"location":"models/anfis/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>ANFIS is based on the Takagi-Sugeno fuzzy model. Each rule has the form:</p> <p>If \\(x_1\\) is \\(A_1^i\\) and \\(x_2\\) is \\(A_2^i\\) ... and \\(x_n\\) is \\(A_n^i\\) then \\(y^i = p_0^i + p_1^i x_1 + \\dots + p_n^i x_n\\)</p> <p>Where \\(A_j^i\\) are fuzzy sets, and \\(p_k^i\\) are consequent parameters.</p> <p>The overall output is a weighted average of rule outputs:</p> <p>\\(y = \\sum_{i=1}^R w^i y^i / \\sum_{i=1}^R w^i\\)</p> <p>Where \\(w^i\\) is the firing strength of rule \\(i\\).</p>"},{"location":"models/anfis/#anfis-class","title":"ANFIS Class","text":"<p>The <code>ANFIS</code> class implements the regression variant of the ANFIS model.</p>"},{"location":"models/anfis/#initialization","title":"Initialization","text":"<pre><code>from anfis_toolbox.membership import GaussianMF\nfrom anfis_toolbox.model import ANFIS\n\ninput_mfs = {\n    'x1': [GaussianMF(0, 1), GaussianMF(1, 1)],\n    'x2': [GaussianMF(0, 1), GaussianMF(1, 1)]\n}\nmodel = ANFIS(input_mfs)\n</code></pre>"},{"location":"models/anfis/#key-methods","title":"Key Methods","text":"<ul> <li><code>forward(x)</code>: Performs a forward pass through the network.</li> <li><code>backward(dL_dy)</code>: Computes gradients via backpropagation.</li> <li><code>predict(x)</code>: Makes predictions on input data.</li> <li><code>fit(x, y, epochs=100, learning_rate=0.01)</code>: Trains the model using hybrid learning.</li> <li><code>get_parameters()</code> / <code>set_parameters()</code>: For parameter management.</li> <li><code>update_parameters(learning_rate)</code>: Applies gradient descent updates.</li> </ul>"},{"location":"models/anfis/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\n\n# Generate sample data\nX = np.random.randn(100, 2)\ny = X[:, 0] + X[:, 1] + np.random.randn(100) * 0.1\n\n# Train the model\nlosses = model.fit(X, y, epochs=50, learning_rate=0.01)\n\n# Make predictions\npredictions = model.predict(X)\n</code></pre>"},{"location":"models/anfis/#detailed-architecture","title":"Detailed Architecture","text":""},{"location":"models/anfis/#layer-1-membership-layer","title":"Layer 1: Membership Layer","text":"<p>For each input \\(x_j\\), computes membership degrees \\(\\mu_{A_j^i}(x_j)\\) for each fuzzy set \\(A_j^i\\).</p>"},{"location":"models/anfis/#layer-2-rule-layer","title":"Layer 2: Rule Layer","text":"<p>Computes firing strengths \\(w^i = \\prod_{j=1}^n \\mu_{A_j^i}(x_j)\\) using product T-norm.</p>"},{"location":"models/anfis/#layer-3-normalization-layer","title":"Layer 3: Normalization Layer","text":"<p>Normalizes weights: \\(\\bar{w}^i = w^i / \\sum_{k=1}^R w^k\\)</p>"},{"location":"models/anfis/#layer-4-consequent-layer","title":"Layer 4: Consequent Layer","text":"<p>Computes rule outputs \\(y^i = \\sum_{k=0}^n p_k^i x_k\\) (with \\(x_0 = 1\\)), then final output \\(y = \\sum_{i=1}^R \\bar{w}^i y^i\\)</p>"},{"location":"models/anfis/#training-process","title":"Training Process","text":"<p>ANFIS uses hybrid learning:</p> <ol> <li>Forward Pass: Compute outputs using current premise parameters.</li> <li>Consequent Parameter Estimation: Use least squares to find optimal \\(p_k^i\\).</li> <li>Backward Pass: Compute gradients for premise parameters.</li> <li>Parameter Update: Update membership function parameters via gradient descent.</li> </ol> <p>This typically converges faster than pure backpropagation.</p>"},{"location":"models/anfis/#choosing-membership-functions","title":"Choosing Membership Functions","text":"<ul> <li>Number of MFs per input: Start with 2-3, increase for complex functions.</li> <li>MF Types: Gaussian for smooth functions, triangular for piecewise linear.</li> <li>Initialization: Place MFs to cover the input range evenly.</li> </ul>"},{"location":"models/anfis/#evaluation-and-metrics","title":"Evaluation and Metrics","text":"<p>Use regression metrics like MSE, RMSE, R\u00b2:</p> <pre><code>from anfis_toolbox.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_true, predictions)\nr2 = r2_score(y_true, predictions)\n</code></pre>"},{"location":"models/anfis/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/anfis/#custom-trainers","title":"Custom Trainers","text":"<pre><code>from anfis_toolbox.optim import HybridTrainer\n\ntrainer = HybridTrainer(learning_rate=0.01, epochs=200)\nlosses = trainer.fit(model, X, y)\n</code></pre>"},{"location":"models/anfis/#parameter-management","title":"Parameter Management","text":"<pre><code># Save parameters\nparams = model.get_parameters()\n\n# Load parameters\nmodel.set_parameters(params)\n</code></pre>"},{"location":"models/anfis/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Scalability: The number of rules grows exponentially with the number of membership functions per input.</li> <li>Training Time: Hybrid training typically converges faster than pure backpropagation.</li> <li>Memory Usage: Parameter storage scales with the number of rules and inputs.</li> </ul>"},{"location":"models/anfis/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Poor Convergence: Try more epochs, adjust learning rate, or add more membership functions.</li> <li>Overfitting: Use fewer rules or add regularization (via custom trainers).</li> <li>Numerical Issues: Ensure inputs are scaled to similar ranges.</li> </ul>"},{"location":"models/anfis/#references","title":"References","text":"<ul> <li>Jang, J.-S. R. (1993). ANFIS: Adaptive-network-based fuzzy inference system. IEEE Transactions on Systems, Man, and Cybernetics, 23(3), 665-685.</li> </ul>"},{"location":"models/fuzzy_c-means/","title":"Fuzzy C-Means Clustering","text":"<p>The Fuzzy C-Means (FCM) clustering algorithm is a soft clustering method that assigns each data point to multiple clusters with varying degrees of membership. It's particularly useful for fuzzy logic applications and can be used to initialize membership functions for ANFIS models.</p>"},{"location":"models/fuzzy_c-means/#overview","title":"Overview","text":"<p>Unlike hard clustering methods like K-Means that assign each point to exactly one cluster, FCM allows points to belong to multiple clusters simultaneously. This makes it suitable for applications where data points have ambiguous cluster memberships.</p>"},{"location":"models/fuzzy_c-means/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>FCM minimizes the objective function:</p> <p>\\(J_m(U, V) = \\sum_{i=1}^n \\sum_{k=1}^c u_{ik}^m \\|x_i - v_k\\|^2\\)</p> <p>Subject to: - \\(\\sum_{k=1}^c u_{ik} = 1\\) for all \\(i\\) - \\(u_{ik} \\in [0, 1]\\) for all \\(i, k\\)</p> <p>Where: - \\(u_{ik}\\) is the membership degree of point \\(i\\) to cluster \\(k\\) - \\(v_k\\) is the center of cluster \\(k\\) - \\(m &gt; 1\\) is the fuzzifier parameter - \\(c\\) is the number of clusters</p> <p>The algorithm iteratively updates memberships and centers using:</p> <p>\\(u_{ik} = \\frac{1}{\\sum_{j=1}^c \\left(\\frac{\\|x_i - v_k\\|}{\\|x_i - v_j\\|}\\right)^{\\frac{2}{m-1}}}\\)</p> <p>\\(v_k = \\frac{\\sum_{i=1}^n u_{ik}^m x_i}{\\sum_{i=1}^n u_{ik}^m}\\)</p>"},{"location":"models/fuzzy_c-means/#fuzzycmeans-class","title":"FuzzyCMeans Class","text":"<p>The <code>FuzzyCMeans</code> class implements the Fuzzy C-Means algorithm.</p>"},{"location":"models/fuzzy_c-means/#initialization","title":"Initialization","text":"<pre><code>from anfis_toolbox.clustering import FuzzyCMeans\n\n# Basic usage\nfcm = FuzzyCMeans(n_clusters=3, m=2.0)\n\n# With custom parameters\nfcm = FuzzyCMeans(\n    n_clusters=4,\n    m=1.5,\n    max_iter=200,\n    tol=1e-5,\n    random_state=42\n)\n</code></pre>"},{"location":"models/fuzzy_c-means/#key-parameters","title":"Key Parameters","text":"<ul> <li><code>n_clusters</code>: Number of clusters (\u2265 2)</li> <li><code>m</code>: Fuzzifier parameter (&gt; 1, default 2.0)</li> <li><code>max_iter</code>: Maximum iterations (default 300)</li> <li><code>tol</code>: Convergence tolerance (default 1e-4)</li> <li><code>random_state</code>: Random seed for reproducibility</li> </ul>"},{"location":"models/fuzzy_c-means/#key-methods","title":"Key Methods","text":"<ul> <li><code>fit(X)</code>: Fit the FCM model to data</li> <li><code>fit_predict(X)</code>: Fit and return hard cluster labels</li> <li><code>predict(X)</code>: Return hard labels for new data</li> <li><code>predict_proba(X)</code>: Return fuzzy memberships for new data</li> <li><code>transform(X)</code>: Alias for predict_proba</li> </ul>"},{"location":"models/fuzzy_c-means/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom anfis_toolbox.clustering import FuzzyCMeans\n\n# Generate sample data\nX = np.random.randn(150, 2)\nX[:50] += [2, 2]   # Cluster 1\nX[50:100] += [-2, 2]  # Cluster 2\nX[100:] += [0, -2]    # Cluster 3\n\n# Fit FCM\nfcm = FuzzyCMeans(n_clusters=3, random_state=42)\nfcm.fit(X)\n\n# Get results\ncenters = fcm.cluster_centers_\nmemberships = fcm.membership_\nhard_labels = fcm.predict(X)\n\nprint(f\"Cluster centers shape: {centers.shape}\")\nprint(f\"Membership matrix shape: {memberships.shape}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>FCM provides several metrics to evaluate clustering quality:</p>"},{"location":"models/fuzzy_c-means/#partition-coefficient-pc","title":"Partition Coefficient (PC)","text":"<p>Measures the amount of fuzziness in the partition:</p> <p>\\(PC = \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^c u_{ik}^2\\)</p> <p>Higher values (closer to 1) indicate crisper partitions.</p> <pre><code>pc = fcm.partition_coefficient()\nprint(f\"Partition Coefficient: {pc:.4f}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#classification-entropy-ce","title":"Classification Entropy (CE)","text":"<p>Measures the entropy of the membership distribution:</p> <p>\\(CE = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^c u_{ik} \\log u_{ik}\\)</p> <p>Lower values indicate better clustering.</p> <pre><code>ce = fcm.classification_entropy()\nprint(f\"Classification Entropy: {ce:.4f}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#xie-beni-index-xb","title":"Xie-Beni Index (XB)","text":"<p>Combines compactness and separation:</p> <p>\\(XB = \\frac{\\sum_{i=1}^n \\sum_{k=1}^c u_{ik}^m \\|x_i - v_k\\|^2}{n \\cdot \\min_{p \\neq q} \\|v_p - v_q\\|^2}\\)</p> <p>Lower values indicate better clustering.</p> <pre><code>xb = fcm.xie_beni_index(X)\nprint(f\"Xie-Beni Index: {xb:.4f}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/fuzzy_c-means/#using-fcm-for-anfis-initialization","title":"Using FCM for ANFIS Initialization","text":"<p>FCM can be used to initialize membership functions for ANFIS models:</p> <pre><code>from anfis_toolbox.membership import GaussianMF\n\n# Cluster data to find centers\nfcm = FuzzyCMeans(n_clusters=3)\nfcm.fit(X)\n\n# Use cluster centers to initialize Gaussians\ncenters = fcm.cluster_centers_[:, 0]  # Assuming 1D input\ninput_mfs = {\n    'x1': [GaussianMF(center, 1.0) for center in centers]\n}\n</code></pre>"},{"location":"models/fuzzy_c-means/#custom-fuzzifier","title":"Custom Fuzzifier","text":"<p>The fuzzifier <code>m</code> controls the fuzziness of the clustering:</p> <ul> <li><code>m \u2192 1</code>: Approaches hard clustering (K-Means)</li> <li><code>m \u2192 \u221e</code>: Maximum fuzziness, all memberships equal</li> </ul> <pre><code># Hard clustering approximation\nhard_fcm = FuzzyCMeans(n_clusters=3, m=1.01)\n\n# Very fuzzy clustering\nfuzzy_fcm = FuzzyCMeans(n_clusters=3, m=5.0)\n</code></pre>"},{"location":"models/fuzzy_c-means/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Convergence: FCM typically converges in fewer iterations than expected</li> <li>Computational Complexity: O(n \u00d7 c \u00d7 d \u00d7 iter), where n=samples, c=clusters, d=dimensions</li> <li>Memory Usage: Stores membership matrix (n \u00d7 c) and centers (c \u00d7 d)</li> <li>Scalability: Suitable for small to medium datasets (&lt; 10,000 samples)</li> </ul>"},{"location":"models/fuzzy_c-means/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Poor Convergence: Increase <code>max_iter</code> or decrease <code>tol</code></li> <li>All Points in One Cluster: Try different <code>random_state</code> or increase <code>m</code></li> <li>Numerical Issues: Ensure data is scaled to similar ranges</li> </ul>"},{"location":"models/fuzzy_c-means/#references","title":"References","text":"<ul> <li>Bezdek, J. C. (1981). Pattern Recognition with Fuzzy Objective Function Algorithms. Springer.</li> <li>Xie, X. L., &amp; Beni, G. (1991). A validity measure for fuzzy clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(8), 841-847.</li> </ul>"}]}