{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"ANFIS Toolbox <p> The most user-friendly Python library for Adaptive Neuro-Fuzzy Inference Systems (ANFIS) </p> <p>ANFIS Toolbox is a comprehensive Python library for creating, training, and deploying Adaptive Neuro-Fuzzy Inference Systems (ANFIS). It provides an intuitive API that makes fuzzy neural networks accessible to both beginners and experts.</p> \ud83d\udd17 GitHub | \ud83d\udce6 PyPI"},{"location":"#key-features","title":"Key Features","text":"\u2728 Easy to Use         Get started with just 3 lines of code               \ud83e\udd16 Versatile Modeling         Supports both classification and regression tasks               \ud83c\udfd7\ufe0f Flexible Architecture         13 membership functions               \ud83d\ude80 Adaptive Initialization         Fuzzy c-means, grid, and random initialization strategies               \ud83d\udcc9 Flexible Optimization         Multiple optimization algorithms               \ud83d\udcd0 Comprehensive Metrics         Rich collection of evaluation metrics               \ud83d\udcda Rich Documentation         Comprehensive examples"},{"location":"#why-anfis-toolbox","title":"Why ANFIS Toolbox?","text":""},{"location":"#simplicity-first","title":"\ud83d\ude80 Simplicity First","text":"<p>Most fuzzy logic libraries require extensive boilerplate code. ANFIS Toolbox gets you running in seconds:</p> RegressionClassification <pre><code>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor()\nmodel.fit(X, y)\n</code></pre> <pre><code>from anfis_toolbox import ANFISClassifier\n\nmodel = ANFISClassifier()\nmodel.fit(X, y)\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"RegressionClassification <pre><code>import numpy as np\nfrom anfis_toolbox import ANFISRegressor\n\nX = np.random.uniform(-2, 2, (100, 2))  # 2 inputs\ny = X[:, 0]**2 + X[:, 1]**2  # Target: x1\u00b2 + x2\u00b2\n\nmodel = ANFISRegressor()\nmodel.fit(X, y)\n</code></pre> <pre><code>import numpy as np\nfrom anfis_toolbox import ANFISClassifier\n\nX = np.r_[np.random.normal(-1, .3, (50, 2)), np.random.normal(1, .3, (50, 2))]\ny = np.r_[np.zeros(50, int), np.ones(50, int)]\n\nmodel = ANFISClassifier()\nmodel.fit(X, y)\n</code></pre>"},{"location":"#metrics-evaluation","title":"Metrics &amp; Evaluation","text":"<p>Want a structured report instead of a plain dictionary? Use <code>evaluate</code> to detect the task type automatically and access every score.</p> <pre><code>metrics = model.evaluate(X, y)\n</code></pre> <p>That's it! \ud83c\udf89 You just created, trained and evaluate a neuro-fuzzy system!</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the core package with minimal dependencies:</p> <pre><code>pip install anfis-toolbox\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"Application Description Function Approximation Learn complex mathematical functions Regression Predict continuous values Classification Predict discrete class labels Pattern Recognition Classify patterns with fuzzy boundaries Control Systems Design fuzzy controllers Time Series Forecast future values"},{"location":"#architecture","title":"Architecture","text":"<p>ANFIS Toolbox implements the complete 4-layer ANFIS architecture:</p> <pre><code>flowchart LR\n\n    %% Layer 1\n    subgraph L1 [layer 1]\n      direction TB\n      A1[\"A1\"]\n      A2[\"A2\"]\n      B1[\"B1\"]\n      B2[\"B2\"]\n    end\n\n    %% Inputs\n    x_input[x] --&gt; A1\n    x_input --&gt; A2\n    y_input[y] --&gt; B1\n    y_input --&gt; B2\n\n    %% Layer 2\n    subgraph L2 [layer 2]\n      direction TB\n      P1((\u03a0))\n      P2((\u03a0))\n    end\n    A1 --&gt; P1\n    B1 --&gt; P1\n    A2 --&gt; P2\n    B2 --&gt; P2\n\n    %% Layer 3\n    subgraph L3 [layer 3]\n      direction TB\n      N1((N))\n      N2((N))\n    end\n    P1 -- w\u2081 --&gt; N1\n    P1 ----&gt; N2\n    P2 ----&gt; N1\n    P2 -- w\u2082 --&gt; N2\n\n    %% Layer 4\n    subgraph L4 [layer 4]\n      direction TB\n      L4_1[x y]\n      L4_2[x y]\n    end\n    N1 -- w\u0305\u2081 --&gt; L4_1\n    N2 -- w\u0305\u2082 --&gt; L4_2\n\n    %% Layer 5\n    subgraph L5 [layer 5]\n      direction TB\n      Sum((\u03a3))\n    end\n    L4_1 -- \"w\u2081 f\u2081\" --&gt; Sum\n    L4_2 -- \"w\u2082 f\u2082\" --&gt; Sum\n\n    %% Output\n    Sum -- f --&gt; f_out[f]</code></pre>"},{"location":"#supported-membership-functions","title":"Supported Membership Functions","text":"<ul> <li>Gaussian (<code>GaussianMF</code>) - Smooth bell curves</li> <li>Gaussian2 (<code>Gaussian2MF</code>) - Two-sided Gaussian with flat region</li> <li>Triangular (<code>TriangularMF</code>) - Simple triangular shapes</li> <li>Trapezoidal (<code>TrapezoidalMF</code>) - Plateau regions</li> <li>Bell-shaped (<code>BellMF</code>) - Generalized bell curves</li> <li>Sigmoidal (<code>SigmoidalMF</code>) - S-shaped transitions</li> <li>Diff-Sigmoidal (<code>DiffSigmoidalMF</code>) - Difference of two sigmoids</li> <li>Prod-Sigmoidal (<code>ProdSigmoidalMF</code>) - Product of two sigmoids</li> <li>S-shaped (<code>SShapedMF</code>) - Smooth S-curve transitions</li> <li>Linear S-shaped (<code>LinSShapedMF</code>) - Piecewise linear S-curve</li> <li>Z-shaped (<code>ZShapedMF</code>) - Smooth Z-curve transitions</li> <li>Linear Z-shaped (<code>LinZShapedMF</code>) - Piecewise linear Z-curve</li> <li>Pi-shaped (<code>PiMF</code>) - Bell with flat top</li> </ul>"},{"location":"#training-methods","title":"Training Methods","text":"<ul> <li>SGD (Stochastic Gradient Descent) \u2013 Classic gradient-based optimization with incremental updates</li> <li>Adam \u2013 Adaptive learning rates with momentum for faster convergence</li> <li>RMSProp \u2013 Scales learning rates by recent gradient magnitudes for stable training</li> <li>PSO (Particle Swarm Optimization) \u2013 Population-based global search strategy</li> <li>Hybrid SGD + OLS \u2013 Combines gradient descent with least-squares parameter refinement</li> <li>Hybrid Adam + OLS \u2013 Integrates adaptive optimization with analytical least-squares adjustment</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>\ud83d\udca1 Examples - Real-world use cases</li> <li>\ud83d\udd27 API Reference - Complete function documentation</li> <li>\ud83e\udd16 ANFIS Models - Regression and classification models</li> <li>\ud83d\udcd0 Membership Functions - All MF classes</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udc1b Report Issues - Bug reports and feature requests</li> <li>\ud83d\udcac Discussions - Questions and community chat</li> <li>\u2b50 Star on GitHub - Show your support!</li> </ul> Ready to dive into fuzzy neural networks? Get started now"},{"location":"api/","title":"API Reference","text":"<p>Complete reference documentation for all ANFIS Toolbox classes, functions, and modules.</p>"},{"location":"api/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":""},{"location":"api/#estimators","title":"Estimators","text":"Estimator Purpose Module <code>ANFISRegressor</code> Scikit-learn style regression interface regressor <code>ANFISClassifier</code> Scikit-learn style classification interface classifier"},{"location":"api/#neural-network-layers","title":"Neural Network Layers","text":"Class Purpose Module <code>MembershipLayer</code> Fuzzy membership function layer layers <code>RuleLayer</code> Fuzzy rule firing strength computation layers <code>NormalizationLayer</code> Rule activation normalization layers <code>ConsequentLayer</code> Takagi-Sugeno consequent computation layers <code>ClassificationConsequentLayer</code> Classification-specific consequent layer layers"},{"location":"api/#membership-functions","title":"\ud83c\udfaf Membership Functions","text":"<p>Complete set of 13 fuzzy membership function implementations:</p> Function Type Parameters Module <code>GaussianMF</code> Gaussian <code>mean</code>, <code>sigma</code> membership-functions <code>Gaussian2MF</code> Two-sided Gaussian <code>sigma1</code>, <code>c1</code>, <code>sigma2</code>, <code>c2</code> membership-functions <code>TriangularMF</code> Triangular <code>a</code>, <code>b</code>, <code>c</code> membership-functions <code>TrapezoidalMF</code> Trapezoidal <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> membership-functions <code>BellMF</code> Bell-shaped <code>a</code>, <code>b</code>, <code>c</code> membership-functions <code>SigmoidalMF</code> Sigmoidal <code>a</code>, <code>c</code> membership-functions <code>DiffSigmoidalMF</code> Difference of sigmoids <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code> membership-functions <code>ProdSigmoidalMF</code> Product of sigmoids <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code> membership-functions <code>SShapedMF</code> S-shaped <code>a</code>, <code>b</code> membership-functions <code>LinSShapedMF</code> Linear S-shaped <code>a</code>, <code>b</code> membership-functions <code>ZShapedMF</code> Z-shaped <code>a</code>, <code>b</code> membership-functions <code>LinZShapedMF</code> Linear Z-shaped <code>a</code>, <code>b</code> membership-functions <code>PiMF</code> Pi-shaped <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> membership-functions"},{"location":"api/#training-optimization","title":"\ud83d\udd27 Training &amp; Optimization","text":""},{"location":"api/#training-algorithms","title":"Training Algorithms","text":"Trainer Method Module <code>HybridTrainer</code> Least squares + backpropagation (recommended) optim <code>HybridAdamTrainer</code> Least squares + Adam with adaptive moments optim <code>SGDTrainer</code> Stochastic gradient descent optim <code>AdamTrainer</code> Adaptive moment estimation optim <code>RMSPropTrainer</code> Root mean square propagation optim <code>PSOTrainer</code> Particle swarm optimization optim"},{"location":"api/#loss-functions","title":"Loss Functions","text":"Function Purpose Module <code>mse_loss</code> Mean squared error for regression losses <code>mse_grad</code> MSE gradient computation losses <code>cross_entropy_loss</code> Cross-entropy for classification losses <code>cross_entropy_grad</code> Cross-entropy gradient computation losses"},{"location":"api/#evaluation-validation","title":"\ud83d\udcca Evaluation &amp; Validation","text":""},{"location":"api/#metrics","title":"Metrics","text":"<p>Comprehensive metrics for model evaluation:</p> Category Functions Module Regression MSE, RMSE, MAE, MAPE, SMAPE, R\u00b2, explained variance, median AE, bias, Pearson, MSLE metrics Classification Accuracy, balanced accuracy, precision/recall/F1, log loss, cross-entropy metrics Clustering Partition coefficient, classification entropy, Xie-Beni index metrics"},{"location":"api/#clustering","title":"\ud83d\udd0d Clustering","text":"Class Purpose Module <code>FuzzyCMeans</code> Fuzzy C-Means clustering algorithm clustering"},{"location":"api/#detailed-documentation","title":"\ud83d\udcda Detailed Documentation","text":""},{"location":"api/#by-category","title":"By Category","text":"<ul> <li>ANFIS Models - High-level model documentation</li> <li>Regressor - High-level regression estimator API</li> <li>Classifier - High-level classification estimator API</li> <li>Builders - Model construction utilities</li> <li>Membership Functions - All 13 MF implementations</li> <li>Models - Core ANFIS and ANFISClassifier classes</li> <li>Layers - Neural network layer implementations</li> <li>Clustering - FuzzyCMeans clustering</li> <li>Losses - Training loss functions and gradients</li> <li>Metrics - Performance evaluation metrics</li> <li>Configuration - Persisting setups and presets</li> <li>Logging - Training log helpers</li> <li>Optimization - Training algorithm implementations</li> </ul>"},{"location":"api/#search-and-navigation","title":"\ud83d\udd0d Search and Navigation","text":""},{"location":"api/#find-by-functionality","title":"Find by Functionality","text":"I want to... Look at... Use a regression estimator ANFISRegressor Use a classification estimator ANFISClassifier Choose membership functions Membership Functions Train my model <code>fit()</code> method in Models Evaluate performance Metrics Configure training Optimization Save configs or presets Configuration Enable training logs Logging"},{"location":"api/#navigation","title":"Navigation","text":"<p>Start here for specific needs:</p> <ul> <li>\ud83d\ude80 New user? \u2192 ANFIS Models</li> <li>\ud83c\udfd7\ufe0f Building models? \u2192 Builders</li> <li>\ud83d\udcca Analyzing results? \u2192 Metrics</li> <li>\ud83d\udd0d Clustering? \u2192 Clustering</li> <li>\u2699\ufe0f Training? \u2192 Optimization</li> <li>\ud83e\uddea Using estimators? \u2192 Regressor / Classifier</li> <li>\ud83d\udcdd Saving configs or logs? \u2192 Configuration &amp; Logging</li> </ul>"},{"location":"api/builders/","title":"Builders","text":""},{"location":"api/builders/#anfis_toolbox.ANFISBuilder","title":"anfis_toolbox.ANFISBuilder","text":"<pre><code>ANFISBuilder()\n</code></pre> <p>Builder class for creating ANFIS models with intuitive API.</p>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.add_input","title":"add_input","text":"<pre><code>add_input(\n    name: str,\n    range_min: float,\n    range_max: float,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n) -&gt; ANFISBuilder\n</code></pre> <p>Add an input variable with automatic membership function generation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the input variable</p> required <code>range_min</code> <code>float</code> <p>Minimum value of the input range</p> required <code>range_max</code> <code>float</code> <p>Maximum value of the input range</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions (default: 3)</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Type of membership functions. Supported: 'gaussian', 'gaussian2', 'triangular', 'trapezoidal', 'bell', 'sigmoidal', 'sshape', 'zshape', 'pi'</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor between adjacent MFs (0.0 to 1.0)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ANFISBuilder</code> <p>Self for method chaining</p>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.add_input_from_data","title":"add_input_from_data","text":"<pre><code>add_input_from_data(\n    name: str,\n    data: ndarray,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    init: str = \"grid\",\n    random_state: int | None = None,\n) -&gt; ANFISBuilder\n</code></pre> <p>Add an input inferring range_min/range_max from data with a margin.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Input name</p> required <code>data</code> <code>ndarray</code> <p>1D array-like samples for this input</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Membership function type (see add_input)</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor between adjacent MFs</p> <code>0.5</code> <code>margin</code> <code>float</code> <p>Fraction of (max-min) to pad on each side</p> <code>0.1</code> <code>init</code> <code>str</code> <p>Initialization strategy: \"grid\" (default) or \"fcm\". When \"fcm\", clusters from the data determine MF centers and widths (supports 'gaussian' and 'bell').</p> <code>'grid'</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for deterministic FCM initialization.</p> <code>None</code>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.build","title":"build","text":"<pre><code>build() -&gt; ANFIS\n</code></pre> <p>Build the ANFIS model with configured parameters.</p>"},{"location":"api/builders/#anfis_toolbox.ANFISBuilder.set_rules","title":"set_rules","text":"<pre><code>set_rules(\n    rules: Sequence[Sequence[int]] | None,\n) -&gt; ANFISBuilder\n</code></pre> <p>Define an explicit set of fuzzy rules to use when building the model.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>Sequence[Sequence[int]] | None</code> <p>Iterable of rules where each rule lists the membership index per input. <code>None</code> removes any previously configured custom rules and restores the default Cartesian-product behaviour.</p> required <p>Returns:</p> Type Description <code>ANFISBuilder</code> <p>Self for method chaining.</p>"},{"location":"api/classifier/","title":"Classifier API","text":"<p>The <code>ANFISClassifier</code> offers a scikit-learn inspired interface for multi-class classification tasks, wrapping membership-function management, model construction, and training into a single estimator.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier","title":"anfis_toolbox.classifier.ANFISClassifier","text":"<pre><code>ANFISClassifier(\n    *,\n    n_classes: int | None = None,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str\n    | BaseTrainer\n    | type[BaseTrainer]\n    | None = \"adam\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimatorLike</code>, <code>FittedMixin</code>, <code>ClassifierMixinLike</code></p> <p>Adaptive Neuro-Fuzzy classifier with a scikit-learn style API.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier--parameters","title":"Parameters","text":"<p>n_classes : int, optional     Number of target classes. Must be &gt;= 2 when provided. If omitted, the     classifier infers the class count during the first call to <code>fit</code>. n_mfs : int, default=3     Default number of membership functions per input. mf_type : str, default=\"gaussian\"     Default membership function family (see :class:<code>ANFISBuilder</code>). init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Strategy used when inferring membership functions from data. <code>None</code>     falls back to <code>\"grid\"</code>. overlap : float, default=0.5     Controls overlap when generating membership functions via the builder. margin : float, default=0.10     Margin added around observed data ranges during grid initialization. inputs_config : Mapping, optional     Per-input overrides. Keys may be feature names (when <code>X</code> is a     :class:<code>pandas.DataFrame</code>) or integer indices. Values may be:</p> <pre><code>* ``dict`` with keys among ``{\"n_mfs\", \"mf_type\", \"init\", \"overlap\",\n    \"margin\", \"range\", \"membership_functions\", \"mfs\"}``.\n* A list/tuple of :class:`MembershipFunction` instances for full control.\n* ``None`` for defaults.\n</code></pre> <p>random_state : int, optional     Random state forwarded to initialization routines and stochastic     optimizers. optimizer : str, BaseTrainer, type[BaseTrainer], or None, default=\"adam\"     Trainer identifier or instance used for fitting. Strings map to entries     in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to \"adam\". optimizer_params : Mapping, optional     Additional keyword arguments forwarded to the trainer constructor. learning_rate, epochs, batch_size, shuffle, verbose : optional scalars     Common trainer hyper-parameters provided for convenience. When the     selected trainer supports the parameter it is included automatically. loss : str or LossFunction, optional     Custom loss forwarded to trainers that expose a <code>loss</code> parameter. rules : Sequence[Sequence[int]] | None, optional     Explicit fuzzy rule indices to use instead of the full Cartesian product. Each     rule lists the membership-function index per input. <code>None</code> keeps the default     exhaustive rule set.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier--parameters","title":"Parameters","text":"<p>n_classes : int, optional     Number of output classes. Must be at least two when provided. If     omitted, the value is inferred from the training targets during     the first <code>fit</code> call. n_mfs : int, default=3     Default number of membership functions to allocate per input when     inferred from data. mf_type : str, default=\"gaussian\"     Membership function family used for automatically generated     membership functions. See :class:<code>ANFISBuilder</code> for admissible     values. init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Initialization strategy applied when synthesizing membership     functions from the training data. <code>None</code> falls back to <code>\"grid\"</code>. overlap : float, default=0.5     Desired overlap between adjacent membership functions during     automatic construction. margin : float, default=0.10     Additional range padding applied around observed feature minima     and maxima for grid initialization. inputs_config : Mapping, optional     Per-feature overrides for the generated membership functions.     Keys may be feature names (when <code>X</code> is a :class:<code>pandas.DataFrame</code>),     integer indices, or <code>\"x{i}\"</code> aliases. Values may include builder     configuration dictionaries, explicit membership function sequences,     or <code>None</code> to retain defaults. random_state : int, optional     Seed forwarded to stochastic initializers and optimizers. optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"adam\"     Training algorithm identifier or instance. String aliases are looked     up in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to <code>\"adam\"</code>.     <code>HybridTrainer</code> and <code>HybridAdamTrainer</code> (least-squares hybrid variants)     are restricted to regression and will raise a <code>ValueError</code> when     supplied here. optimizer_params : Mapping, optional     Additional keyword arguments provided to the trainer constructor     when a string alias or trainer class is supplied. learning_rate, epochs, batch_size, shuffle, verbose : optional     Convenience hyper-parameters injected into the trainer whenever the     chosen implementation accepts them. <code>shuffle</code> supports <code>False</code>     to disable random shuffling. loss : str | LossFunction, optional     Custom loss specification forwarded to trainers that expose a     <code>loss</code> parameter. <code>None</code> resolves to cross-entropy. rules : Sequence[Sequence[int]] | None, optional     Optional explicit fuzzy rule definitions. Each rule lists the     membership-function index for each input. <code>None</code> uses the full     Cartesian product of configured membership functions.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def __init__(\n    self,\n    *,\n    n_classes: int | None = None,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.10,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str | BaseTrainer | type[BaseTrainer] | None = \"adam\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n) -&gt; None:\n    \"\"\"Configure an :class:`ANFISClassifier` with the supplied hyper-parameters.\n\n    Parameters\n    ----------\n    n_classes : int, optional\n        Number of output classes. Must be at least two when provided. If\n        omitted, the value is inferred from the training targets during\n        the first ``fit`` call.\n    n_mfs : int, default=3\n        Default number of membership functions to allocate per input when\n        inferred from data.\n    mf_type : str, default=\"gaussian\"\n        Membership function family used for automatically generated\n        membership functions. See :class:`ANFISBuilder` for admissible\n        values.\n    init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"\n        Initialization strategy applied when synthesizing membership\n        functions from the training data. ``None`` falls back to ``\"grid\"``.\n    overlap : float, default=0.5\n        Desired overlap between adjacent membership functions during\n        automatic construction.\n    margin : float, default=0.10\n        Additional range padding applied around observed feature minima\n        and maxima for grid initialization.\n    inputs_config : Mapping, optional\n        Per-feature overrides for the generated membership functions.\n        Keys may be feature names (when ``X`` is a :class:`pandas.DataFrame`),\n        integer indices, or ``\"x{i}\"`` aliases. Values may include builder\n        configuration dictionaries, explicit membership function sequences,\n        or ``None`` to retain defaults.\n    random_state : int, optional\n        Seed forwarded to stochastic initializers and optimizers.\n    optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"adam\"\n        Training algorithm identifier or instance. String aliases are looked\n        up in :data:`TRAINER_REGISTRY`. ``None`` defaults to ``\"adam\"``.\n        ``HybridTrainer`` and ``HybridAdamTrainer`` (least-squares hybrid variants)\n        are restricted to regression and will raise a ``ValueError`` when\n        supplied here.\n    optimizer_params : Mapping, optional\n        Additional keyword arguments provided to the trainer constructor\n        when a string alias or trainer class is supplied.\n    learning_rate, epochs, batch_size, shuffle, verbose : optional\n        Convenience hyper-parameters injected into the trainer whenever the\n        chosen implementation accepts them. ``shuffle`` supports ``False``\n        to disable random shuffling.\n    loss : str | LossFunction, optional\n        Custom loss specification forwarded to trainers that expose a\n        ``loss`` parameter. ``None`` resolves to cross-entropy.\n    rules : Sequence[Sequence[int]] | None, optional\n        Optional explicit fuzzy rule definitions. Each rule lists the\n        membership-function index for each input. ``None`` uses the full\n        Cartesian product of configured membership functions.\n    \"\"\"\n    if n_classes is not None and int(n_classes) &lt; 2:\n        raise ValueError(\"n_classes must be &gt;= 2\")\n    self.n_classes: int | None = int(n_classes) if n_classes is not None else None\n    self.n_mfs = int(n_mfs)\n    self.mf_type = str(mf_type)\n    self.init = None if init is None else str(init)\n    self.overlap = float(overlap)\n    self.margin = float(margin)\n    self.inputs_config = dict(inputs_config) if inputs_config is not None else None\n    self.random_state = random_state\n    self.optimizer = optimizer\n    self.optimizer_params = dict(optimizer_params) if optimizer_params is not None else None\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    self.verbose = verbose\n    self.loss = loss\n    self.rules = None if rules is None else tuple(tuple(int(idx) for idx in rule) for rule in rules)\n\n    # Fitted attributes (initialised during fit)\n    self.model_: LowLevelANFISClassifier | None = None\n    self.optimizer_: BaseTrainer | None = None\n    self.feature_names_in_: list[str] | None = None\n    self.n_features_in_: int | None = None\n    self.training_history_: TrainingHistory | None = None\n    self.input_specs_: list[dict[str, Any]] | None = None\n    self.classes_: np.ndarray | None = None\n    self._class_to_index_: dict[Any, int] | None = None\n    self.rules_: list[tuple[int, ...]] | None = None\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    X,\n    y,\n    *,\n    return_dict: bool = True,\n    print_results: bool = False,\n)\n</code></pre> <p>Evaluate predictive performance on a labelled dataset.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate--parameters","title":"Parameters","text":"<p>X : array-like     Evaluation inputs. y : array-like     Ground-truth labels. Accepts integer labels or one-hot encodings. return_dict : bool, default=True     When <code>True</code> return the computed metric dictionary; when <code>False</code>     return <code>None</code> after optional printing. print_results : bool, default=False     Emit a formatted summary to stdout when <code>True</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate--returns","title":"Returns:","text":"<p>dict[str, float] | None     Dictionary containing accuracy, balanced accuracy, macro/micro     precision/recall/F1 scores, and the confusion matrix when     <code>return_dict</code> is <code>True</code>; otherwise <code>None</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.evaluate--raises","title":"Raises:","text":"<p>RuntimeError     If called before the estimator has been fitted. ValueError     When <code>X</code> and <code>y</code> disagree on sample count or labels are     incompatible with the configured class count.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def evaluate(self, X, y, *, return_dict: bool = True, print_results: bool = False):\n    \"\"\"Evaluate predictive performance on a labelled dataset.\n\n    Parameters\n    ----------\n    X : array-like\n        Evaluation inputs.\n    y : array-like\n        Ground-truth labels. Accepts integer labels or one-hot encodings.\n    return_dict : bool, default=True\n        When ``True`` return the computed metric dictionary; when ``False``\n        return ``None`` after optional printing.\n    print_results : bool, default=False\n        Emit a formatted summary to stdout when ``True``.\n\n    Returns:\n    -------\n    dict[str, float] | None\n        Dictionary containing accuracy, balanced accuracy, macro/micro\n        precision/recall/F1 scores, and the confusion matrix when\n        ``return_dict`` is ``True``; otherwise ``None``.\n\n    Raises:\n    ------\n    RuntimeError\n        If called before the estimator has been fitted.\n    ValueError\n        When ``X`` and ``y`` disagree on sample count or labels are\n        incompatible with the configured class count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr, _ = _ensure_2d_array(X)\n    encoded_targets, _ = self._encode_targets(y, X_arr.shape[0], allow_partial_classes=True)\n    proba = self.predict_proba(X_arr)\n    metrics = ANFISMetrics.classification_metrics(encoded_targets, proba)\n    metrics.pop(\"log_loss\", None)\n    if print_results:\n        quick = [\n            (\"Accuracy\", metrics[\"accuracy\"]),\n        ]\n        print(\"ANFISClassifier evaluation:\")  # noqa: T201\n        for name, value in quick:\n            print(f\"  {name:&gt;8}: {value:.6f}\")  # noqa: T201\n    return metrics if return_dict else None\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    *,\n    validation_data: tuple[ndarray, ndarray] | None = None,\n    validation_frequency: int = 1,\n    **fit_params: Any,\n)\n</code></pre> <p>Fit the classifier on labelled data.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit--parameters","title":"Parameters","text":"<p>X : array-like     Training inputs with shape <code>(n_samples, n_features)</code>. y : array-like     Target labels. Accepts integer/str labels or one-hot matrices with     <code>(n_samples, n_classes)</code> columns. validation_data : tuple[np.ndarray, np.ndarray], optional     Optional validation split supplied to the underlying trainer.     Targets may be integer encoded or one-hot encoded consistent with     the trainer. validation_frequency : int, default=1     Frequency (in epochs) at which validation metrics are computed when     <code>validation_data</code> is provided. **fit_params : Any     Additional keyword arguments forwarded directly to the trainer     <code>fit</code> method.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit--returns","title":"Returns:","text":"<p>ANFISClassifier     Reference to <code>self</code> to enable fluent-style chaining.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.fit--raises","title":"Raises:","text":"<p>ValueError     If the input arrays disagree on the number of samples or the label     encoding is incompatible with the configured <code>n_classes</code>. TypeError     If the trainer <code>fit</code> implementation does not return a     <code>TrainingHistory</code> dictionary.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    *,\n    validation_data: tuple[np.ndarray, np.ndarray] | None = None,\n    validation_frequency: int = 1,\n    **fit_params: Any,\n):\n    \"\"\"Fit the classifier on labelled data.\n\n    Parameters\n    ----------\n    X : array-like\n        Training inputs with shape ``(n_samples, n_features)``.\n    y : array-like\n        Target labels. Accepts integer/str labels or one-hot matrices with\n        ``(n_samples, n_classes)`` columns.\n    validation_data : tuple[np.ndarray, np.ndarray], optional\n        Optional validation split supplied to the underlying trainer.\n        Targets may be integer encoded or one-hot encoded consistent with\n        the trainer.\n    validation_frequency : int, default=1\n        Frequency (in epochs) at which validation metrics are computed when\n        ``validation_data`` is provided.\n    **fit_params : Any\n        Additional keyword arguments forwarded directly to the trainer\n        ``fit`` method.\n\n    Returns:\n    -------\n    ANFISClassifier\n        Reference to ``self`` to enable fluent-style chaining.\n\n    Raises:\n    ------\n    ValueError\n        If the input arrays disagree on the number of samples or the label\n        encoding is incompatible with the configured ``n_classes``.\n    TypeError\n        If the trainer ``fit`` implementation does not return a\n        ``TrainingHistory`` dictionary.\n    \"\"\"\n    X_arr, feature_names = _ensure_2d_array(X)\n    n_samples = X_arr.shape[0]\n    y_encoded, classes = self._encode_targets(y, n_samples)\n\n    self.classes_ = classes\n    self._class_to_index_ = {self._normalize_class_key(cls): idx for idx, cls in enumerate(classes.tolist())}\n\n    self.feature_names_in_ = feature_names\n    self.n_features_in_ = X_arr.shape[1]\n    self.input_specs_ = self._resolve_input_specs(feature_names)\n\n    _ensure_training_logging(self.verbose)\n    if self.n_classes is None:\n        raise RuntimeError(\"n_classes could not be inferred from the provided targets\")\n    self.model_ = self._build_model(X_arr, feature_names)\n    trainer = self._instantiate_trainer()\n    self.optimizer_ = trainer\n    trainer_kwargs: dict[str, Any] = dict(fit_params)\n    if validation_data is not None:\n        trainer_kwargs.setdefault(\"validation_data\", validation_data)\n    if validation_data is not None or validation_frequency != 1:\n        trainer_kwargs.setdefault(\"validation_frequency\", validation_frequency)\n\n    history = trainer.fit(self.model_, X_arr, y_encoded, **trainer_kwargs)\n    if not isinstance(history, dict):\n        raise TypeError(\"Trainer.fit must return a TrainingHistory dictionary\")\n    self.training_history_ = history\n    self.rules_ = self.model_.rules\n\n    self._mark_fitted()\n    return self\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.get_rules","title":"get_rules","text":"<pre><code>get_rules() -&gt; tuple[tuple[int, ...], ...]\n</code></pre> <p>Return the fuzzy rule index combinations used by the fitted model.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.get_rules--returns","title":"Returns:","text":"<p>tuple[tuple[int, ...], ...]     Immutable tuple describing each fuzzy rule as a per-input     membership index.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.get_rules--raises","title":"Raises:","text":"<p>RuntimeError     If invoked before <code>fit</code> completes.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def get_rules(self) -&gt; tuple[tuple[int, ...], ...]:\n    \"\"\"Return the fuzzy rule index combinations used by the fitted model.\n\n    Returns:\n    -------\n    tuple[tuple[int, ...], ...]\n        Immutable tuple describing each fuzzy rule as a per-input\n        membership index.\n\n    Raises:\n    ------\n    RuntimeError\n        If invoked before ``fit`` completes.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"rules_\"])\n    if not self.rules_:\n        return ()\n    return tuple(tuple(rule) for rule in self.rules_)\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels for the provided samples.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict--parameters","title":"Parameters","text":"<p>X : array-like     Samples to classify. One-dimensional arrays are treated as a single     sample; two-dimensional arrays must have shape <code>(n_samples, n_features)</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict--returns","title":"Returns:","text":"<p>np.ndarray     Predicted class labels with shape <code>(n_samples,)</code>.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict--raises","title":"Raises:","text":"<p>RuntimeError     If invoked before the estimator is fitted. ValueError     When the supplied samples do not match the fitted feature count.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for the provided samples.\n\n    Parameters\n    ----------\n    X : array-like\n        Samples to classify. One-dimensional arrays are treated as a single\n        sample; two-dimensional arrays must have shape ``(n_samples, n_features)``.\n\n    Returns:\n    -------\n    np.ndarray\n        Predicted class labels with shape ``(n_samples,)``.\n\n    Raises:\n    ------\n    RuntimeError\n        If invoked before the estimator is fitted.\n    ValueError\n        When the supplied samples do not match the fitted feature count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\", \"classes_\"])\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n    else:\n        X_arr, _ = _ensure_2d_array(X)\n\n    if self.n_features_in_ is None:\n        raise RuntimeError(\"Model must be fitted before calling predict.\")\n    if X_arr.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Feature mismatch: expected {self.n_features_in_}, got {X_arr.shape[1]}.\")\n\n    encoded = np.asarray(self.model_.predict(X_arr), dtype=int)  # type: ignore[operator]\n    return np.asarray(self.classes_)[encoded]\n</code></pre>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities for the provided samples.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba--parameters","title":"Parameters","text":"<p>X : array-like     Samples for which to estimate class probabilities.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba--returns","title":"Returns:","text":"<p>np.ndarray     Matrix of shape <code>(n_samples, n_classes)</code> containing class     probability estimates.</p>"},{"location":"api/classifier/#anfis_toolbox.classifier.ANFISClassifier.predict_proba--raises","title":"Raises:","text":"<p>RuntimeError     If the estimator has not been fitted. ValueError     If sample dimensionality does not match the fitted feature count.</p> Source code in <code>anfis_toolbox/classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for the provided samples.\n\n    Parameters\n    ----------\n    X : array-like\n        Samples for which to estimate class probabilities.\n\n    Returns:\n    -------\n    np.ndarray\n        Matrix of shape ``(n_samples, n_classes)`` containing class\n        probability estimates.\n\n    Raises:\n    ------\n    RuntimeError\n        If the estimator has not been fitted.\n    ValueError\n        If sample dimensionality does not match the fitted feature count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n    else:\n        X_arr, _ = _ensure_2d_array(X)\n\n    if self.n_features_in_ is None:\n        raise RuntimeError(\"Model must be fitted before calling predict_proba.\")\n    if X_arr.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Feature mismatch: expected {self.n_features_in_}, got {X_arr.shape[1]}.\")\n\n    return np.asarray(self.model_.predict_proba(X_arr), dtype=float)  # type: ignore[operator]\n</code></pre>"},{"location":"api/clustering/","title":"Clustering","text":""},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans","title":"anfis_toolbox.clustering.FuzzyCMeans","text":"<pre><code>FuzzyCMeans(\n    n_clusters: int,\n    m: float = 2.0,\n    max_iter: int = 300,\n    tol: float = 0.0001,\n    random_state: int | None = None,\n)\n</code></pre> <p>Fuzzy C-Means clustering.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters (&gt;= 2).</p> required <code>m</code> <code>float</code> <p>Fuzzifier (&gt; 1). Default 2.0.</p> <code>2.0</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations.</p> <code>300</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on centers.</p> <code>0.0001</code> <code>random_state</code> <code>int | None</code> <p>Optional seed for reproducibility.</p> <code>None</code> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def __init__(\n    self,\n    n_clusters: int,\n    m: float = 2.0,\n    max_iter: int = 300,\n    tol: float = 1e-4,\n    random_state: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize FuzzyCMeans with hyperparameters.\"\"\"\n    if n_clusters &lt; 2:\n        raise ValueError(\"n_clusters must be &gt;= 2\")\n    if m &lt;= 1:\n        raise ValueError(\"m (fuzzifier) must be &gt; 1\")\n    self.n_clusters = int(n_clusters)\n    self.m = float(m)\n    self.max_iter = int(max_iter)\n    self.tol = float(tol)\n    self.random_state = random_state\n    self.cluster_centers_ = None\n    self.membership_ = None\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.classification_entropy","title":"classification_entropy","text":"<pre><code>classification_entropy() -&gt; float\n</code></pre> <p>Classification Entropy (CE). Lower is better (crisper).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def classification_entropy(self) -&gt; float:\n    \"\"\"Classification Entropy (CE). Lower is better (crisper).\"\"\"\n    if self.membership_ is None:\n        raise RuntimeError(\"Fit the model before calling classification_entropy().\")\n    return _ce(self.membership_)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.fit","title":"fit","text":"<pre><code>fit(X: ndarray) -&gt; FuzzyCMeans\n</code></pre> <p>Fit the FCM model.</p> <p>Sets cluster_centers_ (k,d) and membership_ (n,k).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; FuzzyCMeans:\n    \"\"\"Fit the FCM model.\n\n    Sets cluster_centers_ (k,d) and membership_ (n,k).\n    \"\"\"\n    X = self._check_X(X)\n    n, _ = X.shape\n    if n &lt; self.n_clusters:\n        raise ValueError(\"n_samples must be &gt;= n_clusters\")\n    U = self._init_membership(n)\n    m = self.m\n\n    def update_centers(Um: np.ndarray) -&gt; np.ndarray:\n        num = Um.T @ X  # (k,d)\n        den = np.maximum(Um.sum(axis=0)[:, None], 1e-12)\n        return num / den\n\n    Um = U**m\n    C = update_centers(Um)\n    for _ in range(self.max_iter):\n        d2 = np.maximum(self._pairwise_sq_dists(X, C), 1e-12)  # (n,k)\n        inv = d2 ** (-1.0 / (m - 1.0))\n        U_new = inv / np.sum(inv, axis=1, keepdims=True)\n        Um_new = U_new**m\n        C_new = update_centers(Um_new)\n        if np.max(np.linalg.norm(C_new - C, axis=1)) &lt; self.tol:\n            U, C = U_new, C_new\n            break\n        U, C = U_new, C_new\n    self.membership_ = U\n    self.cluster_centers_ = C\n    return self\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Fit and return hard labels via argmax of membership.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def fit_predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Fit and return hard labels via argmax of membership.\"\"\"\n    self.fit(X)\n    return self.predict(X)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.partition_coefficient","title":"partition_coefficient","text":"<pre><code>partition_coefficient() -&gt; float\n</code></pre> <p>Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def partition_coefficient(self) -&gt; float:\n    \"\"\"Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.\"\"\"\n    if self.membership_ is None:\n        raise RuntimeError(\"Fit the model before calling partition_coefficient().\")\n    return _pc(self.membership_)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return hard labels via argmax of predict_proba.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return hard labels via argmax of predict_proba.\"\"\"\n    U = self.predict_proba(X)\n    return np.argmax(U, axis=1)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return membership degrees for samples to clusters (rows sum to 1).</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return membership degrees for samples to clusters (rows sum to 1).\"\"\"\n    if self.cluster_centers_ is None:\n        raise RuntimeError(\"Call fit() before predict_proba().\")\n    X = self._check_X(X)\n    C = self.cluster_centers_\n    m = self.m\n    d2 = np.maximum(self._pairwise_sq_dists(X, C), 1e-12)\n    inv = d2 ** (-1.0 / (m - 1.0))\n    return inv / np.sum(inv, axis=1, keepdims=True)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Alias for predict_proba.</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Alias for predict_proba.\"\"\"\n    return self.predict_proba(X)\n</code></pre>"},{"location":"api/clustering/#anfis_toolbox.clustering.FuzzyCMeans.xie_beni_index","title":"xie_beni_index","text":"<pre><code>xie_beni_index(X: ndarray) -&gt; float\n</code></pre> <p>Xie-Beni index (XB). Lower is better.</p> <p>XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)</p> Source code in <code>anfis_toolbox/clustering.py</code> <pre><code>def xie_beni_index(self, X: np.ndarray) -&gt; float:\n    \"\"\"Xie-Beni index (XB). Lower is better.\n\n    XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)\n    \"\"\"\n    if self.membership_ is None or self.cluster_centers_ is None:\n        raise RuntimeError(\"Fit the model before calling xie_beni_index().\")\n    X = self._check_X(X)\n    return _xb(X, self.membership_, self.cluster_centers_, m=self.m)\n</code></pre>"},{"location":"api/config/","title":"Configuration Utilities","text":"<p>API reference for configuration helpers that let you persist ANFIS setups, manage presets, and export trained models.</p>"},{"location":"api/config/#anfis_toolbox.config","title":"anfis_toolbox.config","text":"<p>Configuration utilities for ANFIS models.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig","title":"ANFISConfig","text":"<pre><code>ANFISConfig()\n</code></pre> <p>Configuration manager for ANFIS models.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation of configuration.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.add_input_config","title":"add_input_config","text":"<pre><code>add_input_config(\n    name: str,\n    range_min: float,\n    range_max: float,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    overlap: float = 0.5,\n) -&gt; ANFISConfig\n</code></pre> <p>Add input configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Input variable name</p> required <code>range_min</code> <code>float</code> <p>Minimum input range</p> required <code>range_max</code> <code>float</code> <p>Maximum input range</p> required <code>n_mfs</code> <code>int</code> <p>Number of membership functions</p> <code>3</code> <code>mf_type</code> <code>str</code> <p>Type of membership functions</p> <code>'gaussian'</code> <code>overlap</code> <code>float</code> <p>Overlap factor</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.build_model","title":"build_model","text":"<pre><code>build_model() -&gt; ANFIS\n</code></pre> <p>Build ANFIS model from configuration.</p> <p>Returns:</p> Type Description <code>ANFIS</code> <p>Configured ANFIS model</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: str | Path) -&gt; ANFISConfig\n</code></pre> <p>Load configuration from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to configuration file</p> required <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>ANFISConfig object</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.save","title":"save","text":"<pre><code>save(filepath: str | Path)\n</code></pre> <p>Save configuration to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to save configuration file</p> required"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.set_training_config","title":"set_training_config","text":"<pre><code>set_training_config(\n    method: str = \"hybrid\",\n    epochs: int = 50,\n    learning_rate: float = 0.01,\n    verbose: bool = False,\n) -&gt; ANFISConfig\n</code></pre> <p>Set training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Training method ('hybrid' or 'backprop')</p> <code>'hybrid'</code> <code>epochs</code> <code>int</code> <p>Number of training epochs</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Learning rate</p> <code>0.01</code> <code>verbose</code> <code>bool</code> <p>Whether to show training progress</p> <code>False</code> <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>Self for method chaining</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert configuration to dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Configuration dictionary</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISModelManager","title":"ANFISModelManager","text":"<p>Model management utilities for saving/loading trained ANFIS models.</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISModelManager.load_model","title":"load_model  <code>staticmethod</code>","text":"<pre><code>load_model(filepath: str | Path) -&gt; ANFIS\n</code></pre> <p>Load trained ANFIS model from file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to model file</p> required <p>Returns:</p> Type Description <code>ANFIS</code> <p>Loaded ANFIS model</p>"},{"location":"api/config/#anfis_toolbox.config.ANFISModelManager.save_model","title":"save_model  <code>staticmethod</code>","text":"<pre><code>save_model(\n    model: ANFIS,\n    filepath: str | Path,\n    include_config: bool = True,\n)\n</code></pre> <p>Save trained ANFIS model to file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ANFIS</code> <p>Trained ANFIS model</p> required <code>filepath</code> <code>str | Path</code> <p>Path to save model file</p> required <code>include_config</code> <code>bool</code> <p>Whether to save model configuration</p> <code>True</code>"},{"location":"api/config/#anfis_toolbox.config.create_config_from_preset","title":"create_config_from_preset","text":"<pre><code>create_config_from_preset(preset_name: str) -&gt; ANFISConfig\n</code></pre> <p>Create configuration from predefined preset.</p> <p>Parameters:</p> Name Type Description Default <code>preset_name</code> <code>str</code> <p>Name of predefined configuration</p> required <p>Returns:</p> Type Description <code>ANFISConfig</code> <p>ANFISConfig object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If preset name not found</p>"},{"location":"api/config/#anfis_toolbox.config.list_presets","title":"list_presets","text":"<pre><code>list_presets() -&gt; dict[str, str]\n</code></pre> <p>List available predefined configurations.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping preset names to descriptions</p>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer","title":"anfis_toolbox.layers.MembershipLayer","text":"<pre><code>MembershipLayer(input_mfs: dict)\n</code></pre> <p>Membership layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This is the first layer of ANFIS that applies membership functions to input variables. Each input variable has multiple membership functions that transform crisp input values into fuzzy membership degrees.</p> <p>This layer serves as the fuzzification stage, converting crisp inputs into fuzzy sets that can be processed by subsequent ANFIS layers.</p> <p>Attributes:</p> Name Type Description <code>input_mfs</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.</p> <code>input_names</code> <code>list</code> <p>List of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_mfs</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.              Format: {input_name: [MembershipFunction, ...]}</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, input_mfs: dict):\n    \"\"\"Initializes the membership layer with input membership functions.\n\n    Parameters:\n        input_mfs (dict): Dictionary mapping input names to lists of membership functions.\n                         Format: {input_name: [MembershipFunction, ...]}\n    \"\"\"\n    self.input_mfs = input_mfs\n    self.input_names = list(input_mfs.keys())\n    self.n_inputs = len(input_mfs)\n    self.mf_per_input = [len(mfs) for mfs in input_mfs.values()]\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.membership_functions","title":"membership_functions  <code>property</code>","text":"<pre><code>membership_functions: dict\n</code></pre> <p>Alias for input_mfs to provide a standardized interface.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to lists of membership functions.</p>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.backward","title":"backward","text":"<pre><code>backward(gradients: dict) -&gt; dict\n</code></pre> <p>Performs backward pass to compute gradients for membership functions.</p> <p>Parameters:</p> Name Type Description Default <code>gradients</code> <code>dict</code> <p>Dictionary mapping input names to gradient arrays.              Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Nested structure with parameter gradients mirroring <code>model.get_gradients()</code>.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, gradients: dict) -&gt; dict:\n    \"\"\"Performs backward pass to compute gradients for membership functions.\n\n    Parameters:\n        gradients (dict): Dictionary mapping input names to gradient arrays.\n                         Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n\n    Returns:\n        dict: Nested structure with parameter gradients mirroring ``model.get_gradients()``.\n    \"\"\"\n    param_grads: dict[str, list[dict[str, float]]] = {}\n\n    for name in self.input_names:\n        mfs = self.input_mfs[name]\n        grad_array = gradients[name]\n        mf_param_grads: list[dict[str, float]] = []\n\n        for mf_idx, mf in enumerate(mfs):\n            prev = {key: float(value) for key, value in mf.gradients.items()}\n            mf_gradient = grad_array[:, mf_idx]\n            mf.backward(mf_gradient)\n            updated = mf.gradients\n            delta = {key: float(updated[key] - prev.get(key, 0.0)) for key in updated}\n            mf_param_grads.append(delta)\n\n        param_grads[name] = mf_param_grads\n\n    return {\"membership\": param_grads}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; dict\n</code></pre> <p>Performs forward pass to compute membership degrees for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data with shape (batch_size, n_inputs).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to membership degree arrays.  Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; dict:\n    \"\"\"Performs forward pass to compute membership degrees for all inputs.\n\n    Parameters:\n        x (np.ndarray): Input data with shape (batch_size, n_inputs).\n\n    Returns:\n        dict: Dictionary mapping input names to membership degree arrays.\n             Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n    \"\"\"\n    _batch_size = x.shape[0]\n    membership_outputs = {}\n\n    # Compute membership degrees for each input variable\n    for i, name in enumerate(self.input_names):\n        mfs = self.input_mfs[name]\n        # Apply each membership function to the i-th input\n        mu_values = []\n        for mf in mfs:\n            mu = mf(x[:, i])  # (batch_size,)\n            mu_values.append(mu)\n\n        # Stack membership values for all MFs of this input\n        membership_outputs[name] = np.stack(mu_values, axis=-1)  # (batch_size, n_mfs)\n\n    # Cache values for backward pass\n    self.last = {\"x\": x, \"membership_outputs\": membership_outputs}\n\n    return membership_outputs\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.MembershipLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets all membership functions to their initial state.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets all membership functions to their initial state.\n\n    Returns:\n        None\n    \"\"\"\n    for name in self.input_names:\n        for mf in self.input_mfs[name]:\n            mf.reset()\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer","title":"anfis_toolbox.layers.RuleLayer","text":"<pre><code>RuleLayer(\n    input_names: list,\n    mf_per_input: list,\n    rules: Sequence[Sequence[int]] | None = None,\n)\n</code></pre> <p>Rule layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer computes the rule strengths (firing strengths) by applying the T-norm (typically product) operation to the membership degrees of all input variables for each rule.</p> <p>This is the second layer of ANFIS that takes membership degrees from the MembershipLayer and computes rule activations.</p> <p>Attributes:</p> Name Type Description <code>input_names</code> <code>list</code> <p>List of input variable names.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input.</p> <code>rules</code> <code>list</code> <p>List of all possible rule combinations.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_names</code> <code>list</code> <p>List of input variable names.</p> required <code>mf_per_input</code> <code>list</code> <p>Number of membership functions per input variable.</p> required <code>rules</code> <code>Sequence[Sequence[int]] | None</code> <p>Optional explicit rule set where each rule is a sequence of membership-function indices, one per input. When <code>None</code>, the full Cartesian product of membership functions is used.</p> <code>None</code> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(\n    self,\n    input_names: list,\n    mf_per_input: list,\n    rules: Sequence[Sequence[int]] | None = None,\n):\n    \"\"\"Initializes the rule layer with input configuration.\n\n    Parameters:\n        input_names (list): List of input variable names.\n        mf_per_input (list): Number of membership functions per input variable.\n        rules (Sequence[Sequence[int]] | None): Optional explicit rule set where each\n            rule is a sequence of membership-function indices, one per input. When\n            ``None``, the full Cartesian product of membership functions is used.\n    \"\"\"\n    self.input_names = input_names\n    self.n_inputs = len(input_names)\n    self.mf_per_input = list(mf_per_input)\n\n    if rules is None:\n        # Generate all possible rule combinations (Cartesian product)\n        self.rules = [tuple(rule) for rule in product(*[range(n) for n in self.mf_per_input])]\n    else:\n        validated_rules: list[tuple[int, ...]] = []\n        for idx, rule in enumerate(rules):\n            if len(rule) != self.n_inputs:\n                raise ValueError(\n                    \"Each rule must specify exactly one membership index per input. \"\n                    f\"Rule at position {idx} has length {len(rule)} while {self.n_inputs} were expected.\"\n                )\n            normalized_rule: list[int] = []\n            for input_idx, mf_idx in enumerate(rule):\n                max_mf = self.mf_per_input[input_idx]\n                if not 0 &lt;= mf_idx &lt; max_mf:\n                    raise ValueError(\n                        \"Rule membership index out of range. \"\n                        f\"Received {mf_idx} for input {input_idx} with {max_mf} membership functions.\"\n                    )\n                normalized_rule.append(int(mf_idx))\n            validated_rules.append(tuple(normalized_rule))\n\n        if not validated_rules:\n            raise ValueError(\"At least one rule must be provided when specifying custom rules.\")\n        self.rules = validated_rules\n\n    self.n_rules = len(self.rules)\n\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer.backward","title":"backward","text":"<pre><code>backward(dL_dw: ndarray) -&gt; dict\n</code></pre> <p>Performs backward pass to compute gradients for membership functions.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dw</code> <code>ndarray</code> <p>Gradient of loss with respect to rule strengths.                Shape: (batch_size, n_rules)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping input names to gradient arrays for membership functions.  Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dw: np.ndarray) -&gt; dict:\n    \"\"\"Performs backward pass to compute gradients for membership functions.\n\n    Parameters:\n        dL_dw (np.ndarray): Gradient of loss with respect to rule strengths.\n                           Shape: (batch_size, n_rules)\n\n    Returns:\n        dict: Dictionary mapping input names to gradient arrays for membership functions.\n             Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n    \"\"\"\n    batch_size = dL_dw.shape[0]\n    mu = self.last[\"mu\"]  # (batch_size, n_inputs, n_mfs)\n\n    # Initialize gradient accumulators for each input's membership functions\n    gradients = {}\n    for i, name in enumerate(self.input_names):\n        n_mfs = self.mf_per_input[i]\n        gradients[name] = np.zeros((batch_size, n_mfs))\n\n    # Compute gradients for each rule\n    for rule_idx, rule in enumerate(self.rules):\n        for input_idx, mf_idx in enumerate(rule):\n            name = self.input_names[input_idx]\n\n            # Compute partial derivative: d(rule_strength)/d(mu_ij)\n            # This is the product of all other membership degrees in the rule\n            other_factors = []\n            for j, j_mf in enumerate(rule):\n                if j == input_idx:\n                    continue  # Skip the current input\n                other_factors.append(mu[:, j, j_mf])\n\n            # Product of other factors (or 1 if no other factors)\n            partial = np.prod(other_factors, axis=0) if other_factors else np.ones(batch_size)\n\n            # Apply chain rule: dL/dmu = dL/dw * dw/dmu\n            gradients[name][:, mf_idx] += dL_dw[:, rule_idx] * partial\n\n    return gradients\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.RuleLayer.forward","title":"forward","text":"<pre><code>forward(membership_outputs: dict) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to compute rule strengths.</p> <p>Parameters:</p> Name Type Description Default <code>membership_outputs</code> <code>dict</code> <p>Dictionary mapping input names to membership degree arrays.                      Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Rule strengths with shape (batch_size, n_rules).</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, membership_outputs: dict) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to compute rule strengths.\n\n    Parameters:\n        membership_outputs (dict): Dictionary mapping input names to membership degree arrays.\n                                 Format: {input_name: np.ndarray with shape (batch_size, n_mfs)}\n\n    Returns:\n        np.ndarray: Rule strengths with shape (batch_size, n_rules).\n    \"\"\"\n    # Convert membership outputs to array format for easier processing\n    mu_list = []\n    for name in self.input_names:\n        mu_list.append(membership_outputs[name])  # (batch_size, n_mfs)\n    mu = np.stack(mu_list, axis=1)  # (batch_size, n_inputs, n_mfs)\n\n    _batch_size = mu.shape[0]\n\n    # Compute rule activations (firing strengths)\n    rule_activations = []\n    for rule in self.rules:\n        rule_mu = []\n        # Get membership degree for each input in this rule\n        for input_idx, mf_idx in enumerate(rule):\n            rule_mu.append(mu[:, input_idx, mf_idx])  # (batch_size,)\n        # Apply T-norm (product) to get rule strength\n        rule_strength = np.prod(rule_mu, axis=0)  # (batch_size,)\n        rule_activations.append(rule_strength)\n\n    rule_activations = np.stack(rule_activations, axis=1)  # (batch_size, n_rules)\n\n    # Cache values for backward pass\n    self.last = {\"membership_outputs\": membership_outputs, \"mu\": mu, \"rule_activations\": rule_activations}\n\n    return rule_activations\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer","title":"anfis_toolbox.layers.NormalizationLayer","text":"<pre><code>NormalizationLayer()\n</code></pre> <p>Normalization layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer normalizes the rule strengths (firing strengths) to ensure they sum to 1.0 for each sample in the batch. This is a crucial step in ANFIS as it converts rule strengths to normalized rule weights.</p> <p>The normalization formula is: norm_w_i = w_i / sum(w_j for all j)</p> <p>Attributes:</p> Name Type Description <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the normalization layer.\"\"\"\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer.backward","title":"backward","text":"<pre><code>backward(dL_dnorm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs backward pass to compute gradients for original rule weights.</p> <p>The gradient computation uses the quotient rule for derivatives: If norm_w_i = w_i / sum_w, then: - d(norm_w_i)/d(w_i) = (sum_w - w_i) / sum_w\u00b2 - d(norm_w_i)/d(w_j) = -w_j / sum_w\u00b2 for j \u2260 i</p> <p>Parameters:</p> Name Type Description Default <code>dL_dnorm_w</code> <code>ndarray</code> <p>Gradient of loss with respect to normalized weights.                     Shape: (batch_size, n_rules)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Gradient of loss with respect to original weights.        Shape: (batch_size, n_rules)</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dnorm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs backward pass to compute gradients for original rule weights.\n\n    The gradient computation uses the quotient rule for derivatives:\n    If norm_w_i = w_i / sum_w, then:\n    - d(norm_w_i)/d(w_i) = (sum_w - w_i) / sum_w\u00b2\n    - d(norm_w_i)/d(w_j) = -w_j / sum_w\u00b2 for j \u2260 i\n\n    Parameters:\n        dL_dnorm_w (np.ndarray): Gradient of loss with respect to normalized weights.\n                                Shape: (batch_size, n_rules)\n\n    Returns:\n        np.ndarray: Gradient of loss with respect to original weights.\n                   Shape: (batch_size, n_rules)\n    \"\"\"\n    w = self.last[\"w\"]  # (batch_size, n_rules)\n    sum_w = self.last[\"sum_w\"]  # (batch_size, 1)\n\n    # Jacobian-vector product without building the full Jacobian:\n    # (J^T g)_j = (sum_w * g_j - (g \u00b7 w)) / sum_w^2\n    g = dL_dnorm_w  # (batch_size, n_rules)\n    s = sum_w  # (batch_size, 1)\n    gw_dot = np.sum(g * w, axis=1, keepdims=True)  # (batch_size, 1)\n    dL_dw = (s * g - gw_dot) / (s**2)  # (batch_size, n_rules)\n\n    return dL_dw\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.NormalizationLayer.forward","title":"forward","text":"<pre><code>forward(w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to normalize rule weights.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>Rule strengths with shape (batch_size, n_rules).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized rule weights with shape (batch_size, n_rules).        Each row sums to 1.0.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to normalize rule weights.\n\n    Parameters:\n        w (np.ndarray): Rule strengths with shape (batch_size, n_rules).\n\n    Returns:\n        np.ndarray: Normalized rule weights with shape (batch_size, n_rules).\n                   Each row sums to 1.0.\n    \"\"\"\n    # Add small epsilon to avoid division by zero\n    sum_w = np.sum(w, axis=1, keepdims=True) + 1e-8\n    norm_w = w / sum_w\n\n    # Cache values for backward pass\n    self.last = {\"w\": w, \"sum_w\": sum_w, \"norm_w\": norm_w}\n    return norm_w\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer","title":"anfis_toolbox.layers.ConsequentLayer","text":"<pre><code>ConsequentLayer(n_rules: int, n_inputs: int)\n</code></pre> <p>Consequent layer for ANFIS (Adaptive Neuro-Fuzzy Inference System).</p> <p>This layer implements the consequent part of fuzzy rules in ANFIS. Each rule has a linear consequent function of the form: f_i(x) = p_i * x_1 + q_i * x_2 + ... + r_i (TSK model)</p> <p>The final output is computed as a weighted sum: y = \u03a3(w_i * f_i(x)) where w_i are normalized rule weights</p> <p>Attributes:</p> Name Type Description <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules.</p> <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>parameters</code> <code>ndarray</code> <p>Linear parameters for each rule with shape (n_rules, n_inputs + 1).                     Each row contains [p_i, q_i, ..., r_i] for rule i.</p> <code>gradients</code> <code>ndarray</code> <p>Accumulated gradients for parameters.</p> <code>last</code> <code>dict</code> <p>Cache of last forward pass computations for backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules.</p> required <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, n_rules: int, n_inputs: int):\n    \"\"\"Initializes the consequent layer with random linear parameters.\n\n    Parameters:\n        n_rules (int): Number of fuzzy rules.\n        n_inputs (int): Number of input variables.\n    \"\"\"\n    # Each rule has (n_inputs + 1) parameters: p_i, q_i, ..., r_i (including bias)\n    self.n_rules = n_rules\n    self.n_inputs = n_inputs\n    self.parameters = np.random.randn(n_rules, n_inputs + 1)\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Performs backward pass to compute gradients for parameters and inputs.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of loss with respect to layer output.                Shape: (batch_size, 1)</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(dL_dnorm_w, dL_dx) where: - dL_dnorm_w: Gradient w.r.t. normalized weights, shape (batch_size, n_rules) - dL_dx: Gradient w.r.t. input x, shape (batch_size, n_inputs)</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Performs backward pass to compute gradients for parameters and inputs.\n\n    Parameters:\n        dL_dy (np.ndarray): Gradient of loss with respect to layer output.\n                           Shape: (batch_size, 1)\n\n    Returns:\n        tuple: (dL_dnorm_w, dL_dx) where:\n            - dL_dnorm_w: Gradient w.r.t. normalized weights, shape (batch_size, n_rules)\n            - dL_dx: Gradient w.r.t. input x, shape (batch_size, n_inputs)\n    \"\"\"\n    X_aug = self.last[\"X_aug\"]  # (batch_size, n_inputs + 1)\n    norm_w = self.last[\"norm_w\"]  # (batch_size, n_rules)\n    f = self.last[\"f\"]  # (batch_size, n_rules)\n\n    batch_size = X_aug.shape[0]\n\n    # Compute gradients for consequent parameters\n    self.gradients = np.zeros_like(self.parameters)\n\n    for i in range(self.n_rules):\n        # Gradient of y_hat w.r.t. parameters of rule i: norm_w_i * x_aug\n        for b in range(batch_size):\n            self.gradients[i] += dL_dy[b, 0] * norm_w[b, i] * X_aug[b]\n\n    # Compute gradient of loss w.r.t. normalized weights\n    # dy/dnorm_w_i = f_i(x), so dL/dnorm_w_i = dL/dy * f_i(x)\n    dL_dnorm_w = dL_dy * f  # (batch_size, n_rules)\n\n    # Compute gradient of loss w.r.t. input x (for backpropagation to previous layers)\n    dL_dx = np.zeros((batch_size, self.n_inputs))\n\n    for b in range(batch_size):\n        for i in range(self.n_rules):\n            # dy/dx = norm_w_i * parameters_i[:-1] (excluding bias term)\n            dL_dx[b] += dL_dy[b, 0] * norm_w[b, i] * self.parameters[i, :-1]\n\n    return dL_dnorm_w, dL_dx\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray, norm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Performs forward pass to compute the final ANFIS output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data with shape (batch_size, n_inputs).</p> required <code>norm_w</code> <code>ndarray</code> <p>Normalized rule weights with shape (batch_size, n_rules).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Final ANFIS output with shape (batch_size, 1).</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray, norm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Performs forward pass to compute the final ANFIS output.\n\n    Parameters:\n        x (np.ndarray): Input data with shape (batch_size, n_inputs).\n        norm_w (np.ndarray): Normalized rule weights with shape (batch_size, n_rules).\n\n    Returns:\n        np.ndarray: Final ANFIS output with shape (batch_size, 1).\n    \"\"\"\n    batch_size = x.shape[0]\n\n    # Augment input with bias term (column of ones)\n    X_aug = np.hstack([x, np.ones((batch_size, 1))])  # (batch_size, n_inputs + 1)\n\n    # Compute consequent function f_i(x) for each rule\n    # f[b, i] = p_i * x[b, 0] + q_i * x[b, 1] + ... + r_i\n    f = X_aug @ self.parameters.T  # (batch_size, n_rules)\n\n    # Compute final output as weighted sum: y = \u03a3(w_i * f_i(x))\n    y_hat = np.sum(norm_w * f, axis=1, keepdims=True)  # (batch_size, 1)\n\n    # Cache values for backward pass\n    self.last = {\"X_aug\": X_aug, \"norm_w\": norm_w, \"f\": f}\n\n    return y_hat\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ConsequentLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets gradients and cached values.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets gradients and cached values.\n\n    Returns:\n        None\n    \"\"\"\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer","title":"anfis_toolbox.layers.ClassificationConsequentLayer","text":"<pre><code>ClassificationConsequentLayer(\n    n_rules: int,\n    n_inputs: int,\n    n_classes: int,\n    random_state: int | None = None,\n)\n</code></pre> <p>Consequent layer that produces per-class logits for classification.</p> <p>Each rule i has a vector of class logits with a linear function of inputs: f_i(x) = W_i x + b_i, where W_i has shape (n_classes, n_inputs) and b_i (n_classes,). We store parameters as a single array of shape (n_rules, n_classes, n_inputs + 1).</p> <p>Parameters:</p> Name Type Description Default <code>n_rules</code> <code>int</code> <p>Number of fuzzy rules in the layer.</p> required <code>n_inputs</code> <code>int</code> <p>Number of input features.</p> required <code>n_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>random_state</code> <code>int | None</code> <p>Random seed for parameter initialization.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_rules</code> <code>int</code> <p>Stores the number of fuzzy rules.</p> <code>n_inputs</code> <code>int</code> <p>Stores the number of input features.</p> <code>n_classes</code> <code>int</code> <p>Stores the number of output classes.</p> <code>parameters</code> <code>ndarray</code> <p>Randomly initialized parameters for each rule, class, and input (including bias).</p> <code>gradients</code> <code>ndarray</code> <p>Gradient values initialized to zeros, matching the shape of parameters.</p> <code>last</code> <code>dict</code> <p>Dictionary for storing intermediate results or state.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def __init__(self, n_rules: int, n_inputs: int, n_classes: int, random_state: int | None = None):\n    \"\"\"Initializes the layer with the specified number of rules, inputs, and classes.\n\n    Args:\n        n_rules (int): Number of fuzzy rules in the layer.\n        n_inputs (int): Number of input features.\n        n_classes (int): Number of output classes.\n        random_state (int | None): Random seed for parameter initialization.\n\n    Attributes:\n        n_rules (int): Stores the number of fuzzy rules.\n        n_inputs (int): Stores the number of input features.\n        n_classes (int): Stores the number of output classes.\n\n\n        parameters (np.ndarray): Randomly initialized parameters for each rule, class, and input (including bias).\n        gradients (np.ndarray): Gradient values initialized to zeros, matching the shape of parameters.\n        last (dict): Dictionary for storing intermediate results or state.\n    \"\"\"\n    self.n_rules = n_rules\n    self.n_inputs = n_inputs\n    self.n_classes = n_classes\n    if random_state is None:\n        self.parameters = np.random.randn(n_rules, n_classes, n_inputs + 1)\n    else:\n        rng = np.random.default_rng(random_state)\n        self.parameters = rng.normal(size=(n_rules, n_classes, n_inputs + 1))\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.backward","title":"backward","text":"<pre><code>backward(dL_dlogits: ndarray)\n</code></pre> <p>Computes the backward pass for the classification consequent layer.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def backward(self, dL_dlogits: np.ndarray):\n    \"\"\"Computes the backward pass for the classification consequent layer.\"\"\"\n    X_aug = self.last[\"X_aug\"]  # (b, d+1)\n    norm_w = self.last[\"norm_w\"]  # (b, r)\n    f = self.last[\"f\"]  # (b, r, k)\n\n    # Gradients w.r.t. per-rule parameters\n    self.gradients = np.zeros_like(self.parameters)\n    # dL/df_{brk} = dL/dlogits_{bk} * norm_w_{br}\n    dL_df = dL_dlogits[:, None, :] * norm_w[:, :, None]  # (b, r, k)\n    # Accumulate over batch: grad[r,k,d] = sum_b dL_df[b,r,k] * X_aug[b,d]\n    self.gradients = np.einsum(\"brk,bd-&gt;rkd\", dL_df, X_aug)\n\n    # dL/dnorm_w: sum_k dL/dlogits_{bk} * f_{brk}\n    dL_dnorm_w = np.einsum(\"bk,brk-&gt;br\", dL_dlogits, f)\n\n    # dL/dx: sum_r sum_k dL/dlogits_{bk} * norm_w_{br} * W_{r,k,:}\n    W = self.parameters[:, :, :-1]  # (r,k,d)\n    dL_dx = np.einsum(\"bk,br,rkd-&gt;bd\", dL_dlogits, norm_w, W)\n    return dL_dnorm_w, dL_dx\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.forward","title":"forward","text":"<pre><code>forward(x: ndarray, norm_w: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes the forward pass for the classification consequent layer.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def forward(self, x: np.ndarray, norm_w: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the forward pass for the classification consequent layer.\"\"\"\n    batch = x.shape[0]\n    X_aug = np.hstack([x, np.ones((batch, 1))])  # (b, d+1)\n    # Compute per-rule class logits: (b, r, k)\n    f = np.einsum(\"bd,rkd-&gt;brk\", X_aug, self.parameters)\n    # Weighted sum over rules -&gt; logits (b, k)\n    logits = np.einsum(\"br,brk-&gt;bk\", norm_w, f)\n    self.last = {\"X_aug\": X_aug, \"norm_w\": norm_w, \"f\": f}\n    return logits\n</code></pre>"},{"location":"api/layers/#anfis_toolbox.layers.ClassificationConsequentLayer.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the gradients and cached values.</p> Source code in <code>anfis_toolbox/layers.py</code> <pre><code>def reset(self):\n    \"\"\"Resets the gradients and cached values.\"\"\"\n    self.gradients = np.zeros_like(self.parameters)\n    self.last = {}\n</code></pre>"},{"location":"api/logging/","title":"Logging Utilities","text":"<p>Helper functions that configure logging for training workflows.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config","title":"anfis_toolbox.logging_config","text":"<p>Logging configuration for ANFIS toolbox.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config.disable_training_logs","title":"disable_training_logs","text":"<pre><code>disable_training_logs() -&gt; None\n</code></pre> <p>Disable training progress logs.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config.enable_training_logs","title":"enable_training_logs","text":"<pre><code>enable_training_logs() -&gt; None\n</code></pre> <p>Enable training progress logs with a simple format.</p>"},{"location":"api/logging/#anfis_toolbox.logging_config.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n    level: str = \"INFO\", format_string: str | None = None\n) -&gt; None\n</code></pre> <p>Setup logging configuration for ANFIS toolbox.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL').</p> <code>'INFO'</code> <code>format_string</code> <code>str</code> <p>Custom format string for log messages.</p> <code>None</code>"},{"location":"api/losses/","title":"Losses API","text":"<p>This module provides loss functions and their gradients used during ANFIS model training.</p>"},{"location":"api/losses/#anfis_toolbox.losses","title":"anfis_toolbox.losses","text":"<p>Loss functions and their gradients for ANFIS Toolbox.</p> <p>This module centralizes the loss definitions used during training to make it explicit which objective is being optimized. Trainers can import from here so the chosen loss is clear in one place.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss","title":"CrossEntropyLoss","text":"<p>               Bases: <code>LossFunction</code></p> <p>Categorical cross-entropy loss operating on logits.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss.gradient","title":"gradient","text":"<pre><code>gradient(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Delegate to :func:<code>cross_entropy_grad</code> for gradient computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss.loss","title":"loss","text":"<pre><code>loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Delegate to :func:<code>cross_entropy_loss</code> for value computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.CrossEntropyLoss.prepare_targets","title":"prepare_targets","text":"<pre><code>prepare_targets(\n    y: Any, *, model: Any | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Convert labels or one-hot encodings into dense float matrices.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction","title":"LossFunction","text":"<p>Base interface for losses used by trainers.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction.gradient","title":"gradient","text":"<pre><code>gradient(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return the gradient of the loss with respect to the predictions.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction.loss","title":"loss","text":"<pre><code>loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Compute the scalar loss for the given targets and predictions.</p>"},{"location":"api/losses/#anfis_toolbox.losses.LossFunction.prepare_targets","title":"prepare_targets","text":"<pre><code>prepare_targets(\n    y: Any, *, model: Any | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Return targets in a format compatible with forward/gradient computations.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss","title":"MSELoss","text":"<p>               Bases: <code>LossFunction</code></p> <p>Mean squared error loss packaged for trainer consumption.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss.gradient","title":"gradient","text":"<pre><code>gradient(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Delegate to :func:<code>mse_grad</code> for gradient computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss.loss","title":"loss","text":"<pre><code>loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Delegate to :func:<code>mse_loss</code> for value computation.</p>"},{"location":"api/losses/#anfis_toolbox.losses.MSELoss.prepare_targets","title":"prepare_targets","text":"<pre><code>prepare_targets(\n    y: Any, *, model: Any | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Convert 1D targets into column vectors expected by MSE computations.</p>"},{"location":"api/losses/#anfis_toolbox.losses.cross_entropy_grad","title":"cross_entropy_grad","text":"<pre><code>cross_entropy_grad(\n    y_true: ndarray, logits: ndarray\n) -&gt; np.ndarray\n</code></pre> <p>Gradient of cross-entropy w.r.t logits.</p> <p>Accepts integer labels (n,) or one-hot (n,k). Returns gradient with the same shape as logits: (n,k).</p>"},{"location":"api/losses/#anfis_toolbox.losses.cross_entropy_loss","title":"cross_entropy_loss","text":"<pre><code>cross_entropy_loss(\n    y_true: ndarray, logits: ndarray\n) -&gt; float\n</code></pre> <p>Cross-entropy loss from labels (int or one-hot) and logits.</p> <p>This delegates to metrics.cross_entropy for the scalar value.</p>"},{"location":"api/losses/#anfis_toolbox.losses.mse_grad","title":"mse_grad","text":"<pre><code>mse_grad(y_true: ndarray, y_pred: ndarray) -&gt; np.ndarray\n</code></pre> <p>Gradient of MSE w.r.t. predictions.</p> <p>d/dy_pred MSE = 2 * (y_pred - y_true) / n</p>"},{"location":"api/losses/#anfis_toolbox.losses.mse_loss","title":"mse_loss","text":"<pre><code>mse_loss(y_true: ndarray, y_pred: ndarray) -&gt; float\n</code></pre> <p>Mean squared error (MSE) loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>Array-like true targets of shape (n, d) or (n,).</p> required <code>y_pred</code> <code>ndarray</code> <p>Array-like predictions of same shape as y_true.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar MSE value.</p>"},{"location":"api/losses/#anfis_toolbox.losses.resolve_loss","title":"resolve_loss","text":"<pre><code>resolve_loss(\n    loss: str | LossFunction | None,\n) -&gt; LossFunction\n</code></pre> <p>Resolve user-provided loss spec into a concrete <code>LossFunction</code> instance.</p>"},{"location":"api/losses/#regression-losses","title":"Regression Losses","text":"<p>Functions for regression tasks (continuous output prediction).</p> <ul> <li><code>mse_loss()</code> - Mean squared error loss</li> <li><code>mse_grad()</code> - Gradient of MSE loss</li> </ul>"},{"location":"api/losses/#classification-losses","title":"Classification Losses","text":"<p>Functions for classification tasks (discrete output prediction).</p> <ul> <li><code>cross_entropy_loss()</code> - Cross-entropy loss</li> <li><code>cross_entropy_grad()</code> - Gradient of cross-entropy loss</li> </ul>"},{"location":"api/membership-functions/","title":"Membership Functions","text":""},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF","title":"anfis_toolbox.membership.GaussianMF","text":"<pre><code>GaussianMF(mean: float = 0.0, sigma: float = 1.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Gaussian Membership Function.</p> <p>Implements a Gaussian (bell-shaped) membership function using the formula: \u03bc(x) = exp(-((x - mean)\u00b2 / (2 * sigma\u00b2)))</p> <p>This function is commonly used in fuzzy logic systems due to its smooth and differentiable properties.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>Mean of the Gaussian (center). Defaults to 0.0.</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation (width). Defaults to 1.0.</p> <code>1.0</code> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, mean: float = 0.0, sigma: float = 1.0):\n    \"\"\"Initialize with mean and standard deviation.\n\n    Args:\n        mean: Mean of the Gaussian (center). Defaults to 0.0.\n        sigma: Standard deviation (width). Defaults to 1.0.\n    \"\"\"\n    super().__init__()\n    self.parameters = {\"mean\": mean, \"sigma\": sigma}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients w.r.t. parameters given upstream gradient.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss with respect to the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients w.r.t. parameters given upstream gradient.\n\n    Args:\n        dL_dy: Gradient of the loss with respect to the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    mean = self.parameters[\"mean\"]\n    sigma = self.parameters[\"sigma\"]\n\n    x = self.last_input\n    y = self.last_output\n\n    z = (x - mean) / sigma\n\n    # Derivatives of the Gaussian function\n    dy_dmean = -y * z / sigma\n    dy_dsigma = y * (z**2) / sigma\n\n    # Gradient with respect to mean\n    dL_dmean = np.sum(dL_dy * dy_dmean)\n\n    # Gradient with respect to sigma\n    dL_dsigma = np.sum(dL_dy * dy_dsigma)\n\n    # Update gradients\n    self.gradients[\"mean\"] += dL_dmean\n    self.gradients[\"sigma\"] += dL_dsigma\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.GaussianMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute Gaussian membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of Gaussian membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Gaussian membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of Gaussian membership values.\n    \"\"\"\n    mean = self.parameters[\"mean\"]\n    sigma = self.parameters[\"sigma\"]\n    self.last_input = x\n    self.last_output = np.exp(-((x - mean) ** 2) / (2 * sigma**2))\n    return self.last_output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF","title":"anfis_toolbox.membership.Gaussian2MF","text":"<pre><code>Gaussian2MF(\n    sigma1: float = 1.0,\n    c1: float = 0.0,\n    sigma2: float = 1.0,\n    c2: float = 0.0,\n)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Gaussian combination Membership Function (two-sided Gaussian).</p> <p>This membership function uses Gaussian tails on both sides with an optional flat region in the middle.</p> <p>Parameters:</p> Name Type Description Default <code>sigma1</code> <code>float</code> <p>Standard deviation of the left Gaussian tail (must be &gt; 0).</p> <code>1.0</code> <code>c1</code> <code>float</code> <p>Center of the left Gaussian tail.</p> <code>0.0</code> <code>sigma2</code> <code>float</code> <p>Standard deviation of the right Gaussian tail (must be &gt; 0).</p> <code>1.0</code> <code>c2</code> <code>float</code> <p>Center of the right Gaussian tail. Must satisfy c1 &lt;= c2.</p> <code>0.0</code> <p>Definition (with c1 &lt;= c2):     - For x &lt; c1: \u03bc(x) = exp(-((x - c1)^2) / (2*sigma1^2))     - For c1 &lt;= x &lt;= c2: \u03bc(x) = 1     - For x &gt; c2: \u03bc(x) = exp(-((x - c2)^2) / (2*sigma2^2))</p> <p>Special case (c1 == c2): asymmetric Gaussian centered at c1 with sigma1 on the left side and sigma2 on the right side (no flat region).</p> <p>Parameters:</p> Name Type Description Default <code>sigma1</code> <code>float</code> <p>Standard deviation of the first Gaussian. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>c1</code> <code>float</code> <p>Center of the first Gaussian. Defaults to 0.0.</p> <code>0.0</code> <code>sigma2</code> <code>float</code> <p>Standard deviation of the second Gaussian. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>c2</code> <code>float</code> <p>Center of the second Gaussian. Must satisfy c1 &lt;= c2. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sigma1 or sigma2 are not positive.</p> <code>ValueError</code> <p>If c1 &gt; c2.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the parameters 'sigma1', 'c1', 'sigma2', 'c2'.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing the gradients for each parameter, initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, sigma1: float = 1.0, c1: float = 0.0, sigma2: float = 1.0, c2: float = 0.0):\n    \"\"\"Initialize the membership function with two Gaussian components.\n\n    Args:\n        sigma1 (float, optional): Standard deviation of the first Gaussian. Must be positive. Defaults to 1.0.\n        c1 (float, optional): Center of the first Gaussian. Defaults to 0.0.\n        sigma2 (float, optional): Standard deviation of the second Gaussian. Must be positive. Defaults to 1.0.\n        c2 (float, optional): Center of the second Gaussian. Must satisfy c1 &lt;= c2. Defaults to 0.0.\n\n    Raises:\n        ValueError: If sigma1 or sigma2 are not positive.\n        ValueError: If c1 &gt; c2.\n\n    Attributes:\n        parameters (dict): Dictionary containing the parameters 'sigma1', 'c1', 'sigma2', 'c2'.\n        gradients (dict): Dictionary containing the gradients for each parameter, initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if sigma1 &lt;= 0:\n        raise ValueError(f\"Parameter 'sigma1' must be positive, got sigma1={sigma1}\")\n    if sigma2 &lt;= 0:\n        raise ValueError(f\"Parameter 'sigma2' must be positive, got sigma2={sigma2}\")\n    if c1 &gt; c2:\n        raise ValueError(f\"Parameters must satisfy c1 &lt;= c2, got c1={c1}, c2={c2}\")\n\n    self.parameters = {\"sigma1\": float(sigma1), \"c1\": float(c1), \"sigma2\": float(sigma2), \"c2\": float(c2)}\n    self.gradients = {\"sigma1\": 0.0, \"c1\": 0.0, \"sigma2\": 0.0, \"c2\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate parameter gradients for the two-sided Gaussian.</p> <p>The flat middle region contributes no gradients.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Upstream gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate parameter gradients for the two-sided Gaussian.\n\n    The flat middle region contributes no gradients.\n\n    Args:\n        dL_dy: Upstream gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    s1 = self.parameters[\"sigma1\"]\n    c1 = self.parameters[\"c1\"]\n    s2 = self.parameters[\"sigma2\"]\n    c2 = self.parameters[\"c2\"]\n\n    # Regions\n    left_mask = x &lt; c1\n    mid_mask = (x &gt;= c1) &amp; (x &lt;= c2)\n    right_mask = x &gt; c2\n\n    # Left Gaussian tail contributions (treat like a GaussianMF on that region)\n    if np.any(left_mask):\n        xl = x[left_mask]\n        yl = np.exp(-((xl - c1) ** 2) / (2.0 * s1 * s1))\n        z1 = (xl - c1) / s1\n        # Match GaussianMF derivative conventions\n        dmu_dc1 = yl * z1 / s1\n        dmu_dsigma1 = yl * (z1**2) / s1\n\n        dL_dc1 = np.sum(dL_dy[left_mask] * dmu_dc1)\n        dL_dsigma1 = np.sum(dL_dy[left_mask] * dmu_dsigma1)\n\n        self.gradients[\"c1\"] += float(dL_dc1)\n        self.gradients[\"sigma1\"] += float(dL_dsigma1)\n\n    # Mid region (flat) contributes no gradients\n    _ = mid_mask  # placeholder to document intentional no-op\n\n    # Right Gaussian tail contributions\n    if np.any(right_mask):\n        xr = x[right_mask]\n        yr = np.exp(-((xr - c2) ** 2) / (2.0 * s2 * s2))\n        z2 = (xr - c2) / s2\n        dmu_dc2 = yr * z2 / s2\n        dmu_dsigma2 = yr * (z2**2) / s2\n\n        dL_dc2 = np.sum(dL_dy[right_mask] * dmu_dc2)\n        dL_dsigma2 = np.sum(dL_dy[right_mask] * dmu_dsigma2)\n\n        self.gradients[\"c2\"] += float(dL_dc2)\n        self.gradients[\"sigma2\"] += float(dL_dsigma2)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.Gaussian2MF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute two-sided Gaussian membership values.</p> <p>The input space is divided by c1 and c2 into: - x &lt; c1: left Gaussian tail with sigma1 centered at c1 - c1 &lt;= x &lt;= c2: flat region (1.0) - x &gt; c2: right Gaussian tail with sigma2 centered at c2</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership degrees for each input value.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute two-sided Gaussian membership values.\n\n    The input space is divided by c1 and c2 into:\n    - x &lt; c1: left Gaussian tail with sigma1 centered at c1\n    - c1 &lt;= x &lt;= c2: flat region (1.0)\n    - x &gt; c2: right Gaussian tail with sigma2 centered at c2\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Membership degrees for each input value.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n\n    s1 = self.parameters[\"sigma1\"]\n    c1 = self.parameters[\"c1\"]\n    s2 = self.parameters[\"sigma2\"]\n    c2 = self.parameters[\"c2\"]\n\n    y = np.zeros_like(x, dtype=float)\n\n    # Regions\n    left_mask = x &lt; c1\n    mid_mask = (x &gt;= c1) &amp; (x &lt;= c2)\n    right_mask = x &gt; c2\n\n    if np.any(left_mask):\n        xl = x[left_mask]\n        y[left_mask] = np.exp(-((xl - c1) ** 2) / (2.0 * s1 * s1))\n\n    if np.any(mid_mask):\n        y[mid_mask] = 1.0\n\n    if np.any(right_mask):\n        xr = x[right_mask]\n        y[right_mask] = np.exp(-((xr - c2) ** 2) / (2.0 * s2 * s2))\n\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF","title":"anfis_toolbox.membership.BellMF","text":"<pre><code>BellMF(a: float = 1.0, b: float = 2.0, c: float = 0.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Bell-shaped (Generalized Bell) Membership Function.</p> <p>Implements a bell-shaped membership function using the formula: \u03bc(x) = 1 / (1 + |((x - c) / a)|^(2b))</p> <p>This function is a generalization of the Gaussian function and provides more flexibility in controlling the shape through the 'b' parameter. It's particularly useful when you need asymmetric membership functions or want to fine-tune the slope characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Width parameter (positive). Controls the width of the curve.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Slope parameter (positive). Controls the steepness of the curve.</p> <code>2.0</code> <code>c</code> <code>float</code> <p>Center parameter. Controls the center position of the curve.</p> <code>0.0</code> Note <p>Parameters 'a' and 'b' must be positive for a valid bell function.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Width parameter (must be positive). Defaults to 1.0.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Slope parameter (must be positive). Defaults to 2.0.</p> <code>2.0</code> <code>c</code> <code>float</code> <p>Center parameter. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' or 'b' are not positive.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float = 1.0, b: float = 2.0, c: float = 0.0):\n    \"\"\"Initialize with width, slope, and center parameters.\n\n    Args:\n        a: Width parameter (must be positive). Defaults to 1.0.\n        b: Slope parameter (must be positive). Defaults to 2.0.\n        c: Center parameter. Defaults to 0.0.\n\n    Raises:\n        ValueError: If 'a' or 'b' are not positive.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if a &lt;= 0:\n        raise ValueError(f\"Parameter 'a' must be positive, got a={a}\")\n\n    if b &lt;= 0:\n        raise ValueError(f\"Parameter 'b' must be positive, got b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients given upstream gradient.</p> <p>Analytical gradients: - \u2202\u03bc/\u2202a: width - \u2202\u03bc/\u2202b: steepness - \u2202\u03bc/\u2202c: center</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients given upstream gradient.\n\n    Analytical gradients:\n    - \u2202\u03bc/\u2202a: width\n    - \u2202\u03bc/\u2202b: steepness\n    - \u2202\u03bc/\u2202c: center\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n\n    x = self.last_input\n    y = self.last_output  # This is \u03bc(x)\n\n    # Intermediate calculations\n    normalized = (x - c) / a\n    abs_normalized = np.abs(normalized)\n\n    # Avoid division by zero and numerical issues\n    # Only compute gradients where abs_normalized &gt; epsilon\n    epsilon = 1e-12\n    valid_mask = abs_normalized &gt; epsilon\n\n    if not np.any(valid_mask):\n        # If all values are at the peak (x \u2248 c), gradients are zero\n        return\n\n    # Initialize gradients\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n\n    # Only compute where we have valid values\n    x_valid = x[valid_mask]\n    y_valid = y[valid_mask]\n    dL_dy_valid = dL_dy[valid_mask]\n    normalized_valid = (x_valid - c) / a\n    abs_normalized_valid = np.abs(normalized_valid)\n\n    # Power term: |normalized|^(2b)\n    power_term_valid = np.power(abs_normalized_valid, 2 * b)\n\n    # For the bell function \u03bc = 1/(1 + z) where z = |normalized|^(2b)\n    # \u2202\u03bc/\u2202z = -1/(1 + z)\u00b2 = -\u03bc\u00b2\n    dmu_dz = -y_valid * y_valid\n\n    # Chain rule: \u2202L/\u2202param = \u2202L/\u2202\u03bc \u00d7 \u2202\u03bc/\u2202z \u00d7 \u2202z/\u2202param\n\n    # \u2202z/\u2202a = \u2202(|normalized|^(2b))/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 \u2202|normalized|/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 \u2202normalized/\u2202a\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (-(x-c)/a\u00b2)\n    # = -2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (x-c)/a\u00b2\n\n    sign_normalized = np.sign(normalized_valid)\n    dz_da = -2 * b * np.power(abs_normalized_valid, 2 * b - 1) * sign_normalized * (x_valid - c) / (a * a)\n    dL_da += np.sum(dL_dy_valid * dmu_dz * dz_da)\n\n    # \u2202z/\u2202b = \u2202(|normalized|^(2b))/\u2202b\n    # = |normalized|^(2b) \u00d7 ln(|normalized|) \u00d7 2\n    # But ln(|normalized|) can be problematic near zero, so we use a safe version\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        ln_abs_normalized = np.log(abs_normalized_valid)\n        ln_abs_normalized = np.where(np.isfinite(ln_abs_normalized), ln_abs_normalized, 0.0)\n\n    dz_db = 2 * power_term_valid * ln_abs_normalized\n    dL_db += np.sum(dL_dy_valid * dmu_dz * dz_db)\n\n    # \u2202z/\u2202c = \u2202(|normalized|^(2b))/\u2202c\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 \u2202normalized/\u2202c\n    # = 2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) \u00d7 (-1/a)\n    # = -2b \u00d7 |normalized|^(2b-1) \u00d7 sign(normalized) / a\n\n    dz_dc = -2 * b * np.power(abs_normalized_valid, 2 * b - 1) * sign_normalized / a\n    dL_dc += np.sum(dL_dy_valid * dmu_dz * dz_dc)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.BellMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute bell membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of bell membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute bell membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of bell membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n\n    self.last_input = x\n\n    # Compute the bell function: \u03bc(x) = 1 / (1 + |((x - c) / a)|^(2b))\n    # To avoid numerical issues, we use the absolute value and handle edge cases\n\n    # Compute (x - c) / a\n    normalized = (x - c) / a\n\n    # Compute |normalized|^(2b)\n    # Use np.abs to handle negative values properly\n    abs_normalized = np.abs(normalized)\n\n    # Handle the case where abs_normalized is very close to zero\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        power_term = np.power(abs_normalized, 2 * b)\n        # Replace any inf or nan with a very large number to make output close to 0\n        power_term = np.where(np.isfinite(power_term), power_term, 1e10)\n\n    # Compute the final result\n    output = 1.0 / (1.0 + power_term)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF","title":"anfis_toolbox.membership.SigmoidalMF","text":"<pre><code>SigmoidalMF(a: float = 1.0, c: float = 0.0)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Sigmoidal Membership Function.</p> <p>Implements a sigmoidal (S-shaped) membership function using the formula: \u03bc(x) = 1 / (1 + exp(-a(x - c)))</p> <p>This function provides a smooth S-shaped curve that transitions from 0 to 1. It's particularly useful for modeling gradual transitions and is commonly used in neural networks and fuzzy systems.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Slope parameter. Controls the steepness of the sigmoid.        - Positive values: standard sigmoid (0 \u2192 1 as x increases)        - Negative values: inverted sigmoid (1 \u2192 0 as x increases)        - Larger |a|: steeper transition</p> <code>1.0</code> <code>c</code> <code>float</code> <p>Center parameter. Controls the inflection point where \u03bc\u00a9 = 0.5.</p> <code>0.0</code> Note <p>Parameter 'a' cannot be zero (would result in constant function).</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Slope parameter (cannot be zero). Defaults to 1.0.</p> <code>1.0</code> <code>c</code> <code>float</code> <p>Center parameter (inflection point). Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is zero.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float = 1.0, c: float = 0.0):\n    \"\"\"Initialize the sigmoidal membership function.\n\n    Args:\n        a: Slope parameter (cannot be zero). Defaults to 1.0.\n        c: Center parameter (inflection point). Defaults to 0.0.\n\n    Raises:\n        ValueError: If 'a' is zero.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if a == 0:\n        raise ValueError(f\"Parameter 'a' cannot be zero, got a={a}\")\n\n    self.parameters = {\"a\": float(a), \"c\": float(c)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients given upstream gradient.</p> <p>For \u03bc(x) = 1/(1 + exp(-a(x-c))): - \u2202\u03bc/\u2202a = \u03bc(x)(1-\u03bc(x))(x-c) - \u2202\u03bc/\u2202c = -a\u03bc(x)(1-\u03bc(x))</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients given upstream gradient.\n\n    For \u03bc(x) = 1/(1 + exp(-a(x-c))):\n    - \u2202\u03bc/\u2202a = \u03bc(x)(1-\u03bc(x))(x-c)\n    - \u2202\u03bc/\u2202c = -a\u03bc(x)(1-\u03bc(x))\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    a = self.parameters[\"a\"]\n    c = self.parameters[\"c\"]\n\n    x = self.last_input\n    y = self.last_output  # This is \u03bc(x)\n\n    # For sigmoid: \u2202\u03bc/\u2202z = \u03bc(1-\u03bc) where z = -a(x-c)\n    # This is a fundamental property of the sigmoid function\n    dmu_dz = y * (1.0 - y)\n\n    # Chain rule: \u2202L/\u2202param = \u2202L/\u2202\u03bc \u00d7 \u2202\u03bc/\u2202z \u00d7 \u2202z/\u2202param\n\n    # For z = a(x-c):\n    # \u2202z/\u2202a = (x-c)\n    # \u2202z/\u2202c = -a\n\n    # Gradient w.r.t. 'a'\n    dz_da = x - c\n    dL_da = np.sum(dL_dy * dmu_dz * dz_da)\n\n    # Gradient w.r.t. 'c'\n    dz_dc = -a\n    dL_dc = np.sum(dL_dy * dmu_dz * dz_dc)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute sigmoidal membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array for which the membership values are computed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of sigmoidal membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute sigmoidal membership values.\n\n    Args:\n        x: Input array for which the membership values are computed.\n\n    Returns:\n        np.ndarray: Array of sigmoidal membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    c = self.parameters[\"c\"]\n\n    self.last_input = x\n\n    # Compute the sigmoid function: \u03bc(x) = 1 / (1 + exp(-a(x - c)))\n    # To avoid numerical overflow, we use a stable implementation\n\n    # Compute a(x - c) (note: not -a(x - c))\n    z = a * (x - c)\n\n    # Use stable sigmoid implementation to avoid overflow\n    # Standard sigmoid: \u03c3(z) = 1 / (1 + exp(-z))\n    # For numerical stability:\n    # If z &gt;= 0: \u03c3(z) = 1 / (1 + exp(-z))\n    # If z &lt; 0: \u03c3(z) = exp(z) / (1 + exp(z))\n\n    output = np.zeros_like(x, dtype=float)\n\n    # Case 1: z &gt;= 0 (standard case)\n    mask_pos = z &gt;= 0\n    if np.any(mask_pos):\n        output[mask_pos] = 1.0 / (1.0 + np.exp(-z[mask_pos]))\n\n    # Case 2: z &lt; 0 (to avoid exp overflow)\n    mask_neg = z &lt; 0\n    if np.any(mask_neg):\n        exp_z = np.exp(z[mask_neg])\n        output[mask_neg] = exp_z / (1.0 + exp_z)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF","title":"anfis_toolbox.membership.DiffSigmoidalMF","text":"<pre><code>DiffSigmoidalMF(a1: float, c1: float, a2: float, c2: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Difference of two sigmoidal functions.</p> <p>Implements y = s1(x) - s2(x), where each s is a logistic curve with its own slope and center parameters.</p> <p>Parameters:</p> Name Type Description Default <code>a1</code> <code>float</code> <p>The first 'a' parameter for the membership function.</p> required <code>c1</code> <code>float</code> <p>The first 'c' parameter for the membership function.</p> required <code>a2</code> <code>float</code> <p>The second 'a' parameter for the membership function.</p> required <code>c2</code> <code>float</code> <p>The second 'c' parameter for the membership function.</p> required <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the membership function parameters.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for each parameter, initialized to 0.0.</p> <code>last_input</code> <code>dict</code> <p>Stores the last input value (initially None).</p> <code>last_output</code> <code>dict</code> <p>Stores the last output value (initially None).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a1: float, c1: float, a2: float, c2: float):\n    \"\"\"Initializes the membership function with two sets of parameters.\n\n    Args:\n        a1 (float): The first 'a' parameter for the membership function.\n        c1 (float): The first 'c' parameter for the membership function.\n        a2 (float): The second 'a' parameter for the membership function.\n        c2 (float): The second 'c' parameter for the membership function.\n\n    Attributes:\n        parameters (dict): Dictionary containing the membership function parameters.\n        gradients (dict): Dictionary containing gradients for each parameter, initialized to 0.0.\n        last_input: Stores the last input value (initially None).\n        last_output: Stores the last output value (initially None).\n    \"\"\"\n    super().__init__()\n    self.parameters = {\n        \"a1\": float(a1),\n        \"c1\": float(c1),\n        \"a2\": float(a2),\n        \"c2\": float(c2),\n    }\n    self.gradients = dict.fromkeys(self.parameters, 0.0)\n    self.last_input = None\n    self.last_output = None\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients w.r.t. parameters and optionally input.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>np.ndarray | None: Gradient of the loss w.r.t. the input, if available.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients w.r.t. parameters and optionally input.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        np.ndarray | None: Gradient of the loss w.r.t. the input, if available.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n    s1, s2 = self._s1, self._s2\n\n    # Sigmoid derivatives\n    ds1_da1 = (x - c1) * s1 * (1 - s1)\n    ds1_dc1 = -a1 * s1 * (1 - s1)\n    ds2_da2 = (x - c2) * s2 * (1 - s2)\n    ds2_dc2 = -a2 * s2 * (1 - s2)\n\n    # Parameter gradients\n    self.gradients[\"a1\"] += float(np.sum(dL_dy * ds1_da1))\n    self.gradients[\"c1\"] += float(np.sum(dL_dy * ds1_dc1))\n    self.gradients[\"a2\"] += float(np.sum(dL_dy * -ds2_da2))\n    self.gradients[\"c2\"] += float(np.sum(dL_dy * -ds2_dc2))\n\n    # Gradient w.r.t. input (optional, for chaining)\n    dmu_dx = a1 * s1 * (1 - s1) - a2 * s2 * (1 - s2)\n    return dL_dy * dmu_dx\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.DiffSigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute y = s1(x) - s2(x).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values for the input.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute y = s1(x) - s2(x).\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Membership values for the input.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n\n    s1 = 1.0 / (1.0 + np.exp(-a1 * (x - c1)))\n    s2 = 1.0 / (1.0 + np.exp(-a2 * (x - c2)))\n    y = s1 - s2\n\n    self.last_output = y\n    self._s1, self._s2 = s1, s2  # store for backward\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF","title":"anfis_toolbox.membership.ProdSigmoidalMF","text":"<pre><code>ProdSigmoidalMF(a1: float, c1: float, a2: float, c2: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Product of two sigmoidal functions.</p> <p>Implements \u03bc(x) = s1(x) * s2(x) with separate parameters for each sigmoid.</p> <p>Parameters:</p> Name Type Description Default <code>a1</code> <code>float</code> <p>The first parameter for the membership function.</p> required <code>c1</code> <code>float</code> <p>The second parameter for the membership function.</p> required <code>a2</code> <code>float</code> <p>The third parameter for the membership function.</p> required <code>c2</code> <code>float</code> <p>The fourth parameter for the membership function.</p> required <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing the membership function parameters.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for each parameter, initialized to 0.0.</p> <code>last_input</code> <code>dict</code> <p>Stores the last input value (initialized to None).</p> <code>last_output</code> <code>dict</code> <p>Stores the last output value (initialized to None).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a1: float, c1: float, a2: float, c2: float):\n    \"\"\"Initializes the membership function with specified parameters.\n\n    Args:\n        a1 (float): The first parameter for the membership function.\n        c1 (float): The second parameter for the membership function.\n        a2 (float): The third parameter for the membership function.\n        c2 (float): The fourth parameter for the membership function.\n\n    Attributes:\n        parameters (dict): Dictionary containing the membership function parameters.\n        gradients (dict): Dictionary containing gradients for each parameter, initialized to 0.0.\n        last_input: Stores the last input value (initialized to None).\n        last_output: Stores the last output value (initialized to None).\n    \"\"\"\n    super().__init__()\n    self.parameters = {\n        \"a1\": float(a1),\n        \"c1\": float(c1),\n        \"a2\": float(a2),\n        \"c2\": float(c2),\n    }\n    self.gradients = dict.fromkeys(self.parameters, 0.0)\n    self.last_input = None\n    self.last_output = None\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute parameter gradients and optionally return input gradient.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>np.ndarray | None: Gradient of the loss w.r.t. the input, if available.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute parameter gradients and optionally return input gradient.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        np.ndarray | None: Gradient of the loss w.r.t. the input, if available.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n    s1, s2 = self._s1, self._s2\n\n    # derivatives of sigmoids w.r.t. parameters\n    ds1_da1 = (x - c1) * s1 * (1 - s1)\n    ds1_dc1 = -a1 * s1 * (1 - s1)\n    ds2_da2 = (x - c2) * s2 * (1 - s2)\n    ds2_dc2 = -a2 * s2 * (1 - s2)\n\n    # parameter gradients using product rule\n    self.gradients[\"a1\"] += float(np.sum(dL_dy * ds1_da1 * s2))\n    self.gradients[\"c1\"] += float(np.sum(dL_dy * ds1_dc1 * s2))\n    self.gradients[\"a2\"] += float(np.sum(dL_dy * s1 * ds2_da2))\n    self.gradients[\"c2\"] += float(np.sum(dL_dy * s1 * ds2_dc2))\n\n    # gradient w.r.t. input (optional)\n    dmu_dx = a1 * s1 * (1 - s1) * s2 + a2 * s2 * (1 - s2) * s1\n    return dL_dy * dmu_dx\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ProdSigmoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes the membership value(s) for input x using the product of two sigmoidal functions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array to the membership function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array after applying the membership function.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the membership value(s) for input x using the product of two sigmoidal functions.\n\n    Args:\n        x (np.ndarray): Input array to the membership function.\n\n    Returns:\n        np.ndarray: Output array after applying the membership function.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a1, c1 = self.parameters[\"a1\"], self.parameters[\"c1\"]\n    a2, c2 = self.parameters[\"a2\"], self.parameters[\"c2\"]\n\n    s1 = 1.0 / (1.0 + np.exp(-a1 * (x - c1)))\n    s2 = 1.0 / (1.0 + np.exp(-a2 * (x - c2)))\n    y = s1 * s2\n\n    self.last_output = y\n    self._s1, self._s2 = s1, s2  # store for backward\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF","title":"anfis_toolbox.membership.SShapedMF","text":"<pre><code>SShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>S-shaped Membership Function.</p> <p>Smoothly transitions from 0 to 1 between two parameters a and b using the smoothstep polynomial S(t) = 3t\u00b2 - 2t\u00b3. Commonly used in fuzzy logic for gradual onset of membership.</p> <p>Definition with a &lt; b: - \u03bc(x) = 0, for x \u2264 a - \u03bc(x) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a), for a &lt; x &lt; b - \u03bc(x) = 1, for x \u2265 b</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot (start of transition from 0).</p> required <code>b</code> <code>float</code> <p>Right shoulder (end of transition at 1).</p> required Note <p>Requires a &lt; b.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter, must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter, must be greater than 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter, must be less than 'b'.\n        b (float): The second parameter, must be greater than 'a'.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a and b using analytical derivatives.</p> <p>Uses S(t) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a) on the transition region.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a and b using analytical derivatives.\n\n    Uses S(t) = 3t\u00b2 - 2t\u00b3, t = (x-a)/(b-a) on the transition region.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    # Only transition region contributes to parameter gradients\n    mask = (x &gt;= a) &amp; (x &lt;= b)\n    if not (np.any(mask) and b != a):\n        return\n\n    x_t = x[mask]\n    dL_dy_t = dL_dy[mask]\n    t = (x_t - a) / (b - a)\n\n    # dS/dt = 6*t*(1-t)\n    dS_dt = _dsmoothstep_dt(t)\n\n    # dt/da and dt/db\n    dt_da = (x_t - b) / (b - a) ** 2\n    dt_db = -(x_t - a) / (b - a) ** 2\n\n    dS_da = dS_dt * dt_da\n    dS_db = dS_dt * dt_db\n\n    self.gradients[\"a\"] += float(np.sum(dL_dy_t * dS_da))\n    self.gradients[\"b\"] += float(np.sum(dL_dy_t * dS_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.SShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute S-shaped membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute S-shaped membership values.\"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # Right side (x \u2265 b): \u03bc = 1\n    mask_right = x &gt;= b\n    y[mask_right] = 1.0\n\n    # Transition region (a &lt; x &lt; b): \u03bc = smoothstep(t)\n    mask_trans = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_trans):\n        x_t = x[mask_trans]\n        t = (x_t - a) / (b - a)\n        y[mask_trans] = _smoothstep(t)\n\n    # Left side (x \u2264 a) remains 0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF","title":"anfis_toolbox.membership.LinSShapedMF","text":"<pre><code>LinSShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Linear S-shaped saturation Membership Function.</p> Piecewise linear ramp from 0 to 1 between parameters a and b <ul> <li>\u03bc(x) = 0, for x \u2264 a</li> <li>\u03bc(x) = (x - a) / (b - a), for a &lt; x &lt; b</li> <li>\u03bc(x) = 1, for x \u2265 b</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot (start of transition from 0).</p> required <code>b</code> <code>float</code> <p>Right shoulder (end of transition at 1). Requires a &lt; b.</p> required <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter, must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter, must be greater than 'a'.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter, must be less than 'b'.\n        b (float): The second parameter, must be greater than 'a'.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for 'a' and 'b' in the ramp region.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for 'a' and 'b' in the ramp region.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    d = b - a\n    if d == 0:\n        return\n    # Only ramp region contributes to parameter gradients\n    mask = (x &gt; a) &amp; (x &lt; b)\n    if not np.any(mask):\n        return\n    xm = x[mask]\n    g = dL_dy[mask]\n    # \u03bc = (x-a)/d with d = b-a\n    # \u2202\u03bc/\u2202a = -(1/d) + (x-a)/d^2\n    dmu_da = -(1.0 / d) + (xm - a) / (d * d)\n    # \u2202\u03bc/\u2202b = -(x-a)/d^2\n    dmu_db = -((xm - a) / (d * d))\n    self.gradients[\"a\"] += float(np.sum(g * dmu_da))\n    self.gradients[\"b\"] += float(np.sum(g * dmu_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinSShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute linear S-shaped membership values for x.</p> <p>The rules based on a and b: - x &gt;= b: 1.0 (right saturated) - a &lt; x &lt; b: linear ramp from 0 to 1 - x &lt;= a: 0.0 (left)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output array with membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute linear S-shaped membership values for x.\n\n    The rules based on a and b:\n    - x &gt;= b: 1.0 (right saturated)\n    - a &lt; x &lt; b: linear ramp from 0 to 1\n    - x &lt;= a: 0.0 (left)\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Output array with membership values.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    y = np.zeros_like(x, dtype=float)\n    # right saturated region\n    mask_right = x &gt;= b\n    y[mask_right] = 1.0\n    # linear ramp\n    mask_mid = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_mid):\n        y[mask_mid] = (x[mask_mid] - a) / (b - a)\n    # left stays 0\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF","title":"anfis_toolbox.membership.ZShapedMF","text":"<pre><code>ZShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Z-shaped Membership Function.</p> <p>Smoothly transitions from 1 to 0 between two parameters a and b using the smoothstep polynomial S(t) = 3t\u00b2 - 2t\u00b3 (Z = 1 - S). Commonly used in fuzzy logic as the complement of the S-shaped function.</p> <p>Definition with a &lt; b: - \u03bc(x) = 1, for x \u2264 a - \u03bc(x) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a), for a &lt; x &lt; b - \u03bc(x) = 0, for x \u2265 b</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left shoulder (start of transition).</p> required <code>b</code> <code>float</code> <p>Right foot (end of transition).</p> required Note <p>Requires a &lt; b. In the degenerate case a == b, the function becomes an instantaneous drop at x=a.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Lower bound parameter.</p> required <code>b</code> <code>float</code> <p>Upper bound parameter.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a is not less than b.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters a and b.\n\n    Args:\n        a: Lower bound parameter.\n        b: Upper bound parameter.\n\n    Raises:\n        ValueError: If a is not less than b.\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a and b using analytical derivatives.</p> <p>Uses Z(t) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a) on the transition region.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a and b using analytical derivatives.\n\n    Uses Z(t) = 1 - (3t\u00b2 - 2t\u00b3), t = (x-a)/(b-a) on the transition region.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    # Only transition region contributes to parameter gradients\n    mask = (x &gt;= a) &amp; (x &lt;= b)\n    if not (np.any(mask) and b != a):\n        return\n\n    x_t = x[mask]\n    dL_dy_t = dL_dy[mask]\n    t = (x_t - a) / (b - a)\n\n    # dZ/dt = -dS/dt = 6*t*(t-1)\n    dZ_dt = -_dsmoothstep_dt(t)\n\n    # dt/da and dt/db\n    dt_da = (x_t - b) / (b - a) ** 2\n    dt_db = -(x_t - a) / (b - a) ** 2\n\n    dZ_da = dZ_dt * dt_da\n    dZ_db = dZ_dt * dt_db\n\n    self.gradients[\"a\"] += float(np.sum(dL_dy_t * dZ_da))\n    self.gradients[\"b\"] += float(np.sum(dL_dy_t * dZ_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.ZShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute Z-shaped membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Z-shaped membership values.\"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # Left side (x \u2264 a): \u03bc = 1\n    mask_left = x &lt;= a\n    y[mask_left] = 1.0\n\n    # Transition region (a &lt; x &lt; b): \u03bc = 1 - smoothstep(t)\n    mask_trans = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_trans):\n        x_t = x[mask_trans]\n        t = (x_t - a) / (b - a)\n        y[mask_trans] = 1.0 - _smoothstep(t)\n\n    # Right side (x \u2265 b) remains 0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF","title":"anfis_toolbox.membership.LinZShapedMF","text":"<pre><code>LinZShapedMF(a: float, b: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Linear Z-shaped saturation Membership Function.</p> Piecewise linear ramp from 1 to 0 between parameters a and b <ul> <li>\u03bc(x) = 1, for x \u2264 a</li> <li>\u03bc(x) = (b - x) / (b - a), for a &lt; x &lt; b</li> <li>\u03bc(x) = 0, for x \u2265 b</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left shoulder (end of saturation at 1).</p> required <code>b</code> <code>float</code> <p>Right foot (end of transition to 0). Requires a &lt; b.</p> required <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The first parameter of the membership function. Must be less than 'b'.</p> required <code>b</code> <code>float</code> <p>The second parameter of the membership function.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'a' is not less than 'b'.</p> <p>Attributes:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>Dictionary containing 'a' and 'b' as floats.</p> <code>gradients</code> <code>dict</code> <p>Dictionary containing gradients for 'a' and 'b', initialized to 0.0.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float):\n    \"\"\"Initialize the membership function with parameters 'a' and 'b'.\n\n    Args:\n        a (float): The first parameter of the membership function. Must be less than 'b'.\n        b (float): The second parameter of the membership function.\n\n    Raises:\n        ValueError: If 'a' is not less than 'b'.\n\n    Attributes:\n        parameters (dict): Dictionary containing 'a' and 'b' as floats.\n        gradients (dict): Dictionary containing gradients for 'a' and 'b', initialized to 0.0.\n    \"\"\"\n    super().__init__()\n    if not (a &lt; b):\n        raise ValueError(f\"Parameters must satisfy a &lt; b, got a={a}, b={b}\")\n    self.parameters = {\"a\": float(a), \"b\": float(b)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for 'a' and 'b'.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for 'a' and 'b'.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    d = b - a\n    if d == 0:\n        return\n    mask = (x &gt; a) &amp; (x &lt; b)\n    if not np.any(mask):\n        return\n    xm = x[mask]\n    g = dL_dy[mask]\n    # \u03bc = (b-x)/(b-a)\n    # \u2202\u03bc/\u2202a = (b-x)/(d^2)\n    # \u2202\u03bc/\u2202b = (x-a)/(d^2)\n    dmu_da = (b - xm) / (d * d)\n    dmu_db = (xm - a) / (d * d)\n    self.gradients[\"a\"] += float(np.sum(g * dmu_da))\n    self.gradients[\"b\"] += float(np.sum(g * dmu_db))\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.LinZShapedMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute linear Z-shaped membership values for x.</p> <p>Rules: - x &lt;= a: 1.0 (left saturated) - a &lt; x &lt; b: linear ramp from 1 to 0 - x &gt;= b: 0.0 (right)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Output membership values for each input.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute linear Z-shaped membership values for x.\n\n    Rules:\n    - x &lt;= a: 1.0 (left saturated)\n    - a &lt; x &lt; b: linear ramp from 1 to 0\n    - x &gt;= b: 0.0 (right)\n\n    Args:\n        x: Input array of values.\n\n    Returns:\n        np.ndarray: Output membership values for each input.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    self.last_input = x\n    a, b = self.parameters[\"a\"], self.parameters[\"b\"]\n    y = np.zeros_like(x, dtype=float)\n\n    # left saturated region\n    mask_left = x &lt;= a\n    y[mask_left] = 1.0\n    # linear ramp\n    mask_mid = (x &gt; a) &amp; (x &lt; b)\n    if np.any(mask_mid):\n        y[mask_mid] = (b - x[mask_mid]) / (b - a)\n    # right stays 0\n    self.last_output = y\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF","title":"anfis_toolbox.membership.PiMF","text":"<pre><code>PiMF(a: float, b: float, c: float, d: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Pi-shaped membership function.</p> <p>The Pi-shaped membership function is characterized by a trapezoidal-like shape with smooth S-shaped transitions on both sides. It is defined by four parameters that control the shape and position:</p> <p>Mathematical definition: \u03bc(x) = S(x; a, b) for x \u2208 [a, b]      = 1 for x \u2208 [b, c]      = Z(x; c, d) for x \u2208 [c, d]      = 0 elsewhere</p> <p>Where: - S(x; a, b) is an S-shaped function from 0 to 1 - Z(x; c, d) is a Z-shaped function from 1 to 0</p> <p>The S and Z functions use smooth cubic splines for differentiability: S(x; a, b) = 2*((x-a)/(b-a))^3 for x \u2208 [a, (a+b)/2]            = 1 - 2*((b-x)/(b-a))^3 for x \u2208 [(a+b)/2, b]</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot of the function (where function starts rising from 0)</p> required <code>b</code> <code>float</code> <p>Left shoulder of the function (where function reaches 1)</p> required <code>c</code> <code>float</code> <p>Right shoulder of the function (where function starts falling from 1)</p> required <code>d</code> <code>float</code> <p>Right foot of the function (where function reaches 0)</p> required Note <p>Parameters must satisfy: a &lt; b \u2264 c &lt; d</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left foot parameter.</p> required <code>b</code> <code>float</code> <p>Left shoulder parameter.</p> required <code>c</code> <code>float</code> <p>Right shoulder parameter.</p> required <code>d</code> <code>float</code> <p>Right foot parameter.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters don't satisfy a &lt; b \u2264 c &lt; d.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float, d: float):\n    \"\"\"Initialize the Pi-shaped membership function.\n\n    Args:\n        a: Left foot parameter.\n        b: Left shoulder parameter.\n        c: Right shoulder parameter.\n        d: Right foot parameter.\n\n    Raises:\n        ValueError: If parameters don't satisfy a &lt; b \u2264 c &lt; d.\n    \"\"\"\n    super().__init__()\n\n    # Parameter validation\n    if not (a &lt; b &lt;= c &lt; d):\n        raise ValueError(f\"Parameters must satisfy a &lt; b \u2264 c &lt; d, got a={a}, b={b}, c={c}, d={d}\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"d\": float(d)}\n    self.gradients = {\"a\": 0.0, \"b\": 0.0, \"c\": 0.0, \"d\": 0.0}\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients for backpropagation.</p> <p>Analytical gradients are computed by region: - S-function: gradients w.r.t. a, b - Z-function: gradients w.r.t. c, d - Flat region: no gradients</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of loss w.r.t. function output.</p> required Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients for backpropagation.\n\n    Analytical gradients are computed by region:\n    - S-function: gradients w.r.t. a, b\n    - Z-function: gradients w.r.t. c, d\n    - Flat region: no gradients\n\n    Args:\n        dL_dy: Gradient of loss w.r.t. function output.\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    x = self.last_input\n    dL_dy = np.asarray(dL_dy)\n\n    a, b, c, d = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"], self.parameters[\"d\"]\n\n    # Initialize gradients\n    grad_a = grad_b = grad_c = grad_d = 0.0\n\n    # S-function gradients [a, b]\n    mask_s = (x &gt;= a) &amp; (x &lt;= b)\n    if np.any(mask_s) and b != a:\n        x_s = x[mask_s]\n        dL_dy_s = dL_dy[mask_s]\n        t = (x_s - a) / (b - a)\n\n        # Calculate parameter derivatives\n        dt_da = (x_s - b) / (b - a) ** 2  # Correct derivative\n        dt_db = -(x_s - a) / (b - a) ** 2\n\n        # For smoothstep S(t) = 3*t\u00b2 - 2*t\u00b3, derivative is dS/dt = 6*t - 6*t\u00b2 = 6*t*(1-t)\n        dS_dt = _dsmoothstep_dt(t)\n\n        # Apply chain rule: dS/da = dS/dt * dt/da\n        dS_da = dS_dt * dt_da\n        dS_db = dS_dt * dt_db\n\n        grad_a += np.sum(dL_dy_s * dS_da)\n        grad_b += np.sum(dL_dy_s * dS_db)\n\n    # Z-function gradients [c, d]\n    mask_z = (x &gt;= c) &amp; (x &lt;= d)\n    if np.any(mask_z) and d != c:\n        x_z = x[mask_z]\n        dL_dy_z = dL_dy[mask_z]\n        t = (x_z - c) / (d - c)\n\n        # Calculate parameter derivatives\n        dt_dc = (x_z - d) / (d - c) ** 2  # Correct derivative\n        dt_dd = -(x_z - c) / (d - c) ** 2\n\n        # For Z(t) = 1 - S(t) = 1 - (3*t\u00b2 - 2*t\u00b3), derivative is dZ/dt = -dS/dt = -6*t*(1-t) = 6*t*(t-1)\n        dZ_dt = -_dsmoothstep_dt(t)\n\n        # Apply chain rule: dZ/dc = dZ/dt * dt/dc\n        dZ_dc = dZ_dt * dt_dc\n        dZ_dd = dZ_dt * dt_dd\n\n        grad_c += np.sum(dL_dy_z * dZ_dc)\n        grad_d += np.sum(dL_dy_z * dZ_dd)\n\n    # Accumulate gradients\n    self.gradients[\"a\"] += grad_a\n    self.gradients[\"b\"] += grad_b\n    self.gradients[\"c\"] += grad_c\n    self.gradients[\"d\"] += grad_d\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.PiMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the Pi-shaped membership function.</p> <p>Combines S and Z functions for smooth transitions: - Rising edge: S-function from a to b - Flat top: constant 1 from b to c - Falling edge: Z-function from c to d - Outside: 0</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values \u03bc(x) \u2208 [0, 1].</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the Pi-shaped membership function.\n\n    Combines S and Z functions for smooth transitions:\n    - Rising edge: S-function from a to b\n    - Flat top: constant 1 from b to c\n    - Falling edge: Z-function from c to d\n    - Outside: 0\n\n    Args:\n        x: Input values.\n\n    Returns:\n        np.ndarray: Membership values \u03bc(x) \u2208 [0, 1].\n    \"\"\"\n    x = np.asarray(x)\n    self.last_input = x.copy()\n\n    a, b, c, d = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"], self.parameters[\"d\"]\n\n    # Initialize output\n    y = np.zeros_like(x, dtype=np.float64)\n\n    # S-function for rising edge [a, b]\n    mask_s = (x &gt;= a) &amp; (x &lt;= b)\n    if np.any(mask_s):\n        x_s = x[mask_s]\n        # Avoid division by zero\n        if b != a:\n            t = (x_s - a) / (b - a)  # Normalize to [0, 1]\n\n            # Smooth S-function using smoothstep: S(t) = 3*t\u00b2 - 2*t\u00b3\n            # This is continuous and differentiable across the entire [0,1] interval\n            y_s = _smoothstep(t)\n\n            y[mask_s] = y_s\n        else:\n            # Degenerate case: instant transition\n            y[mask_s] = 1.0\n\n    # Flat region [b, c]: \u03bc(x) = 1\n    mask_flat = (x &gt;= b) &amp; (x &lt;= c)\n    y[mask_flat] = 1.0\n\n    # Z-function for falling edge [c, d]\n    mask_z = (x &gt;= c) &amp; (x &lt;= d)\n    if np.any(mask_z):\n        x_z = x[mask_z]\n        # Avoid division by zero\n        if d != c:\n            t = (x_z - c) / (d - c)  # Normalize to [0, 1]\n\n            # Smooth Z-function (inverted smoothstep): Z(t) = 1 - S(t) = 1 - (3*t\u00b2 - 2*t\u00b3)\n            # This is continuous and differentiable, going from 1 to 0\n            y_z = 1 - _smoothstep(t)\n\n            y[mask_z] = y_z\n        else:\n            # Degenerate case: instant transition\n            y[mask_z] = 0.0\n\n    self.last_output = y.copy()\n    return y\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF","title":"anfis_toolbox.membership.TriangularMF","text":"<pre><code>TriangularMF(a: float, b: float, c: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Triangular Membership Function.</p> <p>Implements a triangular membership function using piecewise linear segments: \u03bc(x) = { 0,           x \u2264 a or x \u2265 c        { (x-a)/(b-a), a &lt; x &lt; b        { (c-x)/(c-b), b \u2264 x &lt; c</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point of the triangle.</p> required <code>b</code> <code>float</code> <p>Peak point of the triangle (\u03bc(b) = 1).</p> required <code>c</code> <code>float</code> <p>Right base point of the triangle.</p> required Note <p>Must satisfy: a \u2264 b \u2264 c</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point (must satisfy a \u2264 b).</p> required <code>b</code> <code>float</code> <p>Peak point (must satisfy a \u2264 b \u2264 c).</p> required <code>c</code> <code>float</code> <p>Right base point (must satisfy b \u2264 c).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters do not satisfy a \u2264 b \u2264 c or if a == c (zero width).</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float):\n    \"\"\"Initialize the triangular membership function.\n\n    Args:\n        a: Left base point (must satisfy a \u2264 b).\n        b: Peak point (must satisfy a \u2264 b \u2264 c).\n        c: Right base point (must satisfy b \u2264 c).\n\n    Raises:\n        ValueError: If parameters do not satisfy a \u2264 b \u2264 c or if a == c (zero width).\n    \"\"\"\n    super().__init__()\n\n    if not (a &lt;= b &lt;= c):\n        raise ValueError(f\"Triangular MF parameters must satisfy a \u2264 b \u2264 c, got a={a}, b={b}, c={c}\")\n    if a == c:\n        raise ValueError(\"Parameters 'a' and 'c' cannot be equal (zero width triangle)\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c)}\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Accumulate gradients for a, b, c given upstream gradient.</p> <p>Computes analytical derivatives for the rising (a, b) and falling (b, c) regions and sums them over the batch.</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. \u03bc(x); same shape or broadcastable to output.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Accumulate gradients for a, b, c given upstream gradient.\n\n    Computes analytical derivatives for the rising (a, b) and falling (b, c)\n    regions and sums them over the batch.\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. \u03bc(x); same shape or broadcastable to output.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    a, b, c = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"]\n    x = self.last_input\n\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n\n    # Left slope: a &lt; x &lt; b\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        if np.any(left_mask):\n            x_left = x[left_mask]\n            dL_dy_left = dL_dy[left_mask]\n\n            # \u2202\u03bc/\u2202a = (x - b) / (b - a)^2\n            dmu_da_left = (x_left - b) / ((b - a) ** 2)\n            dL_da += np.sum(dL_dy_left * dmu_da_left)\n\n            # \u2202\u03bc/\u2202b = -(x - a) / (b - a)^2\n            dmu_db_left = -(x_left - a) / ((b - a) ** 2)\n            dL_db += np.sum(dL_dy_left * dmu_db_left)\n\n    # Right slope: b &lt; x &lt; c\n    if c &gt; b:\n        right_mask = (x &gt; b) &amp; (x &lt; c)\n        if np.any(right_mask):\n            x_right = x[right_mask]\n            dL_dy_right = dL_dy[right_mask]\n\n            # \u2202\u03bc/\u2202b = (x - c) / (c - b)^2\n            dmu_db_right = (x_right - c) / ((c - b) ** 2)\n            dL_db += np.sum(dL_dy_right * dmu_db_right)\n\n            # \u2202\u03bc/\u2202c = (x - b) / (c - b)^2\n            dmu_dc_right = (x_right - b) / ((c - b) ** 2)\n            dL_dc += np.sum(dL_dy_right * dmu_dc_right)\n\n    # Update gradients\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TriangularMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute triangular membership values \u03bc(x).</p> <p>Uses piecewise linear segments defined by (a, b, c): - 0 outside [a, c] - rising slope in (a, b) - peak 1 at x == b - falling slope in (b, c)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Membership values in [0, 1] with the same shape as x.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute triangular membership values \u03bc(x).\n\n    Uses piecewise linear segments defined by (a, b, c):\n    - 0 outside [a, c]\n    - rising slope in (a, b)\n    - peak 1 at x == b\n    - falling slope in (b, c)\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Membership values in [0, 1] with the same shape as x.\n    \"\"\"\n    a, b, c = self.parameters[\"a\"], self.parameters[\"b\"], self.parameters[\"c\"]\n    self.last_input = x\n\n    output = np.zeros_like(x, dtype=float)\n\n    # Left slope\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        output[left_mask] = (x[left_mask] - a) / (b - a)\n\n    # Peak\n    peak_mask = x == b\n    output[peak_mask] = 1.0\n\n    # Right slope\n    if c &gt; b:\n        right_mask = (x &gt; b) &amp; (x &lt; c)\n        output[right_mask] = (c - x[right_mask]) / (c - b)\n\n    # Clip for numerical stability\n    output = np.clip(output, 0.0, 1.0)\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF","title":"anfis_toolbox.membership.TrapezoidalMF","text":"<pre><code>TrapezoidalMF(a: float, b: float, c: float, d: float)\n</code></pre> <p>               Bases: <code>MembershipFunction</code></p> <p>Trapezoidal Membership Function.</p> <p>Implements a trapezoidal membership function using piecewise linear segments: \u03bc(x) = { 0,           x \u2264 a or x \u2265 d        { (x-a)/(b-a), a &lt; x &lt; b        { 1,           b \u2264 x \u2264 c        { (d-x)/(d-c), c &lt; x &lt; d</p> <p>This function is commonly used in fuzzy logic systems when you need a plateau region of full membership, providing robustness to noise and uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point of the trapezoid (lower support bound).</p> required <code>b</code> <code>float</code> <p>Left peak point (start of plateau where \u03bc(x) = 1).</p> required <code>c</code> <code>float</code> <p>Right peak point (end of plateau where \u03bc(x) = 1).</p> required <code>d</code> <code>float</code> <p>Right base point of the trapezoid (upper support bound).</p> required Note <p>Parameters must satisfy: a \u2264 b \u2264 c \u2264 d for a valid trapezoidal function.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Left base point (\u03bc(a) = 0).</p> required <code>b</code> <code>float</code> <p>Left peak point (\u03bc(b) = 1, start of plateau).</p> required <code>c</code> <code>float</code> <p>Right peak point (\u03bc\u00a9 = 1, end of plateau).</p> required <code>d</code> <code>float</code> <p>Right base point (\u03bc(d) = 0).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters don't satisfy a \u2264 b \u2264 c \u2264 d.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def __init__(self, a: float, b: float, c: float, d: float):\n    \"\"\"Initialize the trapezoidal membership function.\n\n    Args:\n        a: Left base point (\u03bc(a) = 0).\n        b: Left peak point (\u03bc(b) = 1, start of plateau).\n        c: Right peak point (\u03bc(c) = 1, end of plateau).\n        d: Right base point (\u03bc(d) = 0).\n\n    Raises:\n        ValueError: If parameters don't satisfy a \u2264 b \u2264 c \u2264 d.\n    \"\"\"\n    super().__init__()\n\n    # Validate parameters\n    if not (a &lt;= b &lt;= c &lt;= d):\n        raise ValueError(f\"Trapezoidal MF parameters must satisfy a \u2264 b \u2264 c \u2264 d, got a={a}, b={b}, c={c}, d={d}\")\n\n    if a == d:\n        raise ValueError(\"Parameters 'a' and 'd' cannot be equal (zero width trapezoid)\")\n\n    self.parameters = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"d\": float(d)}\n    # Initialize gradients to zero for all parameters\n    self.gradients = dict.fromkeys(self.parameters.keys(), 0.0)\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF.backward","title":"backward","text":"<pre><code>backward(dL_dy: ndarray)\n</code></pre> <p>Compute gradients for parameters based on upstream loss gradient.</p> <p>Analytical gradients for the piecewise linear function: - \u2202\u03bc/\u2202a: left slope - \u2202\u03bc/\u2202b: left slope and plateau transition - \u2202\u03bc/\u2202c: right slope and plateau transition - \u2202\u03bc/\u2202d: right slope</p> <p>Parameters:</p> Name Type Description Default <code>dL_dy</code> <code>ndarray</code> <p>Gradient of the loss w.r.t. the output of this layer.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def backward(self, dL_dy: np.ndarray):\n    \"\"\"Compute gradients for parameters based on upstream loss gradient.\n\n    Analytical gradients for the piecewise linear function:\n    - \u2202\u03bc/\u2202a: left slope\n    - \u2202\u03bc/\u2202b: left slope and plateau transition\n    - \u2202\u03bc/\u2202c: right slope and plateau transition\n    - \u2202\u03bc/\u2202d: right slope\n\n    Args:\n        dL_dy: Gradient of the loss w.r.t. the output of this layer.\n\n    Returns:\n        None\n    \"\"\"\n    if self.last_input is None or self.last_output is None:\n        return\n\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n    d = self.parameters[\"d\"]\n\n    x = self.last_input\n\n    # Initialize gradients\n    dL_da = 0.0\n    dL_db = 0.0\n    dL_dc = 0.0\n    dL_dd = 0.0\n\n    # Left slope region: a &lt; x &lt; b, \u03bc(x) = (x-a)/(b-a)\n    if b &gt; a:\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        if np.any(left_mask):\n            x_left = x[left_mask]\n            dL_dy_left = dL_dy[left_mask]\n\n            # \u2202\u03bc/\u2202a = -1/(b-a) for left slope\n            dmu_da_left = -1.0 / (b - a)\n            dL_da += np.sum(dL_dy_left * dmu_da_left)\n\n            # \u2202\u03bc/\u2202b = -(x-a)/(b-a)\u00b2 for left slope\n            dmu_db_left = -(x_left - a) / ((b - a) ** 2)\n            dL_db += np.sum(dL_dy_left * dmu_db_left)\n\n    # Plateau region: b \u2264 x \u2264 c, \u03bc(x) = 1\n    # No gradients for plateau region (constant function)\n\n    # Right slope region: c &lt; x &lt; d, \u03bc(x) = (d-x)/(d-c)\n    if d &gt; c:\n        right_mask = (x &gt; c) &amp; (x &lt; d)\n        if np.any(right_mask):\n            x_right = x[right_mask]\n            dL_dy_right = dL_dy[right_mask]\n\n            # \u2202\u03bc/\u2202c = (x-d)/(d-c)\u00b2 for right slope\n            dmu_dc_right = (x_right - d) / ((d - c) ** 2)\n            dL_dc += np.sum(dL_dy_right * dmu_dc_right)\n\n            # \u2202\u03bc/\u2202d = (x-c)/(d-c)\u00b2 for right slope (derivative of (d-x)/(d-c) w.r.t. d)\n            dmu_dd_right = (x_right - c) / ((d - c) ** 2)\n            dL_dd += np.sum(dL_dy_right * dmu_dd_right)\n\n    # Update gradients (accumulate for batch processing)\n    self.gradients[\"a\"] += dL_da\n    self.gradients[\"b\"] += dL_db\n    self.gradients[\"c\"] += dL_dc\n    self.gradients[\"d\"] += dL_dd\n</code></pre>"},{"location":"api/membership-functions/#anfis_toolbox.membership.TrapezoidalMF.forward","title":"forward","text":"<pre><code>forward(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute trapezoidal membership values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array containing the trapezoidal membership values.</p> Source code in <code>anfis_toolbox/membership.py</code> <pre><code>def forward(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute trapezoidal membership values.\n\n    Args:\n        x: Input array.\n\n    Returns:\n        np.ndarray: Array containing the trapezoidal membership values.\n    \"\"\"\n    a = self.parameters[\"a\"]\n    b = self.parameters[\"b\"]\n    c = self.parameters[\"c\"]\n    d = self.parameters[\"d\"]\n\n    self.last_input = x\n\n    # Initialize output with zeros\n    output = np.zeros_like(x)\n\n    # Left slope: (x - a) / (b - a) for a &lt; x &lt; b\n    if b &gt; a:  # Avoid division by zero\n        left_mask = (x &gt; a) &amp; (x &lt; b)\n        output[left_mask] = (x[left_mask] - a) / (b - a)\n\n    # Plateau: \u03bc(x) = 1 for b \u2264 x \u2264 c\n    plateau_mask = (x &gt;= b) &amp; (x &lt;= c)\n    output[plateau_mask] = 1.0\n\n    # Right slope: (d - x) / (d - c) for c &lt; x &lt; d\n    if d &gt; c:  # Avoid division by zero\n        right_mask = (x &gt; c) &amp; (x &lt; d)\n        output[right_mask] = (d - x[right_mask]) / (d - c)\n\n    # Values outside [a, d] are already zero\n\n    self.last_output = output\n    return output\n</code></pre>"},{"location":"api/metrics/","title":"Metrics API","text":"<p>This module provides comprehensive metrics for evaluating ANFIS models across regression, classification, and clustering tasks.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics","title":"anfis_toolbox.metrics","text":"<p>Common metrics utilities for ANFIS Toolbox.</p> <p>This module provides lightweight, dependency-free metrics that are useful for training and evaluating ANFIS models.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics","title":"ANFISMetrics","text":"<p>Metrics calculator utilities for ANFIS models.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics.classification_metrics","title":"classification_metrics  <code>staticmethod</code>","text":"<pre><code>classification_metrics(\n    y_true: ArrayLike,\n    y_pred: ArrayLike | None = None,\n    *,\n    y_proba: ArrayLike | None = None,\n    logits: ArrayLike | None = None,\n) -&gt; dict[str, MetricValue]\n</code></pre> <p>Return common classification metrics for encoded targets and predictions.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics.model_complexity_metrics","title":"model_complexity_metrics  <code>staticmethod</code>","text":"<pre><code>model_complexity_metrics(model: ANFIS) -&gt; dict[str, int]\n</code></pre> <p>Compute structural statistics for an ANFIS model instance.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.ANFISMetrics.regression_metrics","title":"regression_metrics  <code>staticmethod</code>","text":"<pre><code>regression_metrics(\n    y_true: ArrayLike, y_pred: ArrayLike\n) -&gt; dict[str, MetricValue]\n</code></pre> <p>Return a suite of regression metrics for predictions vs. targets.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport","title":"MetricReport  <code>dataclass</code>","text":"<pre><code>MetricReport(\n    task: Literal[\"regression\", \"classification\"],\n    _values: Mapping[str, MetricValue],\n)\n</code></pre> <p>Immutable container exposing computed metrics by key or attribute.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(item: str) -&gt; MetricValue\n</code></pre> <p>Allow attribute-style access to stored metrics.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; MetricValue\n</code></pre> <p>Provide dictionary-style access to metric values.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Sanitize stored NumPy scalars/arrays to prevent accidental mutation.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Expose the metric key iterator from the backing mapping.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.MetricReport.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, MetricValue]\n</code></pre> <p>Return a shallow copy of the underlying metric mapping.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.accuracy","title":"accuracy","text":"<pre><code>accuracy(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute accuracy from integer/one-hot labels and logits/probabilities.</p> <p>y_pred can be class indices (n,), logits (n,k), or probabilities (n,k). y_true can be class indices (n,) or one-hot (n,k).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.balanced_accuracy_score","title":"balanced_accuracy_score","text":"<pre><code>balanced_accuracy_score(\n    y_true: ArrayLike, y_pred: ArrayLike\n) -&gt; float\n</code></pre> <p>Return the macro-average recall, balancing performance across classes.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.classification_entropy","title":"classification_entropy","text":"<pre><code>classification_entropy(\n    U: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Classification Entropy (CE). Lower is better (crisper).</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Membership matrix of shape (n_samples, n_clusters).</p> required <code>epsilon</code> <code>float</code> <p>Small constant to avoid log(0).</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>CE value as float.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.compute_metrics","title":"compute_metrics","text":"<pre><code>compute_metrics(\n    y_true: ArrayLike,\n    *,\n    y_pred: ArrayLike | None = None,\n    y_proba: ArrayLike | None = None,\n    logits: ArrayLike | None = None,\n    task: Literal[\n        \"auto\", \"regression\", \"classification\"\n    ] = \"auto\",\n    metrics: Sequence[str] | None = None,\n    custom_metrics: Mapping[str, MetricFn] | None = None,\n) -&gt; MetricReport\n</code></pre> <p>Compute regression or classification metrics and return a report.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(\n    y_true, logits: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute mean cross-entropy from integer labels or one-hot vs logits.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of shape (n_samples,) of integer class labels, or     one-hot array of shape (n_samples, n_classes).</p> required <code>logits</code> <code>ndarray</code> <p>Array-like raw scores, shape (n_samples, n_classes).</p> required <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>Mean cross-entropy (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.explained_variance_score","title":"explained_variance_score","text":"<pre><code>explained_variance_score(\n    y_true, y_pred, epsilon: float = _EPSILON\n) -&gt; float\n</code></pre> <p>Compute the explained variance score for regression predictions.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.log_loss","title":"log_loss","text":"<pre><code>log_loss(\n    y_true, y_prob: ndarray, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute mean log loss from integer/one-hot labels and probabilities.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_absolute_error","title":"mean_absolute_error","text":"<pre><code>mean_absolute_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean absolute error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values, shape (...,)</p> required <code>y_pred</code> <p>Array-like of predicted values, same shape as y_true</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of absolute differences over all elements as a float.</p> Notes <ul> <li>Inputs are coerced to NumPy arrays with dtype=float.</li> <li>Broadcasting follows NumPy semantics. If shapes are not compatible   for element-wise subtraction, a ValueError will be raised by NumPy.</li> </ul>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_absolute_percentage_error","title":"mean_absolute_percentage_error","text":"<pre><code>mean_absolute_percentage_error(\n    y_true,\n    y_pred,\n    epsilon: float = 1e-12,\n    *,\n    ignore_zero_targets: bool = False,\n) -&gt; float\n</code></pre> <p>Compute the mean absolute percentage error (MAPE) in percent.</p> <p>MAPE = mean( abs((y_true - y_pred) / max(abs(y_true), epsilon)) ) * 100</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values.</p> required <code>y_pred</code> <p>Array-like of predicted values, broadcastable to y_true.</p> required <code>epsilon</code> <code>float</code> <p>Small constant to avoid division by zero when y_true == 0.</p> <code>1e-12</code> <code>ignore_zero_targets</code> <code>bool</code> <p>When True, drop samples where |y_true| &lt;= epsilon; if all targets are (near) zero, returns <code>np.inf</code> to signal undefined percentage.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>MAPE value as a percentage (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_bias_error","title":"mean_bias_error","text":"<pre><code>mean_bias_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean signed error, positive when predictions overshoot.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_squared_error","title":"mean_squared_error","text":"<pre><code>mean_squared_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values, shape (...,)</p> required <code>y_pred</code> <p>Array-like of predicted values, same shape as y_true</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of squared differences over all elements as a float.</p> Notes <ul> <li>Inputs are coerced to NumPy arrays with dtype=float.</li> <li>Broadcasting follows NumPy semantics. If shapes are not compatible   for element-wise subtraction, a ValueError will be raised by NumPy.</li> </ul>"},{"location":"api/metrics/#anfis_toolbox.metrics.mean_squared_logarithmic_error","title":"mean_squared_logarithmic_error","text":"<pre><code>mean_squared_logarithmic_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the mean squared logarithmic error (MSLE).</p> <p>Requires non-negative inputs. Uses log1p for numerical stability: MSLE = mean( (log1p(y_true) - log1p(y_pred))^2 ).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.median_absolute_error","title":"median_absolute_error","text":"<pre><code>median_absolute_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Return the median absolute deviation between predictions and targets.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.partition_coefficient","title":"partition_coefficient","text":"<pre><code>partition_coefficient(U: ndarray) -&gt; float\n</code></pre> <p>Bezdek's Partition Coefficient (PC) in [1/k, 1]. Higher is crisper.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Membership matrix of shape (n_samples, n_clusters).</p> required <p>Returns:</p> Type Description <code>float</code> <p>PC value as float.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.pearson_correlation","title":"pearson_correlation","text":"<pre><code>pearson_correlation(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the Pearson correlation coefficient r.</p> <p>Returns 0.0 when the standard deviation of either input is ~0 (undefined r).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.precision_recall_f1","title":"precision_recall_f1","text":"<pre><code>precision_recall_f1(\n    y_true: ArrayLike,\n    y_pred: ArrayLike,\n    average: Literal[\"macro\", \"micro\", \"binary\"] = \"macro\",\n) -&gt; tuple[float, float, float]\n</code></pre> <p>Compute precision, recall, and F1 score with the requested averaging.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.quick_evaluate","title":"quick_evaluate","text":"<pre><code>quick_evaluate(\n    model: object,\n    X_test: ndarray,\n    y_test: ndarray,\n    print_results: bool = True,\n    task: Literal[\n        \"auto\", \"regression\", \"classification\"\n    ] = \"auto\",\n) -&gt; dict[str, float]\n</code></pre> <p>Evaluate a trained ANFIS model or estimator on test data.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.r2_score","title":"r2_score","text":"<pre><code>r2_score(y_true, y_pred, epsilon: float = 1e-12) -&gt; float\n</code></pre> <p>Compute the coefficient of determination R^2.</p> <p>R^2 = 1 - SS_res / SS_tot, where SS_res = sum((y - y_hat)^2) and SS_tot = sum((y - mean(y))^2). If SS_tot is ~0 (constant target), returns 1.0 when predictions match the constant target (SS_res ~0), otherwise 0.0.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.root_mean_squared_error","title":"root_mean_squared_error","text":"<pre><code>root_mean_squared_error(y_true, y_pred) -&gt; float\n</code></pre> <p>Compute the root mean squared error (RMSE).</p> <p>This is simply the square root of mean_squared_error.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.softmax","title":"softmax","text":"<pre><code>softmax(logits: ndarray, axis: int = -1) -&gt; np.ndarray\n</code></pre> <p>Compute a numerically stable softmax along a given axis.</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.symmetric_mean_absolute_percentage_error","title":"symmetric_mean_absolute_percentage_error","text":"<pre><code>symmetric_mean_absolute_percentage_error(\n    y_true, y_pred, epsilon: float = 1e-12\n) -&gt; float\n</code></pre> <p>Compute the symmetric mean absolute percentage error (SMAPE) in percent.</p> <p>SMAPE = mean( 200 * |y_true - y_pred| / (|y_true| + |y_pred|) ) with an epsilon added to denominator to avoid division by zero.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Array-like of true target values.</p> required <code>y_pred</code> <p>Array-like of predicted values, broadcastable to y_true.</p> required <code>epsilon</code> <code>float</code> <p>Small constant added to denominator to avoid division by zero.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>SMAPE value as a percentage (float).</p>"},{"location":"api/metrics/#anfis_toolbox.metrics.xie_beni_index","title":"xie_beni_index","text":"<pre><code>xie_beni_index(\n    X: ndarray,\n    U: ndarray,\n    C: ndarray,\n    m: float = 2.0,\n    epsilon: float = 1e-12,\n) -&gt; float\n</code></pre> <p>Xie-Beni index (XB). Lower is better.</p> <p>XB = sum_i sum_k u_ik^m ||x_i - v_k||^2 / (n * min_{p!=q} ||v_p - v_q||^2)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data array, shape (n_samples, n_features) or (n_samples,).</p> required <code>U</code> <code>ndarray</code> <p>Membership matrix, shape (n_samples, n_clusters).</p> required <code>C</code> <code>ndarray</code> <p>Cluster centers, shape (n_clusters, n_features).</p> required <code>m</code> <code>float</code> <p>Fuzzifier (&gt;1).</p> <code>2.0</code> <code>epsilon</code> <code>float</code> <p>Small constant to avoid division by zero.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>float</code> <p>XB value as float (np.inf when centers &lt; 2).</p>"},{"location":"api/metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Functions for evaluating regression model performance:</p> <ul> <li><code>mean_squared_error()</code> - Mean squared error</li> <li><code>mean_absolute_error()</code> - Mean absolute error</li> <li><code>root_mean_squared_error()</code> - Root mean squared error</li> <li><code>mean_absolute_percentage_error()</code> - Mean absolute percentage error</li> <li><code>symmetric_mean_absolute_percentage_error()</code> - Symmetric MAPE</li> <li><code>r2_score()</code> - Coefficient of determination</li> <li><code>explained_variance_score()</code> - Explained variance of predictions</li> <li><code>median_absolute_error()</code> - Median absolute error (robust to outliers)</li> <li><code>mean_bias_error()</code> - Mean prediction bias</li> <li><code>pearson_correlation()</code> - Pearson correlation coefficient</li> <li><code>mean_squared_logarithmic_error()</code> - Mean squared logarithmic error</li> </ul>"},{"location":"api/metrics/#classification-metrics","title":"Classification Metrics","text":"<p>Functions for evaluating classification model performance:</p> <ul> <li><code>softmax()</code> - Numerically stable softmax</li> <li><code>cross_entropy()</code> - Cross-entropy loss</li> <li><code>log_loss()</code> - Log loss</li> <li><code>accuracy()</code> - Classification accuracy</li> <li><code>balanced_accuracy_score()</code> - Macro average of per-class recall</li> <li><code>precision_recall_f1()</code> - Precision, recall, and F1 with macro/micro/binary averaging</li> </ul>"},{"location":"api/metrics/#clustering-validation","title":"Clustering Validation","text":"<p>Functions for evaluating fuzzy clustering quality:</p> <ul> <li><code>partition_coefficient()</code> - Bezdek's partition coefficient</li> <li><code>classification_entropy()</code> - Classification entropy</li> <li><code>xie_beni_index()</code> - Xie-Beni validity index</li> </ul>"},{"location":"api/metrics/#metric-reports-automation","title":"Metric Reports &amp; Automation","text":"<ul> <li><code>compute_metrics()</code> - One-stop helper that infers the task (regression vs. classification) and returns a <code>MetricReport</code></li> <li><code>MetricReport</code> - Read-only container with attribute/dict-style access and a <code>.to_dict()</code> export</li> </ul>"},{"location":"api/models/","title":"ANFIS Models","text":""},{"location":"api/models/#anfis_toolbox.model.ANFIS","title":"anfis_toolbox.model.ANFIS  <code>module-attribute</code>","text":"<pre><code>ANFIS = TSKANFIS\n</code></pre>"},{"location":"api/models/#anfis_toolbox.model.ANFISClassifier","title":"anfis_toolbox.model.ANFISClassifier  <code>module-attribute</code>","text":"<pre><code>ANFISClassifier = TSKANFISClassifier\n</code></pre>"},{"location":"api/optim/","title":"Optimization","text":""},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer","title":"anfis_toolbox.optim.base.BaseTrainer","text":"<p>               Bases: <code>ABC</code></p> <p>Shared training loop for ANFIS trainers.</p>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.compute_loss","title":"compute_loss  <code>abstractmethod</code>","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Compute loss for the provided data without mutating the model.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:  # pragma: no cover - abstract\n    \"\"\"Compute loss for the provided data without mutating the model.\"\"\"\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.fit","title":"fit","text":"<pre><code>fit(\n    model,\n    X: ndarray,\n    y: ndarray,\n    *,\n    validation_data: tuple[ndarray, ndarray] | None = None,\n    validation_frequency: int = 1,\n) -&gt; TrainingHistory\n</code></pre> <p>Train <code>model</code> on <code>(X, y)</code> and optionally evaluate on validation data.</p> <p>Returns a dictionary containing the per-epoch training losses and, when <code>validation_data</code> is provided, the validation losses (aligned with the training epochs; epochs without validation are recorded as <code>None</code>).</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>def fit(\n    self,\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    *,\n    validation_data: tuple[np.ndarray, np.ndarray] | None = None,\n    validation_frequency: int = 1,\n) -&gt; TrainingHistory:\n    \"\"\"Train ``model`` on ``(X, y)`` and optionally evaluate on validation data.\n\n    Returns a dictionary containing the per-epoch training losses and, when\n    ``validation_data`` is provided, the validation losses (aligned with the\n    training epochs; epochs without validation are recorded as ``None``).\n    \"\"\"\n    if validation_frequency &lt; 1:\n        raise ValueError(\"validation_frequency must be &gt;= 1\")\n\n    X_train, y_train = self._prepare_training_data(model, X, y)\n    state = self.init_state(model, X_train, y_train)\n\n    prepared_val: tuple[np.ndarray, np.ndarray] | None = None\n    if validation_data is not None:\n        prepared_val = self._prepare_validation_data(model, *validation_data)\n\n    epochs = int(getattr(self, \"epochs\", 1))\n    batch_size = getattr(self, \"batch_size\", None)\n    shuffle = bool(getattr(self, \"shuffle\", True))\n    verbose = bool(getattr(self, \"verbose\", False))\n\n    train_history: list[float] = []\n    val_history: list[float | None] = [] if prepared_val is not None else []\n\n    n_samples = X_train.shape[0]\n    for epoch_idx in range(epochs):\n        epoch_losses: list[float] = []\n        if batch_size is None:\n            loss, state = self.train_step(model, X_train, y_train, state)\n            epoch_losses.append(float(loss))\n        else:\n            indices = np.arange(n_samples)\n            if shuffle:\n                np.random.shuffle(indices)\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                batch_idx = indices[start:end]\n                loss, state = self.train_step(\n                    model,\n                    X_train[batch_idx],\n                    y_train[batch_idx],\n                    state,\n                )\n                epoch_losses.append(float(loss))\n\n        epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        train_history.append(epoch_loss)\n\n        val_loss: float | None = None\n        if prepared_val is not None:\n            if (epoch_idx + 1) % validation_frequency == 0:\n                X_val, y_val = prepared_val\n                val_loss = float(self.compute_loss(model, X_val, y_val))\n            val_history.append(val_loss)\n\n        self._log_epoch(epoch_idx, epoch_loss, val_loss, verbose)\n\n    result: TrainingHistory = {\"train\": train_history}\n    if prepared_val is not None:\n        result[\"val\"] = val_history\n    return result\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.init_state","title":"init_state  <code>abstractmethod</code>","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize and return any optimizer-specific state.</p> <p>Called once before training begins. Trainers that don't require state may return None.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to be trained.</p> required <code>X</code> <code>ndarray</code> <p>The full training inputs.</p> required <code>y</code> <code>ndarray</code> <p>The full training targets.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Optimizer state (or None) to be threaded through <code>train_step</code>.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef init_state(self, model, X: np.ndarray, y: np.ndarray):  # pragma: no cover - abstract\n    \"\"\"Initialize and return any optimizer-specific state.\n\n    Called once before training begins. Trainers that don't require state may\n    return None.\n\n    Parameters:\n        model: The model to be trained.\n        X (np.ndarray): The full training inputs.\n        y (np.ndarray): The full training targets.\n\n    Returns:\n        Any: Optimizer state (or None) to be threaded through ``train_step``.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.base.BaseTrainer.train_step","title":"train_step  <code>abstractmethod</code>","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform a single training step on a batch and return (loss, new_state).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to be trained.</p> required <code>Xb</code> <code>ndarray</code> <p>A batch of inputs.</p> required <code>yb</code> <code>ndarray</code> <p>A batch of targets.</p> required <code>state</code> <p>Optimizer state produced by <code>init_state</code>.</p> required <p>Returns:</p> Type Description <p>tuple[float, Any]: The batch loss and the updated optimizer state.</p> Source code in <code>anfis_toolbox/optim/base.py</code> <pre><code>@abstractmethod\ndef train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):  # pragma: no cover - abstract\n    \"\"\"Perform a single training step on a batch and return (loss, new_state).\n\n    Parameters:\n        model: The model to be trained.\n        Xb (np.ndarray): A batch of inputs.\n        yb (np.ndarray): A batch of targets.\n        state: Optimizer state produced by ``init_state``.\n\n    Returns:\n        tuple[float, Any]: The batch loss and the updated optimizer state.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer","title":"anfis_toolbox.optim.hybrid.HybridTrainer  <code>dataclass</code>","text":"<pre><code>HybridTrainer(\n    learning_rate: float = 0.01,\n    epochs: int = 100,\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Original Jang (1993) hybrid training: LSM for consequents + GD for antecedents.</p> Notes <p>This trainer assumes a single-output regression head. It is not compatible with :class:<code>~anfis_toolbox.model.TSKANFISClassifier</code> or the high-level :class:<code>~anfis_toolbox.classifier.ANFISClassifier</code> facade.</p>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Compute the hybrid MSE loss on prepared data without side effects.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute the hybrid MSE loss on prepared data without side effects.\"\"\"\n    X_arr, y_arr = self._prepare_data(X, y)\n    membership_outputs = model.membership_layer.forward(X_arr)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n    preds = model.consequent_layer.forward(X_arr, normalized_weights)\n    return float(mse_loss(y_arr, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Hybrid trainer doesn't maintain optimizer state; returns None.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Hybrid trainer doesn't maintain optimizer state; returns None.\"\"\"\n    return None\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid.HybridTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one hybrid step on a batch and return (loss, state).</p> <p>Equivalent to one iteration of the hybrid algorithm on the given batch.</p> Source code in <code>anfis_toolbox/optim/hybrid.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one hybrid step on a batch and return (loss, state).\n\n    Equivalent to one iteration of the hybrid algorithm on the given batch.\n    \"\"\"\n    Xb, yb = self._prepare_data(Xb, yb)\n    # Forward to get normalized weights\n    membership_outputs = model.membership_layer.forward(Xb)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n\n    # Build LSM system for batch\n    ones_col = np.ones((Xb.shape[0], 1), dtype=float)\n    x_bar = np.concatenate([Xb, ones_col], axis=1)\n    A_blocks = [normalized_weights[:, j : j + 1] * x_bar for j in range(model.n_rules)]\n    A = np.concatenate(A_blocks, axis=1)\n    try:\n        regularization = 1e-6 * np.eye(A.shape[1])\n        ATA_reg = A.T @ A + regularization\n        theta = np.linalg.solve(ATA_reg, A.T @ yb.flatten())\n    except np.linalg.LinAlgError:\n        logging.getLogger(__name__).warning(\"Matrix singular in LSM, using pseudo-inverse\")\n        theta = np.linalg.pinv(A) @ yb.flatten()\n    model.consequent_layer.parameters = theta.reshape(model.n_rules, model.n_inputs + 1)\n\n    # Loss and backward for antecedents only\n    y_pred = model.consequent_layer.forward(Xb, normalized_weights)\n    loss = mse_loss(yb, y_pred)\n    dL_dy = mse_grad(yb, y_pred)\n    dL_dnorm_w, _ = model.consequent_layer.backward(dL_dy)\n    dL_dw = model.normalization_layer.backward(dL_dnorm_w)\n    gradients = model.rule_layer.backward(dL_dw)\n    model.membership_layer.backward(gradients)\n    model._apply_membership_gradients(self.learning_rate)\n    return float(loss), state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer","title":"anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer  <code>dataclass</code>","text":"<pre><code>HybridAdamTrainer(\n    learning_rate: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Hybrid training: LSM for consequents + Adam for antecedents.</p> Notes <p>This variant also targets the regression ANFIS. It is not compatible with the classification head (:class:<code>~anfis_toolbox.model.TSKANFISClassifier</code>) or :class:<code>~anfis_toolbox.classifier.ANFISClassifier</code>.</p>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Evaluate mean squared error on provided data without updates.</p> Source code in <code>anfis_toolbox/optim/hybrid_adam.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Evaluate mean squared error on provided data without updates.\"\"\"\n    X_arr, y_arr = self._prepare_data(X, y)\n    membership_outputs = model.membership_layer.forward(X_arr)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n    preds = model.consequent_layer.forward(X_arr, normalized_weights)\n    return float(mse_loss(y_arr, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize Adam moment tensors for membership parameters.</p> Source code in <code>anfis_toolbox/optim/hybrid_adam.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize Adam moment tensors for membership parameters.\"\"\"\n    params = model.get_parameters()\n    zero_struct = _zeros_like_structure(params)[\"membership\"]\n    return {\"m\": deepcopy(zero_struct), \"v\": deepcopy(zero_struct), \"t\": 0}\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.hybrid_adam.HybridAdamTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Execute one hybrid iteration combining LSM and Adam updates.</p> Source code in <code>anfis_toolbox/optim/hybrid_adam.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Execute one hybrid iteration combining LSM and Adam updates.\"\"\"\n    model.reset_gradients()\n    Xb, yb = self._prepare_data(Xb, yb)\n    membership_outputs = model.membership_layer.forward(Xb)\n    rule_strengths = model.rule_layer.forward(membership_outputs)\n    normalized_weights = model.normalization_layer.forward(rule_strengths)\n    ones_col = np.ones((Xb.shape[0], 1), dtype=float)\n    x_bar = np.concatenate([Xb, ones_col], axis=1)\n    A_blocks = [normalized_weights[:, j : j + 1] * x_bar for j in range(model.n_rules)]\n    A = np.concatenate(A_blocks, axis=1)\n    try:\n        regularization = 1e-6 * np.eye(A.shape[1])\n        ATA_reg = A.T @ A + regularization\n        theta = np.linalg.solve(ATA_reg, A.T @ yb.flatten())\n    except np.linalg.LinAlgError:\n        logging.getLogger(__name__).warning(\"Matrix singular in LSM, using pseudo-inverse\")\n        theta = np.linalg.pinv(A) @ yb.flatten()\n    model.consequent_layer.parameters = theta.reshape(model.n_rules, model.n_inputs + 1)\n\n    # Adam for antecedents\n    y_pred = model.consequent_layer.forward(Xb, normalized_weights)\n    loss = mse_loss(yb, y_pred)\n    dL_dy = mse_grad(yb, y_pred)\n    dL_dnorm_w, _ = model.consequent_layer.backward(dL_dy)\n    dL_dw = model.normalization_layer.backward(dL_dnorm_w)\n    gradients = model.rule_layer.backward(dL_dw)\n    grad_struct = model.membership_layer.backward(gradients)\n    self._apply_adam_update(model, grad_struct, state)\n    return float(loss), state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer","title":"anfis_toolbox.optim.sgd.SGDTrainer  <code>dataclass</code>","text":"<pre><code>SGDTrainer(\n    learning_rate: float = 0.01,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Stochastic gradient descent trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Step size for gradient descent.</p> <code>0.01</code> <code>epochs</code> <code>int</code> <p>Number of passes over the data.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>Mini-batch size; if None uses full batch.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data each epoch.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress (delegated to model logging settings).</p> <code>False</code> Notes <p>Uses the configurable loss provided via <code>loss</code> (defaults to mean squared error). The selected loss is responsible for adapting target shapes via <code>prepare_targets</code>. When used with <code>ANFISClassifier</code> and <code>loss=\"cross_entropy\"</code> it trains on logits with the appropriate softmax gradient.</p>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Return the loss for <code>(X, y)</code> without mutating <code>model</code>.</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Return the loss for ``(X, y)`` without mutating ``model``.\"\"\"\n    preds = model.forward(X)\n    return float(self._loss_fn.loss(y, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>SGD has no persistent optimizer state; returns None.</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"SGD has no persistent optimizer state; returns None.\"\"\"\n    return None\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.sgd.SGDTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one SGD step on a batch and return (loss, state).</p> Source code in <code>anfis_toolbox/optim/sgd.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one SGD step on a batch and return (loss, state).\"\"\"\n    loss = self._compute_loss_backward_and_update(model, Xb, yb)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer","title":"anfis_toolbox.optim.adam.AdamTrainer  <code>dataclass</code>","text":"<pre><code>AdamTrainer(\n    learning_rate: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Adam optimizer-based trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Base step size (alpha).</p> <code>0.001</code> <code>beta1</code> <code>float</code> <p>Exponential decay rate for the first moment estimates.</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>Exponential decay rate for the second moment estimates.</p> <code>0.999</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <code>epochs</code> <code>int</code> <p>Number of passes over the dataset.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>If None, use full-batch; otherwise mini-batches of this size.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at each epoch when using mini-batches.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>False</code> Notes <p>Supports configurable losses via the <code>loss</code> parameter. Defaults to mean squared error for regression, but can minimize other differentiable objectives such as categorical cross-entropy when used with <code>ANFISClassifier</code>.</p>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Evaluate the configured loss on <code>(X, y)</code> without updating parameters.</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Evaluate the configured loss on ``(X, y)`` without updating parameters.\"\"\"\n    preds = model.forward(X)\n    return float(self._loss_fn.loss(y, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize Adam's first and second moments and time step.</p> <p>Returns a dict with keys: params, m, v, t.</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize Adam's first and second moments and time step.\n\n    Returns a dict with keys: params, m, v, t.\n    \"\"\"\n    params = model.get_parameters()\n    return {\n        \"params\": params,\n        \"m\": _zeros_like_structure(params),\n        \"v\": _zeros_like_structure(params),\n        \"t\": 0,\n    }\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.adam.AdamTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>One Adam step on a batch; returns (loss, updated_state).</p> Source code in <code>anfis_toolbox/optim/adam.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"One Adam step on a batch; returns (loss, updated_state).\"\"\"\n    loss, grads = self._compute_loss_and_grads(model, Xb, yb)\n    t_new = self._apply_adam_step(model, state[\"params\"], grads, state[\"m\"], state[\"v\"], state[\"t\"])\n    state[\"t\"] = t_new\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer","title":"anfis_toolbox.optim.rmsprop.RMSPropTrainer  <code>dataclass</code>","text":"<pre><code>RMSPropTrainer(\n    learning_rate: float = 0.001,\n    rho: float = 0.9,\n    epsilon: float = 1e-08,\n    epochs: int = 100,\n    batch_size: None | int = None,\n    shuffle: bool = True,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>RMSProp optimizer-based trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Base step size (alpha).</p> <code>0.001</code> <code>rho</code> <code>float</code> <p>Exponential decay rate for the squared gradient moving average.</p> <code>0.9</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <code>epochs</code> <code>int</code> <p>Number of passes over the dataset.</p> <code>100</code> <code>batch_size</code> <code>None | int</code> <p>If None, use full-batch; otherwise mini-batches of this size.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at each epoch when using mini-batches.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>False</code> Notes <p>Supports configurable losses via the <code>loss</code> parameter. Defaults to mean squared error for regression tasks but can be switched to other differentiable objectives such as categorical cross-entropy when training <code>ANFISClassifier</code> models.</p>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Return the current loss value for <code>(X, y)</code> without modifying state.</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Return the current loss value for ``(X, y)`` without modifying state.\"\"\"\n    preds = model.forward(X)\n    return float(self._loss_fn.loss(y, preds))\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize RMSProp caches for consequents and membership scalars.</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize RMSProp caches for consequents and membership scalars.\"\"\"\n    params = model.get_parameters()\n    return {\"params\": params, \"cache\": _zeros_like_structure(params)}\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.rmsprop.RMSPropTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>One RMSProp step on a batch; returns (loss, updated_state).</p> Source code in <code>anfis_toolbox/optim/rmsprop.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"One RMSProp step on a batch; returns (loss, updated_state).\"\"\"\n    loss, grads = self._compute_loss_and_grads(model, Xb, yb)\n    self._apply_rmsprop_step(model, state[\"params\"], state[\"cache\"], grads)\n    return loss, state\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer","title":"anfis_toolbox.optim.pso.PSOTrainer  <code>dataclass</code>","text":"<pre><code>PSOTrainer(\n    swarm_size: int = 20,\n    inertia: float = 0.7,\n    cognitive: float = 1.5,\n    social: float = 1.5,\n    epochs: int = 100,\n    init_sigma: float = 0.1,\n    clamp_velocity: None | tuple[float, float] = None,\n    clamp_position: None | tuple[float, float] = None,\n    random_state: None | int = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseTrainer</code></p> <p>Particle Swarm Optimization (PSO) trainer for ANFIS.</p> <p>Parameters:</p> Name Type Description Default <code>swarm_size</code> <code>int</code> <p>Number of particles.</p> <code>20</code> <code>inertia</code> <code>float</code> <p>Inertia weight (w).</p> <code>0.7</code> <code>cognitive</code> <code>float</code> <p>Cognitive coefficient (c1).</p> <code>1.5</code> <code>social</code> <code>float</code> <p>Social coefficient (c2).</p> <code>1.5</code> <code>epochs</code> <code>int</code> <p>Number of iterations of the swarm update.</p> <code>100</code> <code>init_sigma</code> <code>float</code> <p>Std-dev for initializing particle positions around current params.</p> <code>0.1</code> <code>clamp_velocity</code> <code>None | tuple[float, float]</code> <p>Optional (min, max) to clip velocities element-wise.</p> <code>None</code> <code>clamp_position</code> <code>None | tuple[float, float]</code> <p>Optional (min, max) to clip positions element-wise.</p> <code>None</code> <code>random_state</code> <code>None | int</code> <p>Seed for RNG to ensure determinism.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Unused here; kept for API parity.</p> <code>False</code> Notes <p>Optimizes the loss specified by <code>loss</code> (defaulting to mean squared error) by searching directly in parameter space without gradients. With <code>ANFISClassifier</code> you can set <code>loss=\"cross_entropy\"</code> to optimize categorical cross-entropy on logits.</p>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(model, X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Evaluate the swarm's current parameters on <code>(X, y)</code> without mutation.</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def compute_loss(self, model, X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Evaluate the swarm's current parameters on ``(X, y)`` without mutation.\"\"\"\n    return self._evaluate_loss(model, X, y)\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.init_state","title":"init_state","text":"<pre><code>init_state(model, X: ndarray, y: ndarray)\n</code></pre> <p>Initialize PSO swarm state and return as a dict.</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def init_state(self, model, X: np.ndarray, y: np.ndarray):\n    \"\"\"Initialize PSO swarm state and return as a dict.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    rng = np.random.default_rng(self.random_state)\n    base_params = model.get_parameters()\n    theta0, meta = _flatten_params(base_params)\n    D = theta0.size\n    positions = theta0[None, :] + self.init_sigma * rng.normal(size=(self.swarm_size, D))\n    velocities = np.zeros((self.swarm_size, D), dtype=float)\n    # Initialize personal/global bests on provided data\n    personal_best_pos = positions.copy()\n    personal_best_val = np.empty(self.swarm_size, dtype=float)\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, base_params)\n        with self._temporary_parameters(model, params_i):\n            personal_best_val[i] = self._evaluate_loss(model, X, y)\n    g_idx = int(np.argmin(personal_best_val))\n    global_best_pos = personal_best_pos[g_idx].copy()\n    global_best_val = float(personal_best_val[g_idx])\n    return {\n        \"meta\": meta,\n        \"template\": base_params,\n        \"positions\": positions,\n        \"velocities\": velocities,\n        \"pbest_pos\": personal_best_pos,\n        \"pbest_val\": personal_best_val,\n        \"gbest_pos\": global_best_pos,\n        \"gbest_val\": global_best_val,\n        \"rng\": rng,\n    }\n</code></pre>"},{"location":"api/optim/#anfis_toolbox.optim.pso.PSOTrainer.train_step","title":"train_step","text":"<pre><code>train_step(model, Xb: ndarray, yb: ndarray, state)\n</code></pre> <p>Perform one PSO iteration over the swarm on a batch and return (best_loss, state).</p> Source code in <code>anfis_toolbox/optim/pso.py</code> <pre><code>def train_step(self, model, Xb: np.ndarray, yb: np.ndarray, state):\n    \"\"\"Perform one PSO iteration over the swarm on a batch and return (best_loss, state).\"\"\"\n    positions = state[\"positions\"]\n    velocities = state[\"velocities\"]\n    personal_best_pos = state[\"pbest_pos\"]\n    personal_best_val = state[\"pbest_val\"]\n    global_best_pos = state[\"gbest_pos\"]\n    global_best_val = state[\"gbest_val\"]\n    meta = state[\"meta\"]\n    template = state[\"template\"]\n    rng = state[\"rng\"]\n\n    D = positions.shape[1]\n    r1 = rng.random(size=(self.swarm_size, D))\n    r2 = rng.random(size=(self.swarm_size, D))\n    cognitive_term = self.cognitive * r1 * (personal_best_pos - positions)\n    social_term = self.social * r2 * (global_best_pos[None, :] - positions)\n    velocities = self.inertia * velocities + cognitive_term + social_term\n    if self.clamp_velocity is not None:\n        vmin, vmax = self.clamp_velocity\n        velocities = np.clip(velocities, vmin, vmax)\n    positions = positions + velocities\n    if self.clamp_position is not None:\n        pmin, pmax = self.clamp_position\n        positions = np.clip(positions, pmin, pmax)\n\n    # Evaluate swarm and update bests\n    for i in range(self.swarm_size):\n        params_i = _unflatten_params(positions[i], meta, template)\n        with self._temporary_parameters(model, params_i):\n            val = self._evaluate_loss(model, Xb, yb)\n        if val &lt; personal_best_val[i]:\n            personal_best_val[i] = val\n            personal_best_pos[i] = positions[i].copy()\n            if val &lt; global_best_val:\n                global_best_val = float(val)\n                global_best_pos = positions[i].copy()\n\n    # Update state and set model to global best\n    state.update(\n        {\n            \"positions\": positions,\n            \"velocities\": velocities,\n            \"pbest_pos\": personal_best_pos,\n            \"pbest_val\": personal_best_val,\n            \"gbest_pos\": global_best_pos,\n            \"gbest_val\": global_best_val,\n        }\n    )\n    best_params = _unflatten_params(global_best_pos, meta, template)\n    model.set_parameters(best_params)\n    return float(global_best_val), state\n</code></pre>"},{"location":"api/regressor/","title":"Regressor API","text":"<p><code>ANFISRegressor</code> provides the high-level fa\u00e7ade for regression workflows, combining membership-function generation, rule construction, and optimization with familiar estimator semantics.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor","title":"anfis_toolbox.regressor.ANFISRegressor","text":"<pre><code>ANFISRegressor(\n    *,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.1,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str\n    | BaseTrainer\n    | type[BaseTrainer]\n    | None = \"hybrid\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimatorLike</code>, <code>FittedMixin</code>, <code>RegressorMixinLike</code></p> <p>Adaptive Neuro-Fuzzy regressor with a scikit-learn style API.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor--parameters","title":"Parameters","text":"<p>n_mfs : int, default=3     Default number of membership functions per input. mf_type : str, default=\"gaussian\"     Default membership function family (see :class:<code>ANFISBuilder</code>). init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Strategy used when inferring membership functions from data. <code>None</code>     falls back to <code>\"grid\"</code>. overlap : float, default=0.5     Controls overlap when generating membership functions via the builder. margin : float, default=0.10     Margin added around observed data ranges during grid initialization. inputs_config : Mapping, optional     Per-input overrides. Keys may be feature names (when <code>X</code> is a     :class:<code>pandas.DataFrame</code>) or integer indices. Values may be:</p> <pre><code>* ``dict`` with keys among ``{\"n_mfs\", \"mf_type\", \"init\", \"overlap\",\n  \"margin\", \"range\", \"membership_functions\", \"mfs\"}``.\n* A list/tuple of :class:`MembershipFunction` instances for full control.\n* ``None`` for defaults.\n</code></pre> <p>random_state : int, optional     Random state forwarded to FCM-based initialization and any stochastic     optimizers. optimizer : str, BaseTrainer, type[BaseTrainer], or None, default=\"hybrid\"     Trainer identifier or instance used for fitting. Strings map to entries     in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to \"hybrid\". optimizer_params : Mapping, optional     Additional keyword arguments forwarded to the trainer constructor. learning_rate, epochs, batch_size, shuffle, verbose : optional scalars     Common trainer hyper-parameters provided for convenience. When the     selected trainer supports the parameter it is included automatically. loss : str or LossFunction, optional     Custom loss forwarded to trainers that expose a <code>loss</code> parameter. rules : Sequence[Sequence[int]] | None, optional     Explicit fuzzy rule indices to use instead of the full Cartesian product. Each     rule lists the membership-function index per input. <code>None</code> keeps the default     exhaustive rule set.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor--parameters","title":"Parameters","text":"<p>n_mfs : int, default=3     Default number of membership functions allocated to each input when     the builder infers them from data. mf_type : str, default=\"gaussian\"     Membership function family used for automatically generated     membership functions. See :class:<code>ANFISBuilder</code> for supported     values. init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"     Initialization strategy employed when synthesizing membership     functions from the training data. <code>None</code> falls back to     <code>\"grid\"</code>. overlap : float, default=0.5     Desired overlap between neighbouring membership functions during     automatic construction. margin : float, default=0.10     Extra range added around the observed feature minima/maxima when     performing grid initialization. inputs_config : Mapping, optional     Per-feature overrides for membership configuration. Keys may be     feature names (e.g. when <code>X</code> is a :class:<code>pandas.DataFrame</code>),     integer indices, or <code>\"x{i}\"</code> aliases. Values accept dictionaries     mirroring builder arguments, explicit membership function lists, or     scalars for simple overrides. <code>None</code> entries keep defaults. random_state : int, optional     Seed propagated to stochastic components such as FCM-based     initialization and optimizers that rely on randomness. optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"hybrid\"     Trainer identifier or instance used for fitting. String aliases are     looked up in :data:<code>TRAINER_REGISTRY</code>. <code>None</code> defaults to     <code>\"hybrid\"</code>. optimizer_params : Mapping, optional     Extra keyword arguments forwarded to the trainer constructor when a     string identifier or class is supplied. learning_rate, epochs, batch_size, shuffle, verbose : optional     Convenience hyper-parameters that are injected into the selected     trainer when supported. <code>shuffle</code> accepts <code>False</code> to disable     randomisation. loss : str | LossFunction, optional     Custom loss forwarded to trainers exposing a <code>loss</code> parameter.     <code>None</code> keeps the trainer default (typically mean squared error). rules : Sequence[Sequence[int]] | None, optional     Optional explicit fuzzy rule definitions. Each rule lists the     membership index for every input. <code>None</code> uses the full Cartesian     product of configured membership functions.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    n_mfs: int = 3,\n    mf_type: str = \"gaussian\",\n    init: str | None = \"grid\",\n    overlap: float = 0.5,\n    margin: float = 0.10,\n    inputs_config: Mapping[Any, Any] | None = None,\n    random_state: int | None = None,\n    optimizer: str | BaseTrainer | type[BaseTrainer] | None = \"hybrid\",\n    optimizer_params: Mapping[str, Any] | None = None,\n    learning_rate: float | None = None,\n    epochs: int | None = None,\n    batch_size: int | None = None,\n    shuffle: bool | None = None,\n    verbose: bool = False,\n    loss: LossFunction | str | None = None,\n    rules: Sequence[Sequence[int]] | None = None,\n) -&gt; None:\n    \"\"\"Construct an :class:`ANFISRegressor` with the provided hyper-parameters.\n\n    Parameters\n    ----------\n    n_mfs : int, default=3\n        Default number of membership functions allocated to each input when\n        the builder infers them from data.\n    mf_type : str, default=\"gaussian\"\n        Membership function family used for automatically generated\n        membership functions. See :class:`ANFISBuilder` for supported\n        values.\n    init : {\"grid\", \"fcm\", \"random\", None}, default=\"grid\"\n        Initialization strategy employed when synthesizing membership\n        functions from the training data. ``None`` falls back to\n        ``\"grid\"``.\n    overlap : float, default=0.5\n        Desired overlap between neighbouring membership functions during\n        automatic construction.\n    margin : float, default=0.10\n        Extra range added around the observed feature minima/maxima when\n        performing grid initialization.\n    inputs_config : Mapping, optional\n        Per-feature overrides for membership configuration. Keys may be\n        feature names (e.g. when ``X`` is a :class:`pandas.DataFrame`),\n        integer indices, or ``\"x{i}\"`` aliases. Values accept dictionaries\n        mirroring builder arguments, explicit membership function lists, or\n        scalars for simple overrides. ``None`` entries keep defaults.\n    random_state : int, optional\n        Seed propagated to stochastic components such as FCM-based\n        initialization and optimizers that rely on randomness.\n    optimizer : str | BaseTrainer | type[BaseTrainer] | None, default=\"hybrid\"\n        Trainer identifier or instance used for fitting. String aliases are\n        looked up in :data:`TRAINER_REGISTRY`. ``None`` defaults to\n        ``\"hybrid\"``.\n    optimizer_params : Mapping, optional\n        Extra keyword arguments forwarded to the trainer constructor when a\n        string identifier or class is supplied.\n    learning_rate, epochs, batch_size, shuffle, verbose : optional\n        Convenience hyper-parameters that are injected into the selected\n        trainer when supported. ``shuffle`` accepts ``False`` to disable\n        randomisation.\n    loss : str | LossFunction, optional\n        Custom loss forwarded to trainers exposing a ``loss`` parameter.\n        ``None`` keeps the trainer default (typically mean squared error).\n    rules : Sequence[Sequence[int]] | None, optional\n        Optional explicit fuzzy rule definitions. Each rule lists the\n        membership index for every input. ``None`` uses the full Cartesian\n        product of configured membership functions.\n    \"\"\"\n    self.n_mfs = int(n_mfs)\n    self.mf_type = str(mf_type)\n    self.init = None if init is None else str(init)\n    self.overlap = float(overlap)\n    self.margin = float(margin)\n    self.inputs_config = dict(inputs_config) if inputs_config is not None else None\n    self.random_state = random_state\n    self.optimizer = optimizer\n    self.optimizer_params = dict(optimizer_params) if optimizer_params is not None else None\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    self.verbose = verbose\n    self.loss = loss\n    self.rules = None if rules is None else tuple(tuple(int(idx) for idx in rule) for rule in rules)\n\n    # Fitted attributes (initialised later)\n    self.model_: LowLevelANFIS | None = None\n    self.optimizer_: BaseTrainer | None = None\n    self.feature_names_in_: list[str] | None = None\n    self.n_features_in_: int | None = None\n    self.training_history_: TrainingHistory | None = None\n    self.input_specs_: list[dict[str, Any]] | None = None\n    self.rules_: list[tuple[int, ...]] | None = None\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    X,\n    y,\n    *,\n    return_dict: bool = True,\n    print_results: bool = False,\n)\n</code></pre> <p>Evaluate predictive performance on a dataset.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate--parameters","title":"Parameters","text":"<p>X : array-like     Evaluation inputs with shape <code>(n_samples, n_features)</code>. y : array-like     Ground-truth targets aligned with <code>X</code>. return_dict : bool, default=True     When <code>True</code>, return the computed metric dictionary. When     <code>False</code>, only perform side effects (such as printing) and return     <code>None</code>. print_results : bool, default=False     If <code>True</code>, log a small human-readable summary to stdout.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate--returns","title":"Returns:","text":"<p>dict[str, float] | None     Regression metrics including mean squared error, root mean squared     error, mean absolute error, and :math:<code>R^2</code> when <code>return_dict</code> is     <code>True</code>; otherwise <code>None</code>.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.evaluate--raises","title":"Raises:","text":"<p>RuntimeError     If called before <code>fit</code>. ValueError     When <code>X</code> and <code>y</code> disagree on the sample count.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def evaluate(self, X, y, *, return_dict: bool = True, print_results: bool = False):\n    \"\"\"Evaluate predictive performance on a dataset.\n\n    Parameters\n    ----------\n    X : array-like\n        Evaluation inputs with shape ``(n_samples, n_features)``.\n    y : array-like\n        Ground-truth targets aligned with ``X``.\n    return_dict : bool, default=True\n        When ``True``, return the computed metric dictionary. When\n        ``False``, only perform side effects (such as printing) and return\n        ``None``.\n    print_results : bool, default=False\n        If ``True``, log a small human-readable summary to stdout.\n\n    Returns:\n    -------\n    dict[str, float] | None\n        Regression metrics including mean squared error, root mean squared\n        error, mean absolute error, and :math:`R^2` when ``return_dict`` is\n        ``True``; otherwise ``None``.\n\n    Raises:\n    ------\n    RuntimeError\n        If called before ``fit``.\n    ValueError\n        When ``X`` and ``y`` disagree on the sample count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr, _ = _ensure_2d_array(X)\n    y_vec = _ensure_vector(y)\n    preds = self.predict(X_arr)\n    metrics = ANFISMetrics.regression_metrics(y_vec, preds)\n    if print_results:\n        quick = [\n            (\"MSE\", metrics[\"mse\"]),\n            (\"RMSE\", metrics[\"rmse\"]),\n            (\"MAE\", metrics[\"mae\"]),\n            (\"R2\", metrics[\"r2\"]),\n        ]\n        print(\"ANFISRegressor evaluation:\")  # noqa: T201\n        for name, value in quick:\n            print(f\"  {name:&gt;6}: {value:.6f}\")  # noqa: T201\n    return metrics if return_dict else None\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    *,\n    validation_data: tuple[ndarray, ndarray] | None = None,\n    validation_frequency: int = 1,\n    **fit_params: Any,\n)\n</code></pre> <p>Fit the ANFIS regressor on labelled data.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit--parameters","title":"Parameters","text":"<p>X : array-like     Training inputs with shape <code>(n_samples, n_features)</code>. y : array-like     Target values aligned with <code>X</code>. One-dimensional vectors are     accepted and reshaped internally. validation_data : tuple[np.ndarray, np.ndarray], optional     Optional validation split supplied to the underlying trainer. Both     arrays must already be numeric and share the same row count. validation_frequency : int, default=1     Frequency (in epochs) at which validation loss is evaluated when     <code>validation_data</code> is provided. **fit_params : Any     Arbitrary keyword arguments forwarded to the trainer <code>fit</code>     method.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit--returns","title":"Returns:","text":"<p>ANFISRegressor     Reference to <code>self</code> for fluent-style chaining.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.fit--raises","title":"Raises:","text":"<p>ValueError     If <code>X</code> and <code>y</code> contain a different number of samples. ValueError     If validation frequency is less than one. TypeError     If the configured trainer returns an object that is not a     <code>dict</code>-like training history.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    *,\n    validation_data: tuple[np.ndarray, np.ndarray] | None = None,\n    validation_frequency: int = 1,\n    **fit_params: Any,\n):\n    \"\"\"Fit the ANFIS regressor on labelled data.\n\n    Parameters\n    ----------\n    X : array-like\n        Training inputs with shape ``(n_samples, n_features)``.\n    y : array-like\n        Target values aligned with ``X``. One-dimensional vectors are\n        accepted and reshaped internally.\n    validation_data : tuple[np.ndarray, np.ndarray], optional\n        Optional validation split supplied to the underlying trainer. Both\n        arrays must already be numeric and share the same row count.\n    validation_frequency : int, default=1\n        Frequency (in epochs) at which validation loss is evaluated when\n        ``validation_data`` is provided.\n    **fit_params : Any\n        Arbitrary keyword arguments forwarded to the trainer ``fit``\n        method.\n\n    Returns:\n    -------\n    ANFISRegressor\n        Reference to ``self`` for fluent-style chaining.\n\n    Raises:\n    ------\n    ValueError\n        If ``X`` and ``y`` contain a different number of samples.\n    ValueError\n        If validation frequency is less than one.\n    TypeError\n        If the configured trainer returns an object that is not a\n        ``dict``-like training history.\n    \"\"\"\n    X_arr, feature_names = _ensure_2d_array(X)\n    y_vec = _ensure_vector(y)\n    if X_arr.shape[0] != y_vec.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples.\")\n\n    self.feature_names_in_ = feature_names\n    self.n_features_in_ = X_arr.shape[1]\n    self.input_specs_ = self._resolve_input_specs(feature_names)\n\n    _ensure_training_logging(self.verbose)\n    self.model_ = self._build_model(X_arr, feature_names)\n    trainer = self._instantiate_trainer()\n    self.optimizer_ = trainer\n    trainer_kwargs: dict[str, Any] = dict(fit_params)\n    if validation_data is not None:\n        trainer_kwargs.setdefault(\"validation_data\", validation_data)\n    if validation_data is not None or validation_frequency != 1:\n        trainer_kwargs.setdefault(\"validation_frequency\", validation_frequency)\n\n    history = trainer.fit(self.model_, X_arr, y_vec, **trainer_kwargs)\n    if not isinstance(history, dict):\n        raise TypeError(\"Trainer.fit must return a TrainingHistory dictionary\")\n    self.training_history_ = history\n    self.rules_ = self.model_.rules\n\n    self._mark_fitted()\n    return self\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.get_rules","title":"get_rules","text":"<pre><code>get_rules() -&gt; tuple[tuple[int, ...], ...]\n</code></pre> <p>Return the fuzzy rule index combinations used by the fitted model.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.get_rules--returns","title":"Returns:","text":"<p>tuple[tuple[int, ...], ...]     Immutable tuple containing one tuple per fuzzy rule, where each     inner tuple lists the membership index chosen for each input.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.get_rules--raises","title":"Raises:","text":"<p>RuntimeError     If invoked before the estimator is fitted.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def get_rules(self) -&gt; tuple[tuple[int, ...], ...]:\n    \"\"\"Return the fuzzy rule index combinations used by the fitted model.\n\n    Returns:\n    -------\n    tuple[tuple[int, ...], ...]\n        Immutable tuple containing one tuple per fuzzy rule, where each\n        inner tuple lists the membership index chosen for each input.\n\n    Raises:\n    ------\n    RuntimeError\n        If invoked before the estimator is fitted.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"rules_\"])\n    if not self.rules_:\n        return ()\n    return tuple(tuple(rule) for rule in self.rules_)\n</code></pre>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict regression targets for the provided samples.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like     Samples to evaluate. Accepts one-dimensional arrays (interpreted as     a single sample) or matrices with shape <code>(n_samples, n_features)</code>.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict--returns","title":"Returns:","text":"<p>np.ndarray     Vector of predictions with shape <code>(n_samples,)</code>.</p>"},{"location":"api/regressor/#anfis_toolbox.regressor.ANFISRegressor.predict--raises","title":"Raises:","text":"<p>RuntimeError     If the estimator has not been fitted yet. ValueError     When the supplied samples do not match the fitted feature count.</p> Source code in <code>anfis_toolbox/regressor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict regression targets for the provided samples.\n\n    Parameters\n    ----------\n    X : array-like\n        Samples to evaluate. Accepts one-dimensional arrays (interpreted as\n        a single sample) or matrices with shape ``(n_samples, n_features)``.\n\n    Returns:\n    -------\n    np.ndarray\n        Vector of predictions with shape ``(n_samples,)``.\n\n    Raises:\n    ------\n    RuntimeError\n        If the estimator has not been fitted yet.\n    ValueError\n        When the supplied samples do not match the fitted feature count.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\"])\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n        feature_names = self.feature_names_in_ or [f\"x{i + 1}\" for i in range(X_arr.shape[1])]\n    else:\n        X_arr, feature_names = _ensure_2d_array(X)\n\n    if self.n_features_in_ is None:\n        raise RuntimeError(\"Model must be fitted before calling predict.\")\n    if X_arr.shape[1] != self.n_features_in_:\n        raise ValueError(f\"Feature mismatch: expected {self.n_features_in_}, got {X_arr.shape[1]}.\")\n\n    preds = self.model_.predict(X_arr)  # type: ignore[operator]\n    return np.asarray(preds, dtype=float).reshape(-1)\n</code></pre>"},{"location":"examples/classifier_basic/","title":"Classification","text":"In\u00a0[121]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn_samples = 500\ntheta = np.linspace(0, np.pi, n_samples // 2)\nx_out = np.c_[np.cos(theta), np.sin(theta)]\nx_in = np.c_[1 - np.cos(theta), 1 - np.sin(theta) - 0.5]\n\nX = np.vstack([x_out, x_in]) + np.random.normal(0, 0.1, (n_samples, 2))\ny = np.hstack([np.zeros(x_out.shape[0]), np.ones(x_in.shape[0])])\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\", alpha=0.7)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42) n_samples = 500 theta = np.linspace(0, np.pi, n_samples // 2) x_out = np.c_[np.cos(theta), np.sin(theta)] x_in = np.c_[1 - np.cos(theta), 1 - np.sin(theta) - 0.5]  X = np.vstack([x_out, x_in]) + np.random.normal(0, 0.1, (n_samples, 2)) y = np.hstack([np.zeros(x_out.shape[0]), np.ones(x_in.shape[0])])  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\", alpha=0.7) plt.show() In\u00a0[122]: Copied! <pre>train_ratio = 0.8\nidx = np.random.permutation(len(X))\nsplit = int(train_ratio * len(X))\nX_train, X_test = X[idx[:split]], X[idx[split:]]\ny_train, y_test = y[idx[:split]], y[idx[split:]]\n</pre> train_ratio = 0.8 idx = np.random.permutation(len(X)) split = int(train_ratio * len(X)) X_train, X_test = X[idx[:split]], X[idx[split:]] y_train, y_test = y[idx[:split]], y[idx[split:]] In\u00a0[123]: Copied! <pre>from anfis_toolbox import ANFISClassifier\n\nclassifier = ANFISClassifier(n_classes=2, optimizer=\"adam\", batch_size=64, epochs=500, random_state=0)\n</pre> from anfis_toolbox import ANFISClassifier  classifier = ANFISClassifier(n_classes=2, optimizer=\"adam\", batch_size=64, epochs=500, random_state=0) In\u00a0[124]: Copied! <pre>classifier.fit(X_train, y_train)\nresult = classifier.evaluate(X_test, y_test)\nresult\n</pre> classifier.fit(X_train, y_train) result = classifier.evaluate(X_test, y_test) result Out[124]: <pre>{'accuracy': 0.94,\n 'balanced_accuracy': 0.938301282051282,\n 'precision_macro': 0.9439935064935066,\n 'recall_macro': 0.938301282051282,\n 'f1_macro': 0.9411387873795479,\n 'precision_micro': 0.94,\n 'recall_micro': 0.94,\n 'f1_micro': 0.94,\n 'confusion_matrix': array([[43,  5],\n        [ 1, 51]]),\n 'classes': array([0, 1]),\n 'log_loss': nan}</pre> In\u00a0[125]: Copied! <pre>x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max()\ny_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()\n\nplus = .5 \nx_min -= plus\nx_max += plus\ny_min -= plus\ny_max += plus\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 160),\n    np.linspace(y_min, y_max, 160)\n)\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Calcular probabilidades para cada ponto do grid\nprobabilities = classifier.predict_proba(grid_points)[:, 1]\n\nplt.figure(figsize=(8, 6))\n\n# Plot decision boundary as a contourf\nZ = probabilities.reshape(xx.shape)\ncontour = plt.contourf(xx, yy, Z, levels=10, cmap=\"coolwarm\", alpha=0.5, vmin=0, vmax=1)\n\n# Plot training and test points\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", edgecolor=\"k\", label=\"train\", marker=\".\", s=60, alpha=0.5)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"coolwarm\", edgecolor=\"k\", label=\"test\", marker=\"o\", alpha=0.5)\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max() y_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()  plus = .5  x_min -= plus x_max += plus y_min -= plus y_max += plus xx, yy = np.meshgrid(     np.linspace(x_min, x_max, 160),     np.linspace(y_min, y_max, 160) ) grid_points = np.c_[xx.ravel(), yy.ravel()]  # Calcular probabilidades para cada ponto do grid probabilities = classifier.predict_proba(grid_points)[:, 1]  plt.figure(figsize=(8, 6))  # Plot decision boundary as a contourf Z = probabilities.reshape(xx.shape) contour = plt.contourf(xx, yy, Z, levels=10, cmap=\"coolwarm\", alpha=0.5, vmin=0, vmax=1)  # Plot training and test points plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", edgecolor=\"k\", label=\"train\", marker=\".\", s=60, alpha=0.5) plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"coolwarm\", edgecolor=\"k\", label=\"test\", marker=\"o\", alpha=0.5)  plt.xlabel(\"x1\") plt.ylabel(\"x2\") plt.legend() plt.tight_layout() plt.show()"},{"location":"examples/classifier_basic/#classification","title":"Classification\u00b6","text":""},{"location":"examples/classifier_basic/#1-synthetic-dataset","title":"1) Synthetic dataset\u00b6","text":"<p>In this section, we create a synthetic dataset for a binary classification problem using pure NumPy operations. The dataset consists of two interleaved semicircles, resembling the well-known \u201ctwo moons\u201d pattern, which is often used to test nonlinear classification algorithms. The coordinates of each moon are generated using trigonometric functions, and a small amount of Gaussian noise is added to make the problem more realistic. Finally, we visualize the dataset using Matplotlib, where each class is displayed in a different color to highlight the nonlinear decision boundary between them.</p>"},{"location":"examples/classifier_basic/#2-traintest-split","title":"2. Train\u2013Test Split\u00b6","text":"<p>To evaluate a model\u2019s ability to generalize, we divide the dataset into training and testing subsets using pure NumPy operations. This manual split mimics the functionality of train_test_split from scikit-learn but avoids any external dependencies. First, we shuffle the dataset indices to ensure randomization, then select a portion (e.g., 80%) for training and the remaining samples for testing. This separation allows us to train the model on one subset and assess its performance on unseen data, providing a fair measure of its predictive capability.</p>"},{"location":"examples/classifier_basic/#3-configuring-the-anfis-classifier","title":"3. Configuring the ANFIS Classifier\u00b6","text":"<p>In this step, we configure the ANFISClassifier by defining its main hyperparameters. The parameter n_classes specifies the number of output classes in the classification problem. We also define the initialization strategy for the fuzzy membership functions and use the Adam optimizer \u2014 an adaptive gradient-based optimization algorithm that combines the advantages of momentum and RMSProp for faster and more stable convergence. Finally, we set the number of training epochs, batch size, and random seed to ensure reproducible results.</p>"},{"location":"examples/classifier_basic/#4-training-the-model","title":"4. Training the Model\u00b6","text":"<p>In this stage, we train the ANFISClassifier using the training set. The <code>fit()</code> method iteratively updates the model parameters using the Adam optimizer to minimize the classification error. After training, we evaluate the model on the test data with the `evaluate() method, which computes several performance metrics, including accuracy, precision, recall, and F1-scores (both macro and micro averages). The resulting confusion matrix shows that the classifier correctly distinguishes between the two classes, achieving an overall accuracy of 94%, demonstrating strong generalization on unseen samples.</p>"},{"location":"examples/classifier_basic/#6-visualizing-the-decision-boundary","title":"6. Visualizing the Decision Boundary\u00b6","text":"<p>To better understand the classifier\u2019s behavior, we visualize the decision boundary learned by the ANFIS model. We create a fine grid of points covering the entire feature space and use the trained classifier to predict the class probabilities for each point. These probabilities are then plotted as a smooth color map, where the transition between colors illustrates the regions assigned to each class. Finally, the training and testing samples are overlaid on the same plot, allowing us to clearly see how well the decision surface separates the two classes and how the model generalizes to unseen data.</p>"},{"location":"examples/modeling/","title":"Modeling a Two-Input Nonlinear Function","text":"In\u00a0[6]: Copied! <pre>import numpy as np\n\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nx, y = np.meshgrid(x, y)\nz = np.sinc(x) * np.sinc(y)\n\nX_train, y_train = np.c_[x.ravel(), y.ravel()], z.ravel()\n</pre> import numpy as np  x = np.linspace(-10, 10, 100) y = np.linspace(-10, 10, 100) x, y = np.meshgrid(x, y) z = np.sinc(x) * np.sinc(y)  X_train, y_train = np.c_[x.ravel(), y.ravel()], z.ravel() In\u00a0[25]: Copied! <pre>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor(n_mfs=10, optimizer=\"hybrid_adam\", epochs=100, learning_rate=1e-3, init=\"fcm\")\nmodel.fit(X_train, y_train)\n_ = model.evaluate(X_train, y_train, print_results=True)\n</pre> from anfis_toolbox import ANFISRegressor  model = ANFISRegressor(n_mfs=10, optimizer=\"hybrid_adam\", epochs=100, learning_rate=1e-3, init=\"fcm\") model.fit(X_train, y_train) _ = model.evaluate(X_train, y_train, print_results=True) <pre>Epoch 1 - train_loss: 0.001527\nEpoch 2 - train_loss: 0.001527\nEpoch 3 - train_loss: 0.001526\nEpoch 4 - train_loss: 0.001526\nEpoch 5 - train_loss: 0.001525\nEpoch 6 - train_loss: 0.001525\nEpoch 7 - train_loss: 0.001524\nEpoch 8 - train_loss: 0.001523\nEpoch 9 - train_loss: 0.001523\nEpoch 10 - train_loss: 0.001522\nEpoch 11 - train_loss: 0.001522\nEpoch 12 - train_loss: 0.001521\nEpoch 13 - train_loss: 0.001521\nEpoch 14 - train_loss: 0.001520\nEpoch 15 - train_loss: 0.001520\nEpoch 16 - train_loss: 0.001519\nEpoch 17 - train_loss: 0.001519\nEpoch 18 - train_loss: 0.001518\nEpoch 19 - train_loss: 0.001518\nEpoch 20 - train_loss: 0.001517\nEpoch 21 - train_loss: 0.001517\nEpoch 22 - train_loss: 0.001516\nEpoch 23 - train_loss: 0.001516\nEpoch 24 - train_loss: 0.001515\nEpoch 25 - train_loss: 0.001514\nEpoch 26 - train_loss: 0.001514\nEpoch 27 - train_loss: 0.001513\nEpoch 28 - train_loss: 0.001513\nEpoch 29 - train_loss: 0.001512\nEpoch 30 - train_loss: 0.001512\nEpoch 31 - train_loss: 0.001511\nEpoch 32 - train_loss: 0.001511\nEpoch 33 - train_loss: 0.001510\nEpoch 34 - train_loss: 0.001510\nEpoch 35 - train_loss: 0.001509\nEpoch 36 - train_loss: 0.001509\nEpoch 37 - train_loss: 0.001508\nEpoch 38 - train_loss: 0.001508\nEpoch 39 - train_loss: 0.001507\nEpoch 40 - train_loss: 0.001507\nEpoch 41 - train_loss: 0.001506\nEpoch 42 - train_loss: 0.001505\nEpoch 43 - train_loss: 0.001505\nEpoch 44 - train_loss: 0.001504\nEpoch 45 - train_loss: 0.001504\nEpoch 46 - train_loss: 0.001503\nEpoch 47 - train_loss: 0.001503\nEpoch 48 - train_loss: 0.001502\nEpoch 49 - train_loss: 0.001502\nEpoch 50 - train_loss: 0.001501\nEpoch 51 - train_loss: 0.001501\nEpoch 52 - train_loss: 0.001500\nEpoch 53 - train_loss: 0.001500\nEpoch 54 - train_loss: 0.001499\nEpoch 55 - train_loss: 0.001499\nEpoch 56 - train_loss: 0.001498\nEpoch 57 - train_loss: 0.001498\nEpoch 58 - train_loss: 0.001497\nEpoch 59 - train_loss: 0.001496\nEpoch 60 - train_loss: 0.001496\nEpoch 61 - train_loss: 0.001495\nEpoch 62 - train_loss: 0.001495\nEpoch 63 - train_loss: 0.001494\nEpoch 64 - train_loss: 0.001494\nEpoch 65 - train_loss: 0.001493\nEpoch 66 - train_loss: 0.001493\nEpoch 67 - train_loss: 0.001492\nEpoch 68 - train_loss: 0.001492\nEpoch 69 - train_loss: 0.001491\nEpoch 70 - train_loss: 0.001491\nEpoch 71 - train_loss: 0.001490\nEpoch 72 - train_loss: 0.001490\nEpoch 73 - train_loss: 0.001489\nEpoch 74 - train_loss: 0.001488\nEpoch 75 - train_loss: 0.001488\nEpoch 76 - train_loss: 0.001487\nEpoch 77 - train_loss: 0.001487\nEpoch 78 - train_loss: 0.001486\nEpoch 79 - train_loss: 0.001486\nEpoch 80 - train_loss: 0.001485\nEpoch 81 - train_loss: 0.001485\nEpoch 82 - train_loss: 0.001484\nEpoch 83 - train_loss: 0.001484\nEpoch 84 - train_loss: 0.001483\nEpoch 85 - train_loss: 0.001482\nEpoch 86 - train_loss: 0.001482\nEpoch 87 - train_loss: 0.001481\nEpoch 88 - train_loss: 0.001481\nEpoch 89 - train_loss: 0.001480\nEpoch 90 - train_loss: 0.001480\nEpoch 91 - train_loss: 0.001479\nEpoch 92 - train_loss: 0.001479\nEpoch 93 - train_loss: 0.001478\nEpoch 94 - train_loss: 0.001477\nEpoch 95 - train_loss: 0.001477\nEpoch 96 - train_loss: 0.001476\nEpoch 97 - train_loss: 0.001476\nEpoch 98 - train_loss: 0.001475\nEpoch 99 - train_loss: 0.001475\nEpoch 100 - train_loss: 0.001474\nANFISRegressor evaluation:\n     MSE: 0.001473\n    RMSE: 0.038386\n     MAE: 0.012121\n      R2: 0.384851\n</pre> In\u00a0[23]: Copied! <pre>z_predict = model.predict(X_train).reshape(z.shape)\n</pre> z_predict = model.predict(X_train).reshape(z.shape) In\u00a0[24]: Copied! <pre>plot_surface(x, y, z_predict)\n</pre> plot_surface(x, y, z_predict) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/modeling/#modeling-a-two-input-nonlinear-function","title":"Modeling a Two-Input Nonlinear Function\u00b6","text":""},{"location":"examples/modeling/#1-task-definition","title":"1. Task Definition\u00b6","text":"<p>The task is to construct an ANFIS model that approximates the two-input nonlinear <code>sinc</code> function, defined as: $$z=sinc(x,y)=\\frac{sin(x)}{x}\\times\\frac{sin(y)}{y}$$</p>"},{"location":"examples/modeling/#2-dataset-generation","title":"2. Dataset Generation\u00b6","text":"<p>The dataset consists of 100 input-output data pairs. These pairs are generated from the grid points of the input space defined by the range $[-10,10] \\times [-10,10]$.</p>"},{"location":"examples/modeling/#2-build-train-and-evaluate-anfis","title":"2) Build, train, and evaluate ANFIS\u00b6","text":"<p>Instantiate <code>ANFISRegressor</code> with Gaussian membership functions and the hybrid trainer:</p> <ul> <li>membership functions are inferred directly from the data;</li> <li><code>fit</code> tunes both antecedent and consequent parameters;</li> <li>we call <code>evaluate</code> on the fitted low-level model for a compact metric report.</li> </ul>"},{"location":"examples/modeling/#3-visualize-predictions","title":"3) Visualize predictions\u00b6","text":""},{"location":"examples/regression_basic/","title":"Regression","text":"<p>We import NumPy and helper utilities from ANFIS-Toolbox, then generate a simple noisy sine dataset for regression:</p> <ul> <li>Inputs <code>X</code> are evenly spaced in [-\u03c0, \u03c0].</li> <li>Targets <code>y</code> follow <code>sin(x)</code> with Gaussian noise. This small problem is ideal to showcase ANFIS function approximation.</li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\n\nnp.random.seed(42)  # For reproducibility\n\nn = 200\nX = np.linspace(-np.pi, np.pi, n).reshape(-1, 1)\ny = np.sin(X[:, 0]) + 0.2 * np.random.randn(n)\ny = y.reshape(-1, 1)\n</pre> import numpy as np  np.random.seed(42)  # For reproducibility  n = 200 X = np.linspace(-np.pi, np.pi, n).reshape(-1, 1) y = np.sin(X[:, 0]) + 0.2 * np.random.randn(n) y = y.reshape(-1, 1) In\u00a0[\u00a0]: Copied! <pre>from anfis_toolbox import ANFISRegressor\n\nmodel = ANFISRegressor().fit(X, y)\n</pre> from anfis_toolbox import ANFISRegressor  model = ANFISRegressor().fit(X, y) In\u00a0[18]: Copied! <pre>import matplotlib.pyplot as plt\n\nx_flat = X[:, 0]\ny_true = y[:, 0]\ny_pred = model.predict(X)\n\nplt.figure(figsize=(6, 3))\nplt.scatter(x_flat, y_true, s=20, alpha=0.25, label=\"Training samples\")\nplt.plot(x_flat, y_pred, linewidth=2, label=\"ANFIS prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"ANFIS regression on noisy sine wave\")\nplt.grid(alpha=0.2)\nplt.legend()\nplt.show()\n</pre> import matplotlib.pyplot as plt  x_flat = X[:, 0] y_true = y[:, 0] y_pred = model.predict(X)  plt.figure(figsize=(6, 3)) plt.scatter(x_flat, y_true, s=20, alpha=0.25, label=\"Training samples\") plt.plot(x_flat, y_pred, linewidth=2, label=\"ANFIS prediction\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.title(\"ANFIS regression on noisy sine wave\") plt.grid(alpha=0.2) plt.legend() plt.show()"},{"location":"examples/regression_basic/#regression","title":"Regression\u00b6","text":""},{"location":"examples/regression_basic/#1-synthetic-dataset","title":"1) Synthetic dataset\u00b6","text":""},{"location":"examples/regression_basic/#2-build-train-and-evaluate-anfis","title":"2) Build, train, and evaluate ANFIS\u00b6","text":"<p>Instantiate <code>ANFISRegressor</code> with Gaussian membership functions and the hybrid trainer:</p> <ul> <li>membership functions are inferred directly from the data;</li> <li><code>fit</code> tunes both antecedent and consequent parameters;</li> <li>we call <code>quick_evaluate</code> on the fitted low-level model for a compact metric report.</li> </ul>"},{"location":"examples/regression_basic/#3-visualize-regression-predictions","title":"3) Visualize regression predictions\u00b6","text":"<p>Compare the noisy samples against the ANFIS regression curve to gauge fit quality.</p>"},{"location":"hooks/test_hook/","title":"Test hook","text":"In\u00a0[\u00a0]: Copied! <pre>def on_post_page(output, page, config):\n    path = str(page.file.src_uri)\n    if not path.endswith(\".ipynb\"):\n        return output\n\n    output = output.replace(\n        \"\"\"&lt;div class=\"highlight-ipynb hl-python\"&gt;\"\"\",\n        \"\"\"&lt;div class=\"language-python highlight\"&gt;\"\"\"\n        )\n\n    return output\n</pre> def on_post_page(output, page, config):     path = str(page.file.src_uri)     if not path.endswith(\".ipynb\"):         return output      output = output.replace(         \"\"\"\"\"\",         \"\"\"\"\"\"         )      return output"},{"location":"membership_functions/bell/","title":"Bell-shaped","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox import BellMF\n\nbellmf = BellMF(a=2, b=4, c=5)\n\nx = np.linspace(0, 10, 100)\ny = bellmf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox import BellMF  bellmf = BellMF(a=2, b=4, c=5)  x = np.linspace(0, 10, 100) y = bellmf(x)  plt.plot(x, y) plt.show() <p>The image displays a series of symmetrical bell-shaped curves. You can observe how increasing the width ($a$) makes the curve broader, while increasing the slope ($b$) makes the sides of the curve steeper.</p>"},{"location":"membership_functions/bell/#bell-shaped","title":"Bell-shaped\u00b6","text":"<p>The Generalized Bell membership function (BellMF), also known as the Bell-shaped curve, is a versatile function used in fuzzy logic to define fuzzy sets. Like other membership functions, it assigns a degree of membership to an element, but it offers greater flexibility than the GaussianMF by using an additional parameter to control its shape. Its form is a smooth, symmetrical bell curve.</p> <p>The function is defined by three parameters:</p> <ul> <li>center ($c$): This parameter determines the center of the curve, representing the point in the domain with a maximum membership value of 1.</li> <li>width ($a$): This parameter controls the width or spread of the curve. A larger value of $a$ results in a wider curve, while a smaller value produces a narrower curve.</li> <li>slope ($b$): This parameter, which must be a positive value, determines the slope of the curve's sides. It directly impacts the steepness of the curve's transition from 0 to 1. A larger $b$ value creates a steeper curve, making the fuzzy set sharper and less \"fuzzy.\"</li> </ul> <p>The mathematical formula for the Generalized Bell membership function is given by:</p> <p>$$\\mu(x) = \\frac{1}{1 + \\left|\\frac{x-c}{a}\\right|^{2b}}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$c$ is the center of the curve.</li> <li>$a$ is the width of the curve.</li> <li>$b$ is the slope of the curve.</li> </ul>"},{"location":"membership_functions/bell/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the GbellMF are essential for training and optimizing fuzzy systems. They show how the membership value changes with respect to small changes in each of the three parameters, which is vital for algorithms like backpropagation used in fuzzy-neural networks.</p> Derivative with respect to the center ($c$) <p>The partial derivative of the function with respect to its center ($c$) is:</p> <p>$$\\frac{\\partial f}{\\partial c} = \\frac{2b(x-c)}{a^2} \\left(1 + \\left|\\frac{x-c}{a}\\right|^{2b}\\right)^{-2}$$</p> <p>This derivative indicates how the membership value changes when the curve is shifted along the x-axis.</p> Derivative with respect to the width ($a$) <p>The partial derivative with respect to the width ($a$) is:</p> <p>$$\\frac{\\partial f}{\\partial a} = \\frac{2b(x-c)^2}{a^3} \\left(1 + \\left|\\frac{x-c}{a}\\right|^{2b}\\right)^{-2}$$</p> <p>This derivative helps in adjusting the spread of the fuzzy set to encompass a broader or narrower range of values.</p> Derivative with respect to the slope ($b$) <p>The partial derivative with respect to the slope ($b$) is:</p> <p>$$\\frac{\\partial f}{\\partial b} = -\\frac{2}{b} \\frac{(x-c)^2}{a^2} \\left(\\frac{1}{1 + \\left|\\frac{x-c}{a}\\right|^{2b}}\\right)^2 \\ln\\left|\\frac{x-c}{a}\\right|$$</p> <p>This derivative is used to modify the steepness of the curve, allowing for fine-tuning of the transition from non-membership to full membership.</p>"},{"location":"membership_functions/bell/#python-example","title":"Python Example\u00b6","text":"<p>The following code demonstrates how to generate a Generalized Bell membership function using the <code>numpy</code> and <code>matplotlib</code> libraries in Python.</p>"},{"location":"membership_functions/bell/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of a Generalized Bell membership function, showing how its shape is influenced by the width ($a$) and slope ($b$) parameters, while the center ($c$) remains fixed.</p>"},{"location":"membership_functions/gaussian-combination/","title":"Gaussian Combination","text":"<p>The Gaussian combination membership function (Gaussian2MF) is a versatile fuzzy membership function that combines two Gaussian curves with an optional flat region in between. This function is particularly useful for representing fuzzy sets with asymmetric shapes or plateau regions, making it suitable for applications where the membership degree needs to be constant over a range of values.</p> <p>The function is characterized by four main parameters:</p> <ul> <li>sigma1 ($\\sigma_1$): Standard deviation of the left Gaussian tail (must be positive).</li> <li>c1 ($c_1$): Center of the left Gaussian tail.</li> <li>sigma2 ($\\sigma_2$): Standard deviation of the right Gaussian tail (must be positive).</li> <li>c2 ($c_2$): Center of the right Gaussian tail. Must satisfy $c_1 \\leq c_2$.</li> </ul> <p>The mathematical formula for the Gaussian combination membership function is defined piecewise:</p> <p>$$\\mu(x) =  \\begin{array}{ll}  e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}} &amp; x &lt; c_1 \\\\[6pt]  1 &amp; c_1 \\leq x \\leq c_2 \\\\[6pt]  e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}} &amp; x &gt; c_2  \\end{array}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$\\sigma_1, c_1$ define the left Gaussian tail.</li> <li>$\\sigma_2, c_2$ define the right Gaussian tail.</li> </ul> <p>When $c_1 = c_2$, the function becomes an asymmetric Gaussian centered at $c_1$ with different spreads on each side.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import Gaussian2MF\n\n\ngaussian2 = Gaussian2MF(sigma1=.5, c1=2, sigma2=1, c2=5)\n\nx = np.linspace(0, 10, 100)\ny = gaussian2(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import Gaussian2MF   gaussian2 = Gaussian2MF(sigma1=.5, c1=2, sigma2=1, c2=5)  x = np.linspace(0, 10, 100) y = gaussian2(x)  plt.plot(x, y) plt.show() <p>The visualization above demonstrates various configurations of the Gaussian2MF. This flexibility makes Gaussian2MF suitable for modeling complex fuzzy concepts with asymmetric uncertainty or plateau regions where membership should remain constant.</p>"},{"location":"membership_functions/gaussian-combination/#gaussian-combination","title":"Gaussian Combination\u00b6","text":""},{"location":"membership_functions/gaussian-combination/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the Gaussian combination membership function are essential for optimization in adaptive fuzzy systems. Since the function is piecewise, derivatives are computed separately for each region.</p> Derivative with respect to $c_1$ <p>For $x &lt; c_1$: $$\\frac{\\partial \\mu}{\\partial c_1} = \\frac{x-c_1}{\\sigma_1^2} e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}}$$</p> <p>For $c_1 \\leq x \\leq c_2$: The derivative is 0 (flat region).</p> <p>For $x &gt; c_2$: The derivative is 0.</p> Derivative with respect to $\\sigma_1$ <p>For $x &lt; c_1$: $$\\frac{\\partial \\mu}{\\partial \\sigma_1} = \\frac{(x-c_1)^2}{\\sigma_1^3} e^{-\\frac{(x-c_1)^2}{2\\sigma_1^2}}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $c_2$ <p>For $x &lt; c_1$: The derivative is 0.</p> <p>For $c_1 \\leq x \\leq c_2$: The derivative is 0.</p> <p>For $x &gt; c_2$: $$\\frac{\\partial \\mu}{\\partial c_2} = \\frac{x-c_2}{\\sigma_2^2} e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}}$$</p> Derivative with respect to $\\sigma_2$ <p>For $x &gt; c_2$: $$\\frac{\\partial \\mu}{\\partial \\sigma_2} = \\frac{(x-c_2)^2}{\\sigma_2^3} e^{-\\frac{(x-c_2)^2}{2\\sigma_2^2}}$$</p> <p>For other regions: The derivative is 0.</p> <p>These derivatives enable gradient-based optimization of the membership function parameters.</p>"},{"location":"membership_functions/gaussian-combination/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/gaussian-combination/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the Gaussian2MF shape changes with different parameter combinations. We'll explore variations in the centers (c\u2081, c\u2082) and standard deviations (\u03c3\u2081, \u03c3\u2082).</p>"},{"location":"membership_functions/gaussian/","title":"Gaussian","text":"<p>The Gaussian membership function (GaussianMF) is a fundamental concept in fuzzy logic, widely used to define fuzzy sets. Unlike a classical set where an element either fully belongs or does not belong, a fuzzy set allows for partial membership, and the GaussianMF provides a smooth, continuous way to represent this degree of belonging. It possesses a smooth, bell-like shape, which contributes to its intuitive nature and popularity across various applications.</p> <p>The function is characterized by two main parameters:</p> <ul> <li>mean ($\\mu$): This parameter determines the center of the curve. It represents the point in the domain where the degree of membership is maximum, specifically 1.</li> <li>sigma ($\\sigma$): This parameter controls the width or spread of the curve. It must be a positive value. A larger $\\sigma$ results in a wider, flatter curve, indicating a broader range of values with high membership. Conversely, a smaller $\\sigma$ produces a sharper, more peaked curve, suggesting a narrower range of values with a high degree of belonging.</li> </ul> <p>The mathematical formula for the Gaussian membership function is given by:</p> <p>$$\\mu(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$\\mu$ is the mean of the curve.</li> <li>$\\sigma$ is the standard deviation (width) of the curve.</li> </ul> <p>The partial derivatives of the Gaussian membership function are crucial for optimization algorithms, especially in adaptive or machine learning-based fuzzy systems. They show how the membership value changes in response to small adjustments to the parameters, which is essential for training models to better fit data.</p> Derivative with respect to $\\mu$ <p>The partial derivative of the function with respect to the mean ($\\mu$) is:</p> <p>$$\\frac{\\partial f}{\\partial \\mu} = \\frac{x-\\mu}{\\sigma^2} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>This derivative indicates how the membership value is affected when the center of the bell curve is shifted. It is used to adjust the position of the function to better align with the data.</p> Derivative with respect to $\\sigma$** <p>The partial derivative with respect to the standard deviation ($\\sigma$) is:</p> <p>$$\\frac{\\partial f}{\\partial \\sigma} = \\frac{(x-\\mu)^2}{\\sigma^3} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>This derivative shows how the membership value changes as the width of the curve is adjusted. It is used to refine the spread of the function, making it sharper or wider as needed to represent the uncertainty in the data more accurately.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox import GaussianMF\n\ngaussian = GaussianMF(5, 1)\n\nx = np.linspace(0, 10, 100)\ny = gaussian(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox import GaussianMF  gaussian = GaussianMF(5, 1)  x = np.linspace(0, 10, 100) y = gaussian(x)  plt.plot(x, y) plt.show() <p>Below is a visual representation of a Gaussian membership function, showing how its shape is influenced by the mean $\\mu$ and sigma ($\\sigma$) parameters.</p> <p>The image displays a classic bell-shaped curve, illustrating how the membership value (on the y-axis) smoothly changes for different input values (on the x-axis). The peak of the curve is located at the mean, and the spread of the curve is controlled by sigma.</p>"},{"location":"membership_functions/gaussian/#gaussian","title":"Gaussian\u00b6","text":""},{"location":"membership_functions/gaussian/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"membership_functions/gaussian/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/gaussian/#visualization","title":"Visualization\u00b6","text":""},{"location":"membership_functions/pi/","title":"Pi-shaped","text":"<p>The Pi-shaped Membership Function is defined piecewise with smooth transitions:</p> <p>$$\\mu(x) = \\begin{array}{ll}  S(x; a, b) &amp; a \\leq x \\leq b \\\\[6pt] 1 &amp; b \\leq x \\leq c \\\\[6pt] Z(x; c, d) &amp; c \\leq x \\leq d \\\\[6pt] 0 &amp; \\text{otherwise} \\end{array}$$</p> <p>Where:</p> <ul> <li>S(x; a, b) is the rising S-shaped function: smooth transition from 0 to 1</li> <li>Z(x; c, d) is the falling Z-shaped function: smooth transition from 1 to 0</li> </ul> <p>The smooth transitions use cubic smoothstep functions:</p> <p>S-function (rising edge): $$S(x; a, b) = 3t^2 - 2t^3 \\quad \\text{where} \\quad t = \\frac{x - a}{b - a}$$</p> <p>Z-function (falling edge): $$Z(x; c, d) = 1 - S(x; c, d) = 1 - (3t^2 - 2t^3) \\quad \\text{where} \\quad t = \\frac{x - c}{d - c}$$</p> <p>The function is characterized by four parameters:</p> <ul> <li>a: Left foot - where the function starts rising from 0</li> <li>b: Left shoulder - where the function reaches the plateau (\u03bc = 1)</li> <li>c: Right shoulder - where the function starts falling from the plateau</li> <li>d: Right foot - where the function reaches 0</li> </ul> <p>For a valid Pi-shaped function, parameters must satisfy: a &lt; b \u2264 c &lt; d</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import PiMF\n\npimf = PiMF(a=1, b=3, c=7, d=9)\n\nx = np.linspace(0, 10, 200)\ny = pimf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import PiMF  pimf = PiMF(a=1, b=3, c=7, d=9)  x = np.linspace(0, 10, 200) y = pimf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/pi/#pi-shaped","title":"Pi-shaped\u00b6","text":"<p>The Pi-shaped Membership Function creates a smooth bell-like curve with a flat plateau at the top. It combines S-shaped rising and Z-shaped falling edges with a trapezoidal-like plateau, making it ideal for representing concepts with gradual transitions and stable regions.</p>"},{"location":"membership_functions/pi/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The PiMF has analytical gradients computed by region:</p> S-Function Region (a \u2264 x \u2264 b) For the rising edge: \u03bc(x) = S(x; a, b) = 3t\u00b2 - 2t\u00b3 where t = (x-a)/(b-a)  <ul> <li>\u2202\u03bc/\u2202a = dS/dt \u00b7 dt/da = [6t(1-t)] \u00b7 [(x-b)/(b-a)\u00b2]</li> <li>\u2202\u03bc/\u2202b = dS/dt \u00b7 dt/db = [6t(1-t)] \u00b7 [-(x-a)/(b-a)\u00b2]</li> <li>\u2202\u03bc/\u2202c = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202d = 0 (no effect in this region)</li> </ul> Plateau Region (b \u2264 x \u2264 c) \u03bc(x) = 1 (constant function)  <ul> <li>\u2202\u03bc/\u2202a = 0 (constant function)</li> <li>\u2202\u03bc/\u2202b = 0 (constant function)</li> <li>\u2202\u03bc/\u2202c = 0 (constant function)</li> <li>\u2202\u03bc/\u2202d = 0 (constant function)</li> </ul> Z-Function Region (c \u2264 x \u2264 d) For the falling edge: \u03bc(x) = Z(x; c, d) = 1 - S(x; c, d) where t = (x-c)/(d-c)  <ul> <li>\u2202\u03bc/\u2202a = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202b = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202c = dZ/dt \u00b7 dt/dc = [-6t(1-t)] \u00b7 [(x-d)/(d-c)\u00b2]</li> <li>\u2202\u03bc/\u2202d = dZ/dt \u00b7 dt/dd = [-6t(1-t)] \u00b7 [-(x-c)/(d-c)\u00b2]</li> </ul>"},{"location":"membership_functions/pi/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a Pi-shaped membership function and visualize it:</p>"},{"location":"membership_functions/pi/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different Pi-shaped membership functions with varying parameter combinations. Each subplot demonstrates how the plateau width and transition smoothness affect the overall shape.</p>"},{"location":"membership_functions/sigmoidal-difference/","title":"Difference of Sigmoidal","text":"<p>The Difference of Sigmoidal Membership Functions is defined as:</p> <p>$$\\mu(x) = s_1(x) - s_2(x)$$</p> <p>Where each sigmoid function is:</p> <p>$$s_1(x) = \\frac{1}{1 + e^{-a_1(x - c_1)}}$$ $$s_2(x) = \\frac{1}{1 + e^{-a_2(x - c_2)}}$$</p> <p>The function is characterized by four parameters (two for each sigmoid):</p> <ul> <li><p>a\u2081: Slope parameter for the first sigmoid (s\u2081)</p> <ul> <li>Positive values: standard sigmoid (0 \u2192 1 as x increases)</li> <li>Negative values: inverted sigmoid (1 \u2192 0 as x increases)</li> <li>Larger |a\u2081|: steeper transition for s\u2081</li> </ul> </li> <li><p>c\u2081: Center parameter for the first sigmoid (s\u2081)</p> <ul> <li>Controls the inflection point where s\u2081(c\u2081) = 0.5</li> <li>Shifts s\u2081 left/right along the x-axis</li> </ul> </li> <li><p>a\u2082: Slope parameter for the second sigmoid (s\u2082)</p> <ul> <li>Same interpretation as a\u2081 but for s\u2082</li> </ul> </li> <li><p>c\u2082: Center parameter for the second sigmoid (s\u2082)</p> <ul> <li>Same interpretation as c\u2081 but for s\u2082</li> </ul> </li> <li><p>a\u2081 \u2260 0 and a\u2082 \u2260 0: Cannot be zero (would result in constant functions)</p> </li> <li><p>All parameters can be any real numbers otherwise</p> </li> </ul> In\u00a0[5]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import DiffSigmoidalMF\n\ndiff_sigmoid = DiffSigmoidalMF(a1=2, c1=4, a2=5, c2=8)\n\nx = np.linspace(0, 10, 100)\ny = diff_sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import DiffSigmoidalMF  diff_sigmoid = DiffSigmoidalMF(a1=2, c1=4, a2=5, c2=8)  x = np.linspace(0, 10, 100) y = diff_sigmoid(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal-difference/#difference-of-sigmoidal","title":"Difference of Sigmoidal\u00b6","text":"<p>The Difference of Sigmoidal Membership Functions implements \u03bc(x) = s\u2081(x) - s\u2082(x), where each s is a logistic curve with its own slope and center parameters. This creates complex membership shapes by combining two sigmoid functions.</p>"},{"location":"membership_functions/sigmoidal-difference/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. Since \u03bc(x) = s\u2081(x) - s\u2082(x), the derivatives follow from the chain rule:</p> Derivative w.r.t. Parameters of First Sigmoid (s\u2081) <p>For s\u2081(x) = 1/(1 + exp(-a\u2081(x - c\u2081))):</p> <ul> <li>\u2202\u03bc/\u2202a\u2081 = \u2202s\u2081/\u2202a\u2081 = s\u2081(x) \u00b7 (1 - s\u2081(x)) \u00b7 (x - c\u2081)</li> <li>\u2202\u03bc/\u2202c\u2081 = \u2202s\u2081/\u2202c\u2081 = -a\u2081 \u00b7 s\u2081(x) \u00b7 (1 - s\u2081(x))</li> </ul> Derivative w.r.t. Parameters of Second Sigmoid (s\u2082) <p>For s\u2082(x) = 1/(1 + exp(-a\u2082(x - c\u2082))), and since \u03bc(x) = s\u2081(x) - s\u2082(x):</p> <ul> <li>\u2202\u03bc/\u2202a\u2082 = -\u2202s\u2082/\u2202a\u2082 = -s\u2082(x) \u00b7 (1 - s\u2082(x)) \u00b7 (x - c\u2082)</li> <li>\u2202\u03bc/\u2202c\u2082 = -\u2202s\u2082/\u2202c\u2082 = -(-a\u2082 \u00b7 s\u2082(x) \u00b7 (1 - s\u2082(x))) = a\u2082 \u00b7 s\u2082(x) \u00b7 (1 - s\u2082(x))</li> </ul> Derivative w.r.t. Input (Optional) <p>For chaining in neural networks:</p> <p>$$\\frac{d\\mu}{dx} = \\frac{ds_1}{dx} - \\frac{ds_2}{dx}$$</p> <p>Where: $$\\frac{ds_1}{dx} = a_1 \\cdot s_1(x) \\cdot (1 - s_1(x))$$ $$\\frac{ds_2}{dx} = a_2 \\cdot s_2(x) \\cdot (1 - s_2(x))$$</p> Gradient Computation Details <p>The gradients are computed using the fundamental sigmoid derivative property: $$\\frac{d}{dx}\\left(\\frac{1}{1+e^{-z}}\\right) = \\frac{1}{1+e^{-z}} \\cdot \\left(1 - \\frac{1}{1+e^{-z}}\\right) = s(x) \\cdot (1 - s(x))$$</p> <p>This property is used extensively in neural network backpropagation and makes the DiffSigmoidalMF computationally efficient for optimization.</p>"},{"location":"membership_functions/sigmoidal-difference/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a difference of sigmoidal membership functions and visualize its components:</p>"},{"location":"membership_functions/sigmoidal-difference/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different difference of sigmoidal membership functions with varying parameter combinations. Each subplot demonstrates how the combination of two sigmoids creates complex membership shapes.</p>"},{"location":"membership_functions/sigmoidal-product/","title":"Product of Sigmoidal","text":"<p>The function is defined as the product of two sigmoidal functions:</p> <p>$$\\mu(x) = f_1(x) \\cdot f_2(x)$$ $$f_1(x) = \\frac{1}{1 + e^{-a_1(x-c_1)}}$$ $$f_2(x) = \\frac{1}{1 + e^{-a_2(x-c_2)}}$$</p> <p>The function is controlled by four parameters, two for each component sigmoid:</p> <ul> <li><p>$a_1$: Slope of the first sigmoid function.</p> </li> <li><p>$c_1$: Center (inflection point) of the first sigmoid function.</p> </li> <li><p>$a_2$: Slope of the second sigmoid function.</p> </li> <li><p>$c_2$: Center (inflection point) of the second sigmoid function.</p> </li> <li><p>To form a proper bell-shaped curve, the slopes $a_1$ and $a_2$ must have opposite signs (e.g., if $a_1 &gt; 0$, then $a_2 &lt; 0$).</p> </li> <li><p>The centers $c_1$ and $c_2$ determine the width and position of the curve's peak.</p> </li> </ul> In\u00a0[12]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import ProdSigmoidalMF\n\nmf = ProdSigmoidalMF(a1=2, c1=3, a2=-2, c2=4)\n\nx = np.linspace(0, 10, 400)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import ProdSigmoidalMF  mf = ProdSigmoidalMF(a1=2, c1=3, a2=-2, c2=4)  x = np.linspace(0, 10, 400) y = mf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal-product/#product-of-sigmoidal","title":"Product of Sigmoidal\u00b6","text":"<p>The Product of Sigmoidal Membership Function creates a smooth, asymmetrical, bell-shaped curve by multiplying two distinct sigmoidal functions. It's used to model fuzzy sets that require a gradual but non-uniform transition, offering more flexibility than symmetric functions.</p>"},{"location":"membership_functions/sigmoidal-product/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives with respect to the parameters $a_1, c_1, a_2,$ and $c_2$ are:</p> <ol> <li><p>With respect to $a_1$: $$\\frac{\\partial \\mu}{\\partial a_1} = (x-c_1) \\cdot (1 - f_1(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $c_1$: $$\\frac{\\partial \\mu}{\\partial c_1} = -a_1 \\cdot (1 - f_1(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $a_2$: $$\\frac{\\partial \\mu}{\\partial a_2} = (x-c_2) \\cdot (1 - f_2(x)) \\cdot \\mu(x)$$</p> </li> <li><p>With respect to $c_2$: $$\\frac{\\partial \\mu}{\\partial c_2} = -a_2 \\cdot (1 - f_2(x)) \\cdot \\mu(x)$$</p> </li> </ol>"},{"location":"membership_functions/sigmoidal-product/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/sigmoidal-product/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the ProdSigmoidalMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/sigmoidal/","title":"Sigmoidal","text":"<p>The Sigmoidal Membership Function is defined by the logistic function:</p> <p>$$\\mu(x) = \\frac{1}{1 + e^{-a(x - c)}}$$</p> <p>The function is characterized by two parameters:</p> <ul> <li><p>a: Slope parameter - controls the steepness of the S-curve</p> <ul> <li>Positive values: standard sigmoid (0 \u2192 1 as x increases)</li> <li>Negative values: inverted sigmoid (1 \u2192 0 as x increases)</li> <li>Larger |a|: steeper transition (more abrupt change)</li> <li>Smaller |a|: gentler transition (more gradual change)</li> </ul> </li> <li><p>c: Center parameter - controls the inflection point where \u03bc(c) = 0.5</p> <ul> <li>\u03bc(c) = 0.5 (50% membership)</li> <li>Shifts the curve left/right along the x-axis</li> </ul> </li> <li><p>a \u2260 0: Cannot be zero (would result in constant function \u03bc(x) = 0.5)</p> </li> <li><p>a and c can be any real numbers otherwise</p> </li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import SigmoidalMF\n\nsigmoid = SigmoidalMF(a=2, c=0)\n\nx = np.linspace(-5, 5, 200)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import SigmoidalMF  sigmoid = SigmoidalMF(a=2, c=0)  x = np.linspace(-5, 5, 200) y = sigmoid(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sigmoidal/#sigmoidal","title":"Sigmoidal\u00b6","text":"<p>The Sigmoidal Membership Function implements a smooth S-shaped curve that transitions gradually from 0 to 1. It is widely used in fuzzy logic systems and neural networks for modeling smooth transitions and gradual changes.</p>"},{"location":"membership_functions/sigmoidal/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The sigmoid function has elegant derivative properties:</p> Derivative of the Sigmoid Function For \u03bc(x) = 1/(1 + e^{-a(x-c)}), the derivative with respect to x is:  <p>$$\\frac{d\\mu}{dx} = \\mu(x) \\cdot (1 - \\mu(x)) \\cdot a$$</p> <p>This is a fundamental property of the sigmoid function and is used extensively in neural networks.</p> Partial Derivatives w.r.t. Parameters <p>For \u03bc(x) = 1/(1 + exp(-a(x-c))):</p> <ul> <li>\u2202\u03bc/\u2202a = \u03bc(x) \u00b7 (1 - \u03bc(x)) \u00b7 (x - c)</li> <li>\u2202\u03bc/\u2202c = -a \u00b7 \u03bc(x) \u00b7 (1 - \u03bc(x))</li> </ul>"},{"location":"membership_functions/sigmoidal/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a sigmoidal membership function and visualize it:</p>"},{"location":"membership_functions/sigmoidal/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different sigmoidal membership functions with varying parameter combinations. Each subplot demonstrates how the slope (a) and center (c) parameters affect the shape of the S-curve.</p>"},{"location":"membership_functions/sshaped-linear/","title":"Linear S-shaped","text":"<p>The mathematical formula for the linear S-shaped membership function is given by:</p> <ul> <li>$\u03bc(x) = 0$, for $x \\le a$</li> <li>$\u03bc(x) = (x - a) / (b - a)$, for $a &lt; x &lt; b$</li> <li>$\u03bc(x) = 1$, for $x \\ge b$</li> </ul> <p>Where:</p> <ul> <li>$\u03bc(x)$ is the degree of membership for element $x$ in the fuzzy set.</li> <li>$a$ and $b$ are the parameters that define the start and end of the linear ramp.</li> </ul> <p>The function is characterized by two main parameters:</p> <ul> <li><code>a</code> (start point): The \"left foot\" of the curve. This is the point where the membership transition begins from 0.</li> <li><code>b</code> (end point): The \"right shoulder\" of the curve. This is the point where the membership reaches and stays at 1. The parameter <code>b</code> must always be greater than <code>a</code>.</li> </ul> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import LinSShapedMF\n\nmf = LinSShapedMF(a=3, b=7)\n\nx = np.linspace(0, 10, 100)\ny = mf.forward(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import LinSShapedMF  mf = LinSShapedMF(a=3, b=7)  x = np.linspace(0, 10, 100) y = mf.forward(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sshaped-linear/#linear-s-shaped","title":"Linear S-shaped\u00b6","text":"<p>The linear S-shaped membership function (<code>LinSShapedMF</code>) is a fundamental concept in fuzzy logic, used to define fuzzy sets. Unlike a classical set where an element either fully belongs or doesn't, a fuzzy set allows for partial membership, and the <code>LinSShapedMF</code> provides a smooth, continuous way to represent this degree of belonging.</p> <p>It's a piecewise linear function that transitions from 0 to 1 over a defined interval.</p>"},{"location":"membership_functions/sshaped-linear/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives of the membership function are crucial for optimization algorithms, such as backpropagation, which allow the system to adapt. They indicate how the membership value changes in response to small adjustments in the parameters <code>a</code> and <code>b</code>.</p> Derivative with respect to `a` ($\\frac{\\partial \\mu}{\\partial a}$) <p>The partial derivative with respect to <code>a</code> indicates how the membership value is affected when the starting point of the ramp is adjusted.</p> <p>$\\frac{\\partial \\mu}{\\partial a} = -\\frac{1}{b-a}$ (for the ramp region)</p> Derivative with respect to `b` ($\\frac{\\partial \\mu}{\\partial b}$) <p>The partial derivative with respect to <code>b</code> shows how the membership value changes when the end point of the ramp is adjusted.</p> <p>$\\frac{\\partial \\mu}{\\partial b} = \\frac{x-a}{(b-a)^2}$ (for the ramp region)</p>"},{"location":"membership_functions/sshaped-linear/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of the S-shaped membership function, showing how its shape is influenced by the parameters <code>a</code> and <code>b</code>.</p>"},{"location":"membership_functions/sshaped/","title":"S-shaped","text":"<p>The S-shaped function's curve is defined by two main parameters:</p> <ul> <li>$a$: The point where the function begins to rise from a membership degree of 0.0.</li> <li>$b$: The point where the function reaches a membership degree of 1.0.</li> </ul> <p>The transition between these two points is described by the following equation:</p> <p>$$ S(x; a, b) = \\begin{array}{ll} 0 &amp; x \\le a \\\\[6pt] 2 \\left( \\frac{x-a}{b-a} \\right)^2 &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt] 1 - 2 \\left( \\frac{x-b}{b-a} \\right)^2 &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt] 1 &amp; x \\ge b \\end{array}{ll} $$</p> <p>This formulation ensures a smooth and continuous transition between the different segments of the curve, which is fundamental for representing uncertainties and imprecision in fuzzy logic systems.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom anfis_toolbox.membership import SShapedMF\n\n\nmf = SShapedMF(a=2, b=8)\n\nx = np.linspace(0, 10, 200)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from anfis_toolbox.membership import SShapedMF   mf = SShapedMF(a=2, b=8)  x = np.linspace(0, 10, 200) y = mf(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/sshaped/#s-shaped","title":"S-shaped\u00b6","text":"<p>The S-shaped Membership Function (<code>SShapedMF</code>) is a type of membership function used in fuzzy logic. It models the gradual transition from a zero degree of membership (0.0) to a full degree (1.0). This form is ideal for representing concepts like \"hot\" or \"fast,\" where membership starts low and progressively increases to a certain point. The transition is defined by a cubic polynomial, resulting in a smooth, continuous curve without angular points.</p>"},{"location":"membership_functions/sshaped/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>To optimize the shape of the S-shaped membership function for a specific application, it's often necessary to calculate the partial derivatives with respect to its parameters, $a$ and $b$. These derivatives are crucial for optimization algorithms like gradient descent.</p> Partial Derivative with Respect to $a$ <p>The partial derivative of the S-shaped function with respect to the parameter $a$ is calculated as follows:</p> <p>$$ \\frac{\\partial S}{\\partial a} = \\begin{array}{ll}   0 &amp; x \\le a \\\\[6pt]   \\frac{-4(x-a)}{(b-a)^2} + \\frac{4(x-a)^2}{(b-a)^3} &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt]   \\frac{4(x-b)^2}{(b-a)^3} &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt]   0 &amp; x \\ge b \\end{array} $$</p> <p>This derivative shows how the membership value changes as the starting point of the curve, $a$, is adjusted.</p> Partial Derivative with Respect to $b$ <p>The partial derivative of the S-shaped function with respect to the parameter $b$ is calculated as follows:</p> <p>$$ \\frac{\\partial S}{\\partial b} = \\begin{array}{ll}   0 &amp; x \\le a \\\\[6pt]   \\frac{-4(x-a)^2}{(b-a)^3} &amp; a &lt; x \\le \\frac{a+b}{2} \\\\[6pt]   \\frac{-4(x-b)}{(b-a)^2} + \\frac{4(x-b)^2}{(b-a)^3} &amp; \\frac{a+b}{2} &lt; x &lt; b \\\\[6pt]   0 &amp; x \\ge b \\end{array} $$</p> <p>This derivative indicates how the membership value changes as the ending point of the curve, $b$, is adjusted.</p> <p>These partial derivatives are essential tools for tuning the S-shaped membership function to better fit data or to meet specific system requirements. They enable gradient-based optimization by providing the direction and magnitude of the steepest ascent/descent for the parameters.</p>"},{"location":"membership_functions/sshaped/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/sshaped/#visualization","title":"Visualization\u00b6","text":"<p>Below is a visual representation of the S-shaped membership function, showing how its shape is influenced by the parameters <code>a</code> and <code>b</code>.</p>"},{"location":"membership_functions/trapezoidal/","title":"Trapezoidal","text":"<p>The Trapezoidal Membership Function is defined by the piecewise linear equation:</p> <p>$$\\mu(x) =  \\begin{array}{ll} 0 &amp; x \\leq a \\\\ \\frac{x - a}{b - a} &amp; a &lt; x &lt; b \\\\ 1 &amp; b \\leq x \\leq c \\\\ \\frac{d - x}{d - c} &amp; c &lt; x &lt; d \\\\ 0 &amp; x \\geq d  \\end{array}$$</p> <p>The function is characterized by four parameters:</p> <ul> <li>a: Left base point (lower support bound) - where \u03bc(x) starts increasing from 0</li> <li>b: Left peak point (start of plateau) - where \u03bc(x) reaches 1</li> <li>c: Right peak point (end of plateau) - where \u03bc(x) starts decreasing from 1</li> <li>d: Right base point (upper support bound) - where \u03bc(x) returns to 0</li> </ul> <p>Parameter Constraints For a valid trapezoidal function, parameters must satisfy: a \u2264 b \u2264 c \u2264 d</p> <p>Geometric Interpretation</p> <ul> <li>The region [a, b] forms the left slope (rising edge)</li> <li>The region [b, c] forms the plateau (full membership region)</li> <li>The region [c, d] forms the right slope (falling edge)</li> <li>Outside [a, d], membership is zero</li> </ul> <p>This shape is particularly useful when you need:</p> <ul> <li>A stable region of full membership (plateau)</li> <li>Gradual transitions at the boundaries</li> <li>Robustness to small variations in input values</li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import TrapezoidalMF\n\ntrapezoidal = TrapezoidalMF(a=2, b=4, c=6, d=8)\n\n\nx = np.linspace(0, 10, 200)\ny = trapezoidal(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import TrapezoidalMF  trapezoidal = TrapezoidalMF(a=2, b=4, c=6, d=8)   x = np.linspace(0, 10, 200) y = trapezoidal(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/trapezoidal/#trapezoidal","title":"Trapezoidal\u00b6","text":"<p>The Trapezoidal Membership Function is a piecewise linear function that creates a trapezoid-shaped membership curve. It is widely used in fuzzy logic systems when you need a plateau region of full membership, providing robustness to noise and uncertainty.</p>"},{"location":"membership_functions/trapezoidal/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>For optimization in ANFIS networks, we need the gradients of the membership function with respect to each parameter. The derivatives are computed analytically for each region:</p> Left Slope Region (a &lt; x &lt; b) $$\\mu(x) = \\frac{x - a}{b - a}$$  <ul> <li>\u2202\u03bc/\u2202a = -1/(b-a)</li> <li>\u2202\u03bc/\u2202b = -(x-a)/(b-a)\u00b2</li> <li>\u2202\u03bc/\u2202c = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202d = 0 (no effect in this region)</li> </ul> Plateau Region (b \u2264 x \u2264 c) $$\\mu(x) = 1$$  <ul> <li>\u2202\u03bc/\u2202a = 0 (constant function)</li> <li>\u2202\u03bc/\u2202b = 0 (constant function)</li> <li>\u2202\u03bc/\u2202c = 0 (constant function)</li> <li>\u2202\u03bc/\u2202d = 0 (constant function)</li> </ul> Right Slope Region (c &lt; x &lt; d) $$\\mu(x) = \\frac{d - x}{d - c}$$  <ul> <li>\u2202\u03bc/\u2202a = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202b = 0 (no effect in this region)</li> <li>\u2202\u03bc/\u2202c = (x-d)/(d-c)\u00b2</li> <li>\u2202\u03bc/\u2202d = (x-c)/(d-c)\u00b2</li> </ul>"},{"location":"membership_functions/trapezoidal/#python-example","title":"Python Example\u00b6","text":"<p>Let's create a trapezoidal membership function and visualize it:</p>"},{"location":"membership_functions/trapezoidal/#visualization","title":"Visualization\u00b6","text":"<p>The following interactive plot shows different trapezoidal membership functions with varying parameter combinations. Each subplot demonstrates how the shape changes with different plateau widths and slope characteristics.</p>"},{"location":"membership_functions/triangular/","title":"Triangular","text":"<p>The Triangular membership function (TriangularMF) is one of the most fundamental and widely used membership functions in fuzzy logic. It represents fuzzy sets with a simple triangular shape, making it intuitive and computationally efficient. The function is defined by three key points that form the triangle: the left base point, the peak, and the right base point.</p> <p>The function is characterized by three main parameters:</p> <ul> <li>a: Left base point of the triangle (\u03bc(x) = 0 for x \u2264 a).</li> <li>b: Peak point of the triangle (\u03bc(b) = 1, the maximum membership value).</li> <li>c: Right base point of the triangle (\u03bc(x) = 0 for x \u2265 c).</li> </ul> <p>These parameters must satisfy the constraint: a \u2264 b \u2264 c.</p> <p>The mathematical formula for the triangular membership function is defined piecewise:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 0, &amp; x \\leq a, \\\\[4pt] \\dfrac{x-a}{b-a}, &amp; a &lt; x \\leq b, \\\\[4pt] \\dfrac{c-x}{c-b}, &amp; b &lt; x &lt; c, \\\\[4pt] 0, &amp; x \\geq c. \\end{array} $$</p> <p>where:</p> <ul> <li>$\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</li> <li>$x$ is the input value.</li> <li>$a$ is the left base point.</li> <li>$b$ is the peak point.</li> <li>$c$ is the right base point.</li> </ul> <p>The triangular membership function is particularly useful for representing concepts like \"approximately equal to b\" or \"around b\", where the membership decreases linearly as we move away from the peak value.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom anfis_toolbox.membership import TriangularMF\n\ntriangular = TriangularMF(a=2, b=5, c=8)\n\nx = np.linspace(0, 10, 100)\ny = triangular(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  from anfis_toolbox.membership import TriangularMF  triangular = TriangularMF(a=2, b=5, c=8)  x = np.linspace(0, 10, 100) y = triangular(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/triangular/#triangular","title":"Triangular\u00b6","text":""},{"location":"membership_functions/triangular/#partial-derivatives","title":"Partial Derivatives\u00b6","text":"<p>The partial derivatives of the triangular membership function are essential for optimization in adaptive fuzzy systems. Since the function is piecewise linear, the derivatives are computed separately for each region.</p> Derivative with respect to $a$ <p>For $a &lt; x &lt; b$: $$\\frac{\\partial \\mu}{\\partial a} = \\frac{x-b}{(b-a)^2}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $b$ <p>For $a &lt; x &lt; b$: $$\\frac{\\partial \\mu}{\\partial b} = \\frac{-(x-a)}{(b-a)^2}$$</p> <p>For $b &lt; x &lt; c$: $$\\frac{\\partial \\mu}{\\partial b} = \\frac{x-c}{(c-b)^2}$$</p> <p>For other regions: The derivative is 0.</p> Derivative with respect to $c$ <p>For $b &lt; x &lt; c$: $$\\frac{\\partial \\mu}{\\partial c} = \\frac{x-b}{(c-b)^2}$$</p> <p>For other regions: The derivative is 0.</p> <p>These derivatives enable gradient-based optimization of the triangular membership function parameters, allowing the triangle to adapt its shape during training to better fit the data.</p>"},{"location":"membership_functions/triangular/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/triangular/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the TriangularMF shape changes with different parameter combinations. We'll explore variations in the base points (a, c) and peak position (b).</p>"},{"location":"membership_functions/zshaped-linear/","title":"Linear Z-shaped","text":"<p>The formula for the <code>LinZShapedMF</code> is a piecewise linear function:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 1, &amp; x \\le a \\\\[6pt] \\frac{b - x}{b - a}, &amp; a &lt; x &lt; b \\\\[6pt] 0, &amp; x \\ge b \\end{array} $$</p> <p>Where $\\mu(x)$ is the degree of membership of element $x$ in the fuzzy set.</p> <p>The function is defined by two parameters that delimit the linear transition region:</p> <ul> <li><code>a</code> (Left Shoulder): The point where the membership value begins to transition from 1.0. For input values less than or equal to <code>a</code>, the membership is always 1.0.</li> <li><code>b</code> (Right Foot): The point where the membership value reaches and stays at zero. For input values greater than or equal to <code>b</code>, the membership is always 0.0.</li> </ul> <p>It is crucial that the parameter <code>a</code> is less than <code>b</code> for the linear transition to occur correctly.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom anfis_toolbox.membership import LinZShapedMF\n\nmf = LinZShapedMF(a=15, b=35)\n\nx = np.linspace(0, 50, 100)\ny = mf.forward(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  from anfis_toolbox.membership import LinZShapedMF  mf = LinZShapedMF(a=15, b=35)  x = np.linspace(0, 50, 100) y = mf.forward(x)  plt.plot(x, y) plt.show()"},{"location":"membership_functions/zshaped-linear/#linear-z-shaped","title":"Linear Z-shaped\u00b6","text":"<p>The Linear Z-shaped Membership Function (<code>LinZShapedMF</code>) is a type of membership function in fuzzy logic that represents a smooth linear transition from a full degree of membership (1.0) to zero. Its shape is ideal for modeling concepts like \"cold\" or \"low,\" where membership is high up to a certain point and then decreases linearly to zero.</p>"},{"location":"membership_functions/zshaped-linear/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives are essential for optimizing the parameters <code>a</code> and <code>b</code> in adaptive fuzzy systems.</p> Derivative with respect to `a` <p>$$\\frac{\\partial \\mu}{\\partial a} = \\frac{b - x}{(b - a)^2}$$</p> Derivative with respect to `b` <p>$$\\frac{\\partial \\mu}{\\partial b} = -\\frac{x - a}{(b - a)^2}$$</p>"},{"location":"membership_functions/zshaped-linear/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/zshaped-linear/#visualization","title":"Visualization\u00b6","text":"<p>Below is a comprehensive visualization showing how the LinZShapedMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/zshaped/","title":"Z-shaped","text":"<p>The function is defined by two key parameters that delimit the transition region:</p> <ul> <li><code>a</code> (Left Shoulder): The point where the transition from a full degree of membership (1.0) begins. For input values less than or equal to <code>a</code>, the membership is always 1.0.</li> <li><code>b</code> (Right Foot): The point where the transition ends and the degree of membership becomes zero. For input values greater than or equal to <code>b</code>, the membership is always 0.0.</li> </ul> <p>It is crucial that <code>a</code> is less than <code>b</code> for the transition to occur correctly.</p> <p>The formula for the <code>ZShapedMF</code> is based on the smoothstep function, a third-degree polynomial. The function is defined in parts:</p> <p>$$ \\mu(x) = \\begin{array}{ll} 1, &amp; x \\leq a, \\\\[6pt] 1 - \\bigl(3t^{2} - 2t^{3}\\bigr),  &amp; a &lt; x &lt; b \\\\[6pt] 0, &amp; x \\geq b. \\end{array} $$</p> <p>Where:</p> <p>$\\mu(x)$ is the degree of membership of element $x$ and $t = \\dfrac{x-a}{\\,b-a\\,}$.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom anfis_toolbox.membership import ZShapedMF\n\nmf = ZShapedMF(a=15, b=25)\n\nx = np.linspace(0, 40, 100)\ny = mf(x)\n\nplt.plot(x, y)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from anfis_toolbox.membership import ZShapedMF  mf = ZShapedMF(a=15, b=25)  x = np.linspace(0, 40, 100) y = mf(x)  plt.plot(x, y) plt.show() <p>Below is a comprehensive visualization showing how the ZShapedMF shape changes with different parameter combinations.</p>"},{"location":"membership_functions/zshaped/#z-shaped","title":"Z-shaped\u00b6","text":"<p>The Z-shaped Membership Function (<code>ZShapedMF</code>) is a fundamental type of membership function in fuzzy logic. It provides a smooth and continuous transition from a full degree of membership (1.0) to zero. This form is ideal for modeling concepts like \"cold\" or \"slow,\" where membership is high up to a certain point and then decreases gradually. The transition is defined using a cubic polynomial, resulting in a smooth curve without angular points.</p>"},{"location":"membership_functions/zshaped/#partial-derivatives-gradients","title":"Partial Derivatives (Gradients)\u00b6","text":"<p>The partial derivatives are crucial for optimizing the parameters <code>a</code> and <code>b</code> in adaptive fuzzy systems. They show how the function's output changes in response to small changes in these parameters.</p> Derivative with respect to `a` ($\\frac{\\partial \\mu}{\\partial a}$) <p>This derivative indicates how the degree of membership is affected by adjusting the starting point of the transition.</p> <p>$$\\frac{\\partial \\mu}{\\partial a} = \\frac{\\partial \\mu}{\\partial t} \\frac{\\partial t}{\\partial a} = - (6t(t-1)) \\cdot \\frac{x-b}{(b-a)^2}$$</p> Derivative with respect to `b` ($\\frac{\\partial \\mu}{\\partial b}$) <p>This derivative indicates how the degree of membership is affected by adjusting the endpoint of the transition.</p> <p>$$\\frac{\\partial \\mu}{\\partial b} = \\frac{\\partial \\mu}{\\partial t} \\frac{\\partial t}{\\partial b} = - (6t(t-1)) \\cdot - \\frac{x-a}{(b-a)^2}$$</p>"},{"location":"membership_functions/zshaped/#python-example","title":"Python Example\u00b6","text":""},{"location":"membership_functions/zshaped/#visualization","title":"Visualization\u00b6","text":""},{"location":"models/anfis-classifier/","title":"ANFIS Classifier","text":"<p>The ANFIS Classifier is a variant of the Adaptive Neuro-Fuzzy Inference System designed for multi-class classification tasks. It extends the ANFIS architecture with a softmax output layer and trains using cross-entropy loss.</p>"},{"location":"models/anfis-classifier/#overview","title":"Overview","text":"<p>The ANFIS Classifier uses the same four-layer architecture as the regression model:</p> <ol> <li>Membership Layer: Fuzzifies crisp inputs using membership functions.</li> <li>Rule Layer: Computes rule strengths using T-norm operations.</li> <li>Normalization Layer: Normalizes rule weights to ensure they sum to 1.</li> <li>Consequent Layer: Computes class logits using Takagi-Sugeno-Kang (TSK) models with multiple outputs.</li> </ol>"},{"location":"models/anfis-classifier/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Each rule produces logits for all classes:</p> <p>If \\(x_1\\) is \\(A_1^i\\) and \\(x_2\\) is \\(A_2^i\\) ... and \\(x_n\\) is \\(A_n^i\\) then \\(y^i_k = p_{0,k}^i + p_{1,k}^i x_1 + \\dots + p_{n,k}^i x_n\\) for \\(k = 1, \\dots, K\\)</p> <p>Where \\(K\\) is the number of classes.</p> <p>The final logits are weighted sums: \\(z_k = \\sum_{i=1}^R w^i y^i_k / \\sum_{i=1}^R w^i\\)</p> <p>Probabilities are computed via softmax: \\(p_k = \\exp(z_k) / \\sum_{j=1}^K \\exp(z_j)\\)</p> <p>Training minimizes cross-entropy loss.</p>"},{"location":"models/anfis-classifier/#anfisclassifier-class","title":"ANFISClassifier Class","text":"<p>The <code>ANFISClassifier</code> class implements the classification variant of the ANFIS model.</p>"},{"location":"models/anfis-classifier/#initialization","title":"Initialization","text":"<pre><code>from anfis_toolbox.membership import GaussianMF\nfrom anfis_toolbox.model import ANFISClassifier\n\ninput_mfs = {\n    'x1': [GaussianMF(0, 1), GaussianMF(1, 1)],\n    'x2': [GaussianMF(0, 1), GaussianMF(1, 1)]\n}\nclassifier = ANFISClassifier(input_mfs, n_classes=3)\n</code></pre>"},{"location":"models/anfis-classifier/#key-methods","title":"Key Methods","text":"<ul> <li><code>forward(x)</code>: Returns logits for classification.</li> <li><code>predict_proba(x)</code>: Returns class probabilities.</li> <li><code>predict(x)</code>: Returns predicted class labels.</li> <li><code>fit(X, y, epochs=100, learning_rate=0.01)</code>: Trains the classifier.</li> <li><code>get_parameters()</code> / <code>set_parameters()</code>: For parameter management.</li> <li><code>update_parameters(learning_rate)</code>: Applies gradient descent updates.</li> </ul>"},{"location":"models/anfis-classifier/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\n\n# Generate classification data\nX = np.random.randn(100, 2)\ny = np.random.randint(0, 3, 100)\n\n# Train the classifier\nlosses = classifier.fit(X, y, epochs=50, learning_rate=0.01)\n\n# Make predictions\nprobabilities = classifier.predict_proba(X)\nlabels = classifier.predict(X)\n</code></pre>"},{"location":"models/anfis-classifier/#detailed-architecture","title":"Detailed Architecture","text":""},{"location":"models/anfis-classifier/#layer-1-membership-layer","title":"Layer 1: Membership Layer","text":"<p>For each input \\(x_j\\), computes membership degrees \\(\\mu_{A_j^i}(x_j)\\) for each fuzzy set \\(A_j^i\\).</p>"},{"location":"models/anfis-classifier/#layer-2-rule-layer","title":"Layer 2: Rule Layer","text":"<p>Computes firing strengths \\(w^i = \\prod_{j=1}^n \\mu_{A_j^i}(x_j)\\) using product T-norm.</p>"},{"location":"models/anfis-classifier/#layer-3-normalization-layer","title":"Layer 3: Normalization Layer","text":"<p>Normalizes weights: \\(\\bar{w}^i = w^i / \\sum_{k=1}^R w^k\\)</p>"},{"location":"models/anfis-classifier/#layer-4-consequent-layer","title":"Layer 4: Consequent Layer","text":"<p>Computes rule logits \\(y^i_k = \\sum_{m=0}^n p_{m,k}^i x_m\\), then final logits \\(z_k = \\sum_{i=1}^R \\bar{w}^i y^i_k\\)</p>"},{"location":"models/anfis-classifier/#training-process","title":"Training Process","text":"<p>The classifier uses gradient descent on cross-entropy loss:</p> <ol> <li>Forward Pass: Compute logits and probabilities.</li> <li>Loss Computation: Cross-entropy between true labels and predicted probabilities.</li> <li>Backward Pass: Compute gradients through softmax and all layers.</li> <li>Parameter Update: Update membership and consequent parameters.</li> </ol>"},{"location":"models/anfis-classifier/#choosing-membership-functions","title":"Choosing Membership Functions","text":"<ul> <li>Number of MFs per input: Start with 2-3, increase for complex decision boundaries.</li> <li>MF Types: Gaussian for smooth boundaries, triangular for piecewise linear.</li> <li>Initialization: Place MFs to cover the input range evenly.</li> </ul>"},{"location":"models/anfis-classifier/#evaluation-and-metrics","title":"Evaluation and Metrics","text":"<p>Use classification metrics like accuracy, cross-entropy:</p> <pre><code>from anfis_toolbox.metrics import accuracy, cross_entropy\n\nacc = accuracy(y_true, predictions)\nloss = cross_entropy(y_true_onehot, logits)\n</code></pre>"},{"location":"models/anfis-classifier/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/anfis-classifier/#custom-training","title":"Custom Training","text":"<pre><code># Manual training loop\nfor epoch in range(100):\n    classifier.reset_gradients()\n    logits = classifier.forward(X)\n    # Compute loss and gradients...\n    classifier.update_parameters(0.01)\n</code></pre>"},{"location":"models/anfis-classifier/#parameter-management","title":"Parameter Management","text":"<pre><code># Save parameters\nparams = classifier.get_parameters()\n\n# Load parameters\nclassifier.set_parameters(params)\n</code></pre>"},{"location":"models/anfis-classifier/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Scalability: Rule count grows exponentially; suitable for low-dimensional data.</li> <li>Training Time: May require more epochs than regression due to cross-entropy.</li> <li>Memory Usage: Scales with rules \u00d7 classes \u00d7 inputs.</li> </ul>"},{"location":"models/anfis-classifier/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Poor Accuracy: Increase membership functions or epochs.</li> <li>Overfitting: Reduce rules or add regularization.</li> <li>Class Imbalance: Ensure balanced training data.</li> </ul>"},{"location":"models/anfis-classifier/#references","title":"References","text":"<ul> <li>Jang, J.-S. R. (1993). ANFIS: Adaptive-network-based fuzzy inference system. IEEE Transactions on Systems, Man, and Cybernetics, 23(3), 665-685.</li> </ul>"},{"location":"models/anfis/","title":"ANFIS Model","text":"<p>The ANFIS (Adaptive Neuro-Fuzzy Inference System) model is the core component of the ANFIS Toolbox. It implements a fuzzy inference system that combines the benefits of fuzzy logic and neural networks for function approximation tasks.</p>"},{"location":"models/anfis/#overview","title":"Overview","text":"<p>The ANFIS architecture consists of four layers:</p> <ol> <li>Membership Layer: Fuzzifies crisp inputs using membership functions.</li> <li>Rule Layer: Computes rule strengths using T-norm operations.</li> <li>Normalization Layer: Normalizes rule weights to ensure they sum to 1.</li> <li>Consequent Layer: Computes the final output using Takagi-Sugeno-Kang (TSK) models.</li> </ol>"},{"location":"models/anfis/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>ANFIS is based on the Takagi-Sugeno fuzzy model. Each rule has the form:</p> <p>If \\(x_1\\) is \\(A_1^i\\) and \\(x_2\\) is \\(A_2^i\\) ... and \\(x_n\\) is \\(A_n^i\\) then \\(y^i = p_0^i + p_1^i x_1 + \\dots + p_n^i x_n\\)</p> <p>Where \\(A_j^i\\) are fuzzy sets, and \\(p_k^i\\) are consequent parameters.</p> <p>The overall output is a weighted average of rule outputs:</p> <p>\\(y = \\sum_{i=1}^R w^i y^i / \\sum_{i=1}^R w^i\\)</p> <p>Where \\(w^i\\) is the firing strength of rule \\(i\\).</p>"},{"location":"models/anfis/#anfis-class","title":"ANFIS Class","text":"<p>The <code>ANFIS</code> class implements the regression variant of the ANFIS model.</p>"},{"location":"models/anfis/#initialization","title":"Initialization","text":"<pre><code>from anfis_toolbox.membership import GaussianMF\nfrom anfis_toolbox.model import ANFIS\n\ninput_mfs = {\n    'x1': [GaussianMF(0, 1), GaussianMF(1, 1)],\n    'x2': [GaussianMF(0, 1), GaussianMF(1, 1)]\n}\nmodel = ANFIS(input_mfs)\n</code></pre>"},{"location":"models/anfis/#key-methods","title":"Key Methods","text":"<ul> <li><code>forward(x)</code>: Performs a forward pass through the network.</li> <li><code>backward(dL_dy)</code>: Computes gradients via backpropagation.</li> <li><code>predict(x)</code>: Makes predictions on input data.</li> <li><code>fit(x, y, epochs=100, learning_rate=0.01)</code>: Trains the model using hybrid learning.</li> <li><code>get_parameters()</code> / <code>set_parameters()</code>: For parameter management.</li> <li><code>update_parameters(learning_rate)</code>: Applies gradient descent updates.</li> </ul>"},{"location":"models/anfis/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\n\n# Generate sample data\nX = np.random.randn(100, 2)\ny = X[:, 0] + X[:, 1] + np.random.randn(100) * 0.1\n\n# Train the model\nlosses = model.fit(X, y, epochs=50, learning_rate=0.01)\n\n# Make predictions\npredictions = model.predict(X)\n</code></pre>"},{"location":"models/anfis/#detailed-architecture","title":"Detailed Architecture","text":""},{"location":"models/anfis/#layer-1-membership-layer","title":"Layer 1: Membership Layer","text":"<p>For each input \\(x_j\\), computes membership degrees \\(\\mu_{A_j^i}(x_j)\\) for each fuzzy set \\(A_j^i\\).</p>"},{"location":"models/anfis/#layer-2-rule-layer","title":"Layer 2: Rule Layer","text":"<p>Computes firing strengths \\(w^i = \\prod_{j=1}^n \\mu_{A_j^i}(x_j)\\) using product T-norm.</p>"},{"location":"models/anfis/#layer-3-normalization-layer","title":"Layer 3: Normalization Layer","text":"<p>Normalizes weights: \\(\\bar{w}^i = w^i / \\sum_{k=1}^R w^k\\)</p>"},{"location":"models/anfis/#layer-4-consequent-layer","title":"Layer 4: Consequent Layer","text":"<p>Computes rule outputs \\(y^i = \\sum_{k=0}^n p_k^i x_k\\) (with \\(x_0 = 1\\)), then final output \\(y = \\sum_{i=1}^R \\bar{w}^i y^i\\)</p>"},{"location":"models/anfis/#training-process","title":"Training Process","text":"<p>ANFIS uses hybrid learning:</p> <ol> <li>Forward Pass: Compute outputs using current premise parameters.</li> <li>Consequent Parameter Estimation: Use least squares to find optimal \\(p_k^i\\).</li> <li>Backward Pass: Compute gradients for premise parameters.</li> <li>Parameter Update: Update membership function parameters via gradient descent.</li> </ol> <p>This typically converges faster than pure backpropagation.</p>"},{"location":"models/anfis/#choosing-membership-functions","title":"Choosing Membership Functions","text":"<ul> <li>Number of MFs per input: Start with 2-3, increase for complex functions.</li> <li>MF Types: Gaussian for smooth functions, triangular for piecewise linear.</li> <li>Initialization: Place MFs to cover the input range evenly.</li> </ul>"},{"location":"models/anfis/#evaluation-and-metrics","title":"Evaluation and Metrics","text":"<p>Use regression metrics like MSE, RMSE, R\u00b2:</p> <pre><code>from anfis_toolbox.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_true, predictions)\nr2 = r2_score(y_true, predictions)\n</code></pre>"},{"location":"models/anfis/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/anfis/#custom-trainers","title":"Custom Trainers","text":"<pre><code>from anfis_toolbox.optim import HybridTrainer\n\ntrainer = HybridTrainer(learning_rate=0.01, epochs=200)\nlosses = trainer.fit(model, X, y)\n</code></pre>"},{"location":"models/anfis/#parameter-management","title":"Parameter Management","text":"<pre><code># Save parameters\nparams = model.get_parameters()\n\n# Load parameters\nmodel.set_parameters(params)\n</code></pre>"},{"location":"models/anfis/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Scalability: The number of rules grows exponentially with the number of membership functions per input.</li> <li>Training Time: Hybrid training typically converges faster than pure backpropagation.</li> <li>Memory Usage: Parameter storage scales with the number of rules and inputs.</li> </ul>"},{"location":"models/anfis/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Poor Convergence: Try more epochs, adjust learning rate, or add more membership functions.</li> <li>Overfitting: Use fewer rules or add regularization (via custom trainers).</li> <li>Numerical Issues: Ensure inputs are scaled to similar ranges.</li> </ul>"},{"location":"models/anfis/#references","title":"References","text":"<ul> <li>Jang, J.-S. R. (1993). ANFIS: Adaptive-network-based fuzzy inference system. IEEE Transactions on Systems, Man, and Cybernetics, 23(3), 665-685.</li> </ul>"},{"location":"models/fuzzy_c-means/","title":"Fuzzy C-Means Clustering","text":"<p>The Fuzzy C-Means (FCM) clustering algorithm is a soft clustering method that assigns each data point to multiple clusters with varying degrees of membership. It's particularly useful for fuzzy logic applications and can be used to initialize membership functions for ANFIS models.</p>"},{"location":"models/fuzzy_c-means/#overview","title":"Overview","text":"<p>Unlike hard clustering methods like K-Means that assign each point to exactly one cluster, FCM allows points to belong to multiple clusters simultaneously. This makes it suitable for applications where data points have ambiguous cluster memberships.</p>"},{"location":"models/fuzzy_c-means/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>FCM minimizes the objective function:</p> <p>\\(J_m(U, V) = \\sum_{i=1}^n \\sum_{k=1}^c u_{ik}^m \\|x_i - v_k\\|^2\\)</p> <p>Subject to: - \\(\\sum_{k=1}^c u_{ik} = 1\\) for all \\(i\\) - \\(u_{ik} \\in [0, 1]\\) for all \\(i, k\\)</p> <p>Where: - \\(u_{ik}\\) is the membership degree of point \\(i\\) to cluster \\(k\\) - \\(v_k\\) is the center of cluster \\(k\\) - \\(m &gt; 1\\) is the fuzzifier parameter - \\(c\\) is the number of clusters</p> <p>The algorithm iteratively updates memberships and centers using:</p> <p>\\(u_{ik} = \\frac{1}{\\sum_{j=1}^c \\left(\\frac{\\|x_i - v_k\\|}{\\|x_i - v_j\\|}\\right)^{\\frac{2}{m-1}}}\\)</p> <p>\\(v_k = \\frac{\\sum_{i=1}^n u_{ik}^m x_i}{\\sum_{i=1}^n u_{ik}^m}\\)</p>"},{"location":"models/fuzzy_c-means/#fuzzycmeans-class","title":"FuzzyCMeans Class","text":"<p>The <code>FuzzyCMeans</code> class implements the Fuzzy C-Means algorithm.</p>"},{"location":"models/fuzzy_c-means/#initialization","title":"Initialization","text":"<pre><code>from anfis_toolbox.clustering import FuzzyCMeans\n\n# Basic usage\nfcm = FuzzyCMeans(n_clusters=3, m=2.0)\n\n# With custom parameters\nfcm = FuzzyCMeans(\n    n_clusters=4,\n    m=1.5,\n    max_iter=200,\n    tol=1e-5,\n    random_state=42\n)\n</code></pre>"},{"location":"models/fuzzy_c-means/#key-parameters","title":"Key Parameters","text":"<ul> <li><code>n_clusters</code>: Number of clusters (\u2265 2)</li> <li><code>m</code>: Fuzzifier parameter (&gt; 1, default 2.0)</li> <li><code>max_iter</code>: Maximum iterations (default 300)</li> <li><code>tol</code>: Convergence tolerance (default 1e-4)</li> <li><code>random_state</code>: Random seed for reproducibility</li> </ul>"},{"location":"models/fuzzy_c-means/#key-methods","title":"Key Methods","text":"<ul> <li><code>fit(X)</code>: Fit the FCM model to data</li> <li><code>fit_predict(X)</code>: Fit and return hard cluster labels</li> <li><code>predict(X)</code>: Return hard labels for new data</li> <li><code>predict_proba(X)</code>: Return fuzzy memberships for new data</li> <li><code>transform(X)</code>: Alias for predict_proba</li> </ul>"},{"location":"models/fuzzy_c-means/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom anfis_toolbox.clustering import FuzzyCMeans\n\n# Generate sample data\nX = np.random.randn(150, 2)\nX[:50] += [2, 2]   # Cluster 1\nX[50:100] += [-2, 2]  # Cluster 2\nX[100:] += [0, -2]    # Cluster 3\n\n# Fit FCM\nfcm = FuzzyCMeans(n_clusters=3, random_state=42)\nfcm.fit(X)\n\n# Get results\ncenters = fcm.cluster_centers_\nmemberships = fcm.membership_\nhard_labels = fcm.predict(X)\n\nprint(f\"Cluster centers shape: {centers.shape}\")\nprint(f\"Membership matrix shape: {memberships.shape}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>FCM provides several metrics to evaluate clustering quality:</p>"},{"location":"models/fuzzy_c-means/#partition-coefficient-pc","title":"Partition Coefficient (PC)","text":"<p>Measures the amount of fuzziness in the partition:</p> <p>\\(PC = \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^c u_{ik}^2\\)</p> <p>Higher values (closer to 1) indicate crisper partitions.</p> <pre><code>pc = fcm.partition_coefficient()\nprint(f\"Partition Coefficient: {pc:.4f}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#classification-entropy-ce","title":"Classification Entropy (CE)","text":"<p>Measures the entropy of the membership distribution:</p> <p>\\(CE = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^c u_{ik} \\log u_{ik}\\)</p> <p>Lower values indicate better clustering.</p> <pre><code>ce = fcm.classification_entropy()\nprint(f\"Classification Entropy: {ce:.4f}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#xie-beni-index-xb","title":"Xie-Beni Index (XB)","text":"<p>Combines compactness and separation:</p> <p>\\(XB = \\frac{\\sum_{i=1}^n \\sum_{k=1}^c u_{ik}^m \\|x_i - v_k\\|^2}{n \\cdot \\min_{p \\neq q} \\|v_p - v_q\\|^2}\\)</p> <p>Lower values indicate better clustering.</p> <pre><code>xb = fcm.xie_beni_index(X)\nprint(f\"Xie-Beni Index: {xb:.4f}\")\n</code></pre>"},{"location":"models/fuzzy_c-means/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/fuzzy_c-means/#using-fcm-for-anfis-initialization","title":"Using FCM for ANFIS Initialization","text":"<p>FCM can be used to initialize membership functions for ANFIS models:</p> <pre><code>from anfis_toolbox.membership import GaussianMF\n\n# Cluster data to find centers\nfcm = FuzzyCMeans(n_clusters=3)\nfcm.fit(X)\n\n# Use cluster centers to initialize Gaussians\ncenters = fcm.cluster_centers_[:, 0]  # Assuming 1D input\ninput_mfs = {\n    'x1': [GaussianMF(center, 1.0) for center in centers]\n}\n</code></pre>"},{"location":"models/fuzzy_c-means/#custom-fuzzifier","title":"Custom Fuzzifier","text":"<p>The fuzzifier <code>m</code> controls the fuzziness of the clustering:</p> <ul> <li><code>m \u2192 1</code>: Approaches hard clustering (K-Means)</li> <li><code>m \u2192 \u221e</code>: Maximum fuzziness, all memberships equal</li> </ul> <pre><code># Hard clustering approximation\nhard_fcm = FuzzyCMeans(n_clusters=3, m=1.01)\n\n# Very fuzzy clustering\nfuzzy_fcm = FuzzyCMeans(n_clusters=3, m=5.0)\n</code></pre>"},{"location":"models/fuzzy_c-means/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Convergence: FCM typically converges in fewer iterations than expected</li> <li>Computational Complexity: O(n \u00d7 c \u00d7 d \u00d7 iter), where n=samples, c=clusters, d=dimensions</li> <li>Memory Usage: Stores membership matrix (n \u00d7 c) and centers (c \u00d7 d)</li> <li>Scalability: Suitable for small to medium datasets (&lt; 10,000 samples)</li> </ul>"},{"location":"models/fuzzy_c-means/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Poor Convergence: Increase <code>max_iter</code> or decrease <code>tol</code></li> <li>All Points in One Cluster: Try different <code>random_state</code> or increase <code>m</code></li> <li>Numerical Issues: Ensure data is scaled to similar ranges</li> </ul>"},{"location":"models/fuzzy_c-means/#references","title":"References","text":"<ul> <li>Bezdek, J. C. (1981). Pattern Recognition with Fuzzy Objective Function Algorithms. Springer.</li> <li>Xie, X. L., &amp; Beni, G. (1991). A validity measure for fuzzy clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(8), 841-847.</li> </ul>"}]}