# Basic Function Approximation

**Difficulty**: üü¢ Beginner
**Time**: 15-20 minutes
**Prerequisites**: Basic Python, NumPy

Learn ANFIS fundamentals by approximating mathematical functions. This example demonstrates core concepts like model creation, training, and evaluation using simple synthetic data.

## üìñ Learning Objectives

By the end of this example, you'll understand:

- ‚úÖ How to create ANFIS models for function approximation
- ‚úÖ Different membership function types and their effects
- ‚úÖ Training process and convergence monitoring
- ‚úÖ Model evaluation and prediction accuracy
- ‚úÖ Basic visualization of results

## üéØ Problem Description

We'll approximate the function:
```
f(x, y) = sin(x) √ó cos(y) + 0.5 √ó x √ó y
```

This function has interesting properties:
- **Nonlinear**: Cannot be represented by linear combinations
- **Smooth**: Continuous and differentiable everywhere
- **Bounded**: Output range is approximately [-2.5, 2.5]
- **Multi-modal**: Has multiple peaks and valleys

## üìä Step 1: Generate Training Data

```python
import numpy as np
import matplotlib.pyplot as plt
from anfis_toolbox import QuickANFIS, quick_evaluate

# Set random seed for reproducible results
np.random.seed(42)

# Define the target function
def target_function(x, y):
    """Our target function: f(x,y) = sin(x) * cos(y) + 0.5 * x * y"""
    return np.sin(x) * np.cos(y) + 0.5 * x * y

# Generate training data
n_train = 300
x_range = (-3, 3)
y_range = (-3, 3)

# Random sampling from the input space
X_train = np.random.uniform(
    low=[x_range[0], y_range[0]],
    high=[x_range[1], y_range[1]],
    size=(n_train, 2)
)

# Compute target values
y_train = target_function(X_train[:, 0], X_train[:, 1])

# Add small amount of noise to make it realistic
noise_level = 0.05
y_train += np.random.normal(0, noise_level, n_train)

print(f"Training data: X_train.shape = {X_train.shape}, y_train.shape = {y_train.shape}")
print(f"Input range: [{X_train.min():.2f}, {X_train.max():.2f}]")
print(f"Output range: [{y_train.min():.2f}, {y_train.max():.2f}]")
```

Expected output:
```
Training data: X_train.shape = (300, 2), y_train.shape = (300,)
Input range: [-2.98, 2.99]
Output range: [-2.34, 2.41]
```

## üèóÔ∏è Step 2: Create ANFIS Model

```python
# Create ANFIS model using QuickANFIS
print("Creating ANFIS model...")

model = QuickANFIS.for_regression(
    X=X_train,
    n_mfs=3,                    # 3 membership functions per input
    mf_type='gaussian',         # Gaussian membership functions
    random_state=42             # For reproducible initialization
)

# Print model information
print(f"‚úÖ Model created successfully!")
print(f"Number of inputs: {model.n_inputs}")
print(f"Number of rules: {model.n_rules}")
print(f"Total parameters: {len(model.get_parameters())}")

# Show the rule structure
print("\nRule structure:")
for i in range(min(5, model.n_rules)):  # Show first 5 rules
    print(f"Rule {i+1}: IF x1 is MF{i%3+1} AND x2 is MF{(i//3)+1} THEN y = linear_function")
```

Expected output:
```
Creating ANFIS model...
‚úÖ Model created successfully!
Number of inputs: 2
Number of rules: 9
Total parameters: 45

Rule structure:
Rule 1: IF x1 is MF1 AND x2 is MF1 THEN y = linear_function
Rule 2: IF x1 is MF2 AND x2 is MF1 THEN y = linear_function
Rule 3: IF x1 is MF3 AND x2 is MF1 THEN y = linear_function
Rule 4: IF x1 is MF1 AND x2 is MF2 THEN y = linear_function
Rule 5: IF x1 is MF2 AND x2 is MF2 THEN y = linear_function
```

## üéØ Step 3: Train the Model

```python
print("Training ANFIS model...")

# Train using hybrid learning (recommended)
losses = model.fit_hybrid(
    X=X_train,
    y=y_train,
    epochs=100,
    learning_rate=0.02,
    tolerance=1e-6,
    verbose=True
)

print(f"\n‚úÖ Training completed!")
print(f"Initial loss: {losses[0]:.6f}")
print(f"Final loss: {losses[-1]:.6f}")
print(f"Improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%")
print(f"Converged in {len(losses)} epochs")
```

Expected output:
```
Training ANFIS model...
Epoch 10/100, Loss: 0.234567
Epoch 20/100, Loss: 0.156789
Epoch 30/100, Loss: 0.098765
...
Epoch 80/100, Loss: 0.012345
Early stopping at epoch 85 (tolerance reached)

‚úÖ Training completed!
Initial loss: 0.456789
Final loss: 0.012345
Improvement: 97.30%
Converged in 85 epochs
```

## üìà Step 4: Evaluate Performance

```python
# Quick evaluation using built-in metrics
metrics = quick_evaluate(model, X_train, y_train)

print("üìä Training Performance:")
print(f"R¬≤ Score:     {metrics['r2']:.4f}")
print(f"MAE:          {metrics['mae']:.4f}")
print(f"RMSE:         {metrics['rmse']:.4f}")
print(f"MAPE:         {metrics['mape']:.2f}%")

# Generate test data for evaluation
n_test = 100
X_test = np.random.uniform(
    low=[x_range[0], y_range[0]],
    high=[x_range[1], y_range[1]],
    size=(n_test, 2)
)
y_test_true = target_function(X_test[:, 0], X_test[:, 1])

# Test predictions
y_test_pred = model.predict(X_test)
test_metrics = quick_evaluate(model, X_test, y_test_true)

print("\nüìä Test Performance:")
print(f"R¬≤ Score:     {test_metrics['r2']:.4f}")
print(f"MAE:          {test_metrics['mae']:.4f}")
print(f"RMSE:         {test_metrics['rmse']:.4f}")
print(f"MAPE:         {test_metrics['mape']:.2f}%")

# Detailed prediction examples
print("\nüéØ Example Predictions:")
test_points = np.array([
    [0.0, 0.0],      # Should be close to 0
    [1.57, 0.0],     # Should be close to sin(œÄ/2)*cos(0) = 1
    [-1.57, 1.57],   # Should be close to sin(-œÄ/2)*cos(œÄ/2) = 0
    [1.0, 1.0]       # Should be close to sin(1)*cos(1) + 0.5
])

predictions = model.predict(test_points)
true_values = target_function(test_points[:, 0], test_points[:, 1])

for i, (point, pred, true) in enumerate(zip(test_points, predictions, true_values)):
    error = abs(pred - true)
    print(f"Point {point}: pred={pred:.4f}, true={true:.4f}, error={error:.4f}")
```

Expected output:
```
üìä Training Performance:
R¬≤ Score:     0.9876
MAE:          0.0543
RMSE:         0.0721
MAPE:         8.32%

üìä Test Performance:
R¬≤ Score:     0.9834
MAE:          0.0612
RMSE:         0.0798
MAPE:         9.15%

üéØ Example Predictions:
Point [0. 0.]: pred=0.0123, true=0.0000, error=0.0123
Point [1.57 0.]: pred=0.9876, true=1.0000, error=0.0124
Point [-1.57 1.57]: pred=0.0234, true=0.0000, error=0.0234
Point [1. 1.]: pred=0.9567, true=0.9589, error=0.0022
```

## üìä Step 5: Visualize Results

```python
# Plot training curve
plt.figure(figsize=(15, 10))

# 1. Training Loss Curve
plt.subplot(2, 3, 1)
plt.plot(losses, 'b-', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.title('Training Loss Curve')
plt.grid(True, alpha=0.3)
plt.yscale('log')  # Log scale to see convergence better

# 2. Predictions vs True Values
plt.subplot(2, 3, 2)
plt.scatter(y_test_true, y_test_pred, alpha=0.6, s=30)
plt.plot([y_test_true.min(), y_test_true.max()],
         [y_test_true.min(), y_test_true.max()], 'r--', linewidth=2)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title(f'Predictions vs True (R¬≤ = {test_metrics["r2"]:.3f})')
plt.grid(True, alpha=0.3)

# 3. Residuals Plot
plt.subplot(2, 3, 3)
residuals = y_test_pred - y_test_true
plt.scatter(y_test_pred, residuals, alpha=0.6, s=30)
plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('Predictions')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.grid(True, alpha=0.3)

# 4. Surface Plot (if possible)
if X_test.shape[1] == 2:  # Only for 2D inputs
    plt.subplot(2, 3, 4)

    # Create mesh for surface plot
    x1_mesh = np.linspace(x_range[0], x_range[1], 50)
    x2_mesh = np.linspace(y_range[0], y_range[1], 50)
    X1_mesh, X2_mesh = np.meshgrid(x1_mesh, x2_mesh)
    X_mesh = np.column_stack([X1_mesh.ravel(), X2_mesh.ravel()])

    # Predict on mesh
    Y_mesh_pred = model.predict(X_mesh).reshape(X1_mesh.shape)

    # Plot predicted surface
    contour = plt.contour(X1_mesh, X2_mesh, Y_mesh_pred, levels=15, alpha=0.8)
    plt.contourf(X1_mesh, X2_mesh, Y_mesh_pred, levels=15, alpha=0.6, cmap='viridis')
    plt.colorbar()
    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_true, s=30,
                cmap='viridis', edgecolors='black', alpha=0.8)
    plt.xlabel('x‚ÇÅ')
    plt.ylabel('x‚ÇÇ')
    plt.title('Learned Function Surface')

# 5. Error Distribution
plt.subplot(2, 3, 5)
plt.hist(residuals, bins=20, alpha=0.7, density=True, edgecolor='black')
plt.axvline(x=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('Residuals')
plt.ylabel('Density')
plt.title('Error Distribution')
plt.grid(True, alpha=0.3)

# 6. Learning Progress
plt.subplot(2, 3, 6)
epochs = range(1, len(losses) + 1)
plt.plot(epochs, losses, 'b-', linewidth=2, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Learning Progress')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("üìà Visualization complete! Check the plots above.")
```

## üîç Step 6: Analyze Membership Functions

```python
# Visualize membership functions (if visualization available)
try:
    from anfis_toolbox import ANFISVisualizer

    plt.figure(figsize=(12, 4))

    viz = ANFISVisualizer(model)

    # Plot membership functions for each input
    plt.subplot(1, 2, 1)
    viz.plot_membership_functions(input_idx=0, title='Input 1 Membership Functions')

    plt.subplot(1, 2, 2)
    viz.plot_membership_functions(input_idx=1, title='Input 2 Membership Functions')

    plt.tight_layout()
    plt.show()

    # Print membership function parameters
    print("\nüîç Learned Membership Function Parameters:")
    mf_params = model.get_membership_parameters()

    for input_idx, input_mfs in enumerate(mf_params):
        print(f"\nInput {input_idx + 1}:")
        for mf_idx, params in enumerate(input_mfs):
            print(f"  MF {mf_idx + 1}: center={params['center']:.3f}, width={params['width']:.3f}")

except ImportError:
    print("Visualization not available. Install with: pip install anfis-toolbox[visualization]")
```

Expected output:
```
üîç Learned Membership Function Parameters:

Input 1:
  MF 1: center=-1.523, width=1.124
  MF 2: center=0.087, width=1.089
  MF 3: center=1.634, width=1.156

Input 2:
  MF 1: center=-1.598, width=1.201
  MF 2: center=0.023, width=1.134
  MF 3: center=1.587, width=1.187
```

## üíæ Step 7: Save and Load Model

```python
# Save the trained model
model_filename = 'basic_function_approximation.pkl'
model.save(model_filename)
print(f"‚úÖ Model saved as '{model_filename}'")

# Load the model (to verify persistence works)
from anfis_toolbox import load_anfis

loaded_model = load_anfis(model_filename)
print("‚úÖ Model loaded successfully")

# Verify loaded model works
test_point = np.array([[1.0, -1.0]])
original_pred = model.predict(test_point)
loaded_pred = loaded_model.predict(test_point)

print(f"\nVerification:")
print(f"Original model prediction: {original_pred[0]:.6f}")
print(f"Loaded model prediction:   {loaded_pred[0]:.6f}")
print(f"Difference: {abs(original_pred[0] - loaded_pred[0]):.8f}")

if abs(original_pred[0] - loaded_pred[0]) < 1e-10:
    print("‚úÖ Model persistence verified!")
else:
    print("‚ùå Model persistence issue detected")
```

## üîÑ Step 8: Compare Different Membership Functions

```python
print("\nüîÑ Comparing Different Membership Function Types...")

mf_types = ['gaussian', 'triangular', 'bell', 'trapezoidal']
results = {}

for mf_type in mf_types:
    print(f"\nTesting {mf_type} membership functions...")

    # Create model with specific MF type
    test_model = QuickANFIS.for_regression(
        X=X_train,
        n_mfs=3,
        mf_type=mf_type,
        random_state=42
    )

    # Train model
    test_losses = test_model.fit_hybrid(
        X_train, y_train,
        epochs=50,  # Fewer epochs for comparison
        learning_rate=0.02,
        verbose=False
    )

    # Evaluate
    test_metrics = quick_evaluate(test_model, X_test, y_test_true)

    results[mf_type] = {
        'r2': test_metrics['r2'],
        'rmse': test_metrics['rmse'],
        'final_loss': test_losses[-1],
        'epochs': len(test_losses)
    }

    print(f"  R¬≤ Score: {test_metrics['r2']:.4f}")
    print(f"  RMSE: {test_metrics['rmse']:.4f}")

# Summary comparison
print("\nüìä Membership Function Comparison:")
print("=" * 60)
print(f"{'MF Type':<12} {'R¬≤':<8} {'RMSE':<8} {'Final Loss':<12} {'Epochs':<8}")
print("=" * 60)

for mf_type, metrics in results.items():
    print(f"{mf_type:<12} {metrics['r2']:<8.4f} {metrics['rmse']:<8.4f} "
          f"{metrics['final_loss']:<12.6f} {metrics['epochs']:<8}")

# Find best performing
best_mf = max(results.keys(), key=lambda k: results[k]['r2'])
print(f"\nüèÜ Best performing: {best_mf} (R¬≤ = {results[best_mf]['r2']:.4f})")
```

## üìù Key Takeaways

### ‚úÖ What We Learned

1. **ANFIS Creation**: `QuickANFIS.for_regression()` makes model creation simple
2. **Training**: Hybrid learning combines analytical and gradient methods effectively
3. **Evaluation**: Multiple metrics (R¬≤, MAE, RMSE, MAPE) provide comprehensive assessment
4. **Interpretability**: Membership functions adapt to data characteristics
5. **Persistence**: Models can be saved and loaded for deployment

### üéØ Performance Insights

- **R¬≤ > 0.98**: Excellent approximation quality
- **Fast Convergence**: Usually converges within 50-100 epochs
- **Gaussian MFs**: Generally perform well for smooth functions
- **Rule Complexity**: 3√ó3 = 9 rules sufficient for this 2D function

### üöÄ Next Steps

Now that you've mastered basic function approximation:

1. **Try Different Functions**: Experiment with your own mathematical functions
2. **Explore MF Types**: Test all 6 membership function types available
3. **Increase Complexity**: Try higher-dimensional functions (3+ inputs)
4. **Add Noise**: Test robustness with different noise levels
5. **Real Data**: Move to [Simple Regression](regression.md) with actual datasets

### üõ†Ô∏è Troubleshooting

**Poor Performance?**
- Increase number of membership functions (`n_mfs=4` or `n_mfs=5`)
- Try different MF types (`'bell'` or `'triangular'`)
- Increase training epochs (`epochs=200`)
- Adjust learning rate (`learning_rate=0.01` or `0.05`)

**Slow Training?**
- Reduce number of membership functions
- Use fewer training samples initially
- Decrease epochs for initial experiments

**Overfitting?**
- Reduce model complexity (fewer MFs)
- Add more training data
- Use validation split in training

---

üéâ **Congratulations!** You've successfully learned ANFIS fundamentals through function approximation.

**Continue Learning**: [Simple Regression Example ‚Üí](regression.md)
